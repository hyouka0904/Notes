# Dynaconf Cheat Sheet - æ©Ÿå™¨å­¸ç¿’èˆ‡æ™‚é–“åºåˆ—åˆ†æ

## ğŸš€ å¿«é€Ÿé–‹å§‹

### å®‰è£
```bash
pip install dynaconf
pip install python-dotenv  # å¦‚æœè¦ä½¿ç”¨ .env æª”æ¡ˆ
```

### åˆå§‹åŒ–å°ˆæ¡ˆ
```bash
dynaconf init -f toml  # ä½¿ç”¨ TOML æ ¼å¼
# æˆ–
dynaconf init -f yaml  # ä½¿ç”¨ YAML æ ¼å¼
```

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
project/
â”œâ”€â”€ config.py            # Dynaconf è¨­å®šæª”
â”œâ”€â”€ settings.toml        # ä¸»è¦è¨­å®šæª”
â”œâ”€â”€ .secrets.toml        # æ•æ„Ÿè³‡æ–™ï¼ˆåŠ å…¥ .gitignoreï¼‰
â”œâ”€â”€ .env                 # ç’°å¢ƒè®Šæ•¸
â””â”€â”€ configs/
    â”œâ”€â”€ development.toml # é–‹ç™¼ç’°å¢ƒè¨­å®š
    â”œâ”€â”€ production.toml  # ç”Ÿç”¢ç’°å¢ƒè¨­å®š
    â””â”€â”€ testing.toml     # æ¸¬è©¦ç’°å¢ƒè¨­å®š
```

## âš™ï¸ åŸºæœ¬è¨­å®š

### config.py
```python
from dynaconf import Dynaconf

settings = Dynaconf(
    envvar_prefix="DYNACONF",
    settings_files=['settings.toml', '.secrets.toml'],
    environments=True,
    load_dotenv=True,
    env_switcher="ENV_FOR_DYNACONF",
)
```

## ğŸ¤– æ©Ÿå™¨å­¸ç¿’ + æ™‚é–“åºåˆ—åˆ†æé…ç½®

### settings.toml
```toml
[default]
# å°ˆæ¡ˆåŸºæœ¬è¨­å®š
project_name = "ml_timeseries_project"
random_seed = 42
debug = false

# è³‡æ–™è·¯å¾‘
[default.paths]
data_dir = "data/"
raw_data = "@format {this.data_dir}/raw/"
processed_data = "@format {this.data_dir}/processed/"
models_dir = "models/"
logs_dir = "logs/"
results_dir = "results/"

# æ™‚é–“åºåˆ—è¨­å®š
[default.timeseries]
date_column = "date"
target_column = "value"
frequency = "D"  # D=daily, H=hourly, M=monthly
seasonal_periods = 7  # é€±æœŸæ€§
forecast_horizon = 30  # é æ¸¬æœŸé–“

# ç‰¹å¾µå·¥ç¨‹
[default.features]
lag_features = [1, 7, 14, 30]
rolling_windows = [7, 14, 30]
rolling_stats = ["mean", "std", "min", "max"]
use_holiday_features = true
use_fourier_features = true
fourier_order = 5

# è³‡æ–™é è™•ç†
[default.preprocessing]
handle_missing = "interpolate"  # forward_fill, backward_fill, interpolate
outlier_method = "iqr"  # iqr, zscore, isolation_forest
outlier_threshold = 1.5
scale_method = "standard"  # standard, minmax, robust
train_test_split = 0.8
validation_split = 0.2

# æ¨¡å‹è¨­å®š
[default.models]
# å‚³çµ±æ™‚é–“åºåˆ—æ¨¡å‹
[default.models.arima]
enabled = true
auto_arima = true
seasonal = true
stepwise = true
max_p = 5
max_q = 5
max_d = 2

[default.models.prophet]
enabled = true
yearly_seasonality = true
weekly_seasonality = true
daily_seasonality = false
changepoint_prior_scale = 0.05

# æ©Ÿå™¨å­¸ç¿’æ¨¡å‹
[default.models.random_forest]
enabled = true
n_estimators = 100
max_depth = 10
min_samples_split = 5
n_jobs = -1

[default.models.xgboost]
enabled = true
n_estimators = 100
learning_rate = 0.1
max_depth = 6
subsample = 0.8
colsample_bytree = 0.8

[default.models.lightgbm]
enabled = true
num_leaves = 31
learning_rate = 0.05
n_estimators = 100
boosting_type = "gbdt"

# æ·±åº¦å­¸ç¿’æ¨¡å‹
[default.models.lstm]
enabled = true
units = [128, 64, 32]
dropout = 0.2
batch_size = 32
epochs = 100
early_stopping_patience = 10
optimizer = "adam"
learning_rate = 0.001

[default.models.gru]
enabled = true
units = [100, 50]
dropout = 0.2
batch_size = 32
epochs = 100

# æ¨¡å‹è©•ä¼°
[default.evaluation]
metrics = ["mae", "rmse", "mape", "r2"]
cv_folds = 5
cv_method = "timeseries"  # timeseries, blocking
backtesting_windows = 3

# é–‹ç™¼ç’°å¢ƒ
[development]
debug = true
log_level = "DEBUG"
sample_size = 10000  # ä½¿ç”¨éƒ¨åˆ†è³‡æ–™åŠ é€Ÿé–‹ç™¼

[development.models]
quick_mode = true  # ä½¿ç”¨è¼ƒå°‘çš„åƒæ•¸åŠ é€Ÿè¨“ç·´

# ç”Ÿç”¢ç’°å¢ƒ
[production]
debug = false
log_level = "INFO"
use_gpu = true
parallel_jobs = 8

[production.models]
ensemble = true  # ä½¿ç”¨æ¨¡å‹é›†æˆ
save_predictions = true
```

## ğŸ”§ å–®ç´”æ™‚é–“åºåˆ—åˆ†æé…ç½®

### timeseries_only_settings.toml
```toml
[default]
# åŸºæœ¬è¨­å®š
project_name = "timeseries_analysis"
random_seed = 42

# è³‡æ–™è¨­å®š
[default.data]
file_path = "data/timeseries.csv"
date_column = "date"
value_columns = ["sales", "revenue"]  # å¯å¤šè®Šé‡
date_format = "%Y-%m-%d"
frequency = "D"
missing_value_strategy = "interpolate"

# æ¢ç´¢æ€§åˆ†æ
[default.eda]
plot_components = true
check_stationarity = true
seasonal_decompose = true
decompose_model = "additive"  # additive, multiplicative

# çµ±è¨ˆæª¢å®š
[default.statistical_tests]
adf_test = true  # Augmented Dickey-Fuller
kpss_test = true  # KPSS
acf_pacf_lags = 40

# æ™‚é–“åºåˆ—æ¨¡å‹
[default.models]
# ARIMA è¨­å®š
[default.models.arima]
auto = true
seasonal = true
m = 12  # å­£ç¯€æ€§é€±æœŸ
stepwise = true
trace = true
error_action = "ignore"
suppress_warnings = true

# SARIMA è¨­å®š
[default.models.sarima]
order = [1, 1, 1]  # (p, d, q)
seasonal_order = [1, 1, 1, 12]  # (P, D, Q, m)
enforce_stationarity = false
enforce_invertibility = false

# æŒ‡æ•¸å¹³æ»‘
[default.models.exponential_smoothing]
trend = "add"  # add, mul, None
seasonal = "add"  # add, mul, None
seasonal_periods = 12
use_boxcox = true

# VAR (å‘é‡è‡ªè¿´æ­¸)
[default.models.var]
maxlags = 15
ic = "aic"  # aic, bic, hqic

# é æ¸¬è¨­å®š
[default.forecast]
periods = 30
confidence_intervals = [80, 95]
return_confidence = true

# è¦–è¦ºåŒ–
[default.visualization]
plot_size = [12, 6]
style = "seaborn"
save_plots = true
plot_format = "png"
dpi = 300
```

## ğŸ’¡ å¯¦ç”¨ç¨‹å¼ç¢¼ç‰‡æ®µ

### 1. è¼‰å…¥è¨­å®š
```python
from config import settings

# åŸºæœ¬ä½¿ç”¨
data_path = settings.paths.raw_data
model_type = settings.models.xgboost.enabled

# å‹•æ…‹å­˜å–
model_name = "lightgbm"
model_config = settings.models[model_name]
```

### 2. ç’°å¢ƒåˆ‡æ›
```python
# é€éç’°å¢ƒè®Šæ•¸
# export ENV_FOR_DYNACONF=production

# æˆ–åœ¨ç¨‹å¼ç¢¼ä¸­
from config import settings
settings.setenv('production')

# è‡¨æ™‚åˆ‡æ›
with settings.using_env('development'):
    debug = settings.debug  # True
```

### 3. å‹•æ…‹æ›´æ–°è¨­å®š
```python
# é‹è¡Œæ™‚æ›´æ–°
settings.set('models.lstm.epochs', 200)

# æ‰¹é‡æ›´æ–°
settings.update({
    'models.lstm.batch_size': 64,
    'models.lstm.learning_rate': 0.0001
})
```

### 4. é©—è­‰è¨­å®š
```python
# åœ¨ config.py ä¸­åŠ å…¥é©—è­‰
from dynaconf import Dynaconf, Validator

settings = Dynaconf(
    validators=[
        Validator('random_seed', must_exist=True, is_type_of=int),
        Validator('models.lstm.epochs', gte=1, lte=1000),
        Validator('preprocessing.train_test_split', gte=0.5, lte=0.95),
    ]
)
```

### 5. æ©Ÿå™¨å­¸ç¿’ç®¡ç·šæ•´åˆ
```python
# ml_pipeline.py
from config import settings
import pandas as pd
from sklearn.model_selection import train_test_split

class MLPipeline:
    def __init__(self):
        self.config = settings
        
    def load_data(self):
        df = pd.read_csv(f"{self.config.paths.raw_data}/data.csv")
        return df
        
    def preprocess(self, df):
        # ä½¿ç”¨è¨­å®šä¸­çš„åƒæ•¸
        X_train, X_test = train_test_split(
            df, 
            test_size=1-self.config.preprocessing.train_test_split,
            random_state=self.config.random_seed
        )
        return X_train, X_test
        
    def train_models(self, X_train, y_train):
        models = {}
        
        if self.config.models.xgboost.enabled:
            from xgboost import XGBRegressor
            models['xgboost'] = XGBRegressor(
                **self.config.models.xgboost.to_dict()
            )
            
        return models
```

### 6. æ™‚é–“åºåˆ—ç‰¹å¾µå·¥ç¨‹
```python
def create_time_features(df):
    config = settings.features
    
    # å»¶é²ç‰¹å¾µ
    for lag in config.lag_features:
        df[f'lag_{lag}'] = df[settings.timeseries.target_column].shift(lag)
    
    # æ»¾å‹•çµ±è¨ˆ
    for window in config.rolling_windows:
        for stat in config.rolling_stats:
            df[f'rolling_{stat}_{window}'] = (
                df[settings.timeseries.target_column]
                .rolling(window=window)
                .agg(stat)
            )
    
    return df
```

## ğŸ”’ å®‰å…¨æœ€ä½³å¯¦è¸

### .secrets.toml
```toml
[default]
# API é‡‘é‘°
api_keys.weather_api = "your-secret-key"
api_keys.database_url = "postgresql://user:pass@localhost/db"

# æ†‘è­‰
credentials.aws_access_key = "xxx"
credentials.aws_secret_key = "yyy"
```

### .env
```bash
# ç’°å¢ƒè®Šæ•¸
DYNACONF_DATABASE_URL="@format {env[DATABASE_URL]}"
DYNACONF_API_KEY="@format {env[API_KEY]}"
```

## ğŸ“Š é€²éšæŠ€å·§

### 1. åƒæ•¸çµ„åˆ
```toml
# ä½¿ç”¨ @format é€²è¡Œå­—ä¸²æ’å€¼
[default]
base_path = "/home/user/project"
data_path = "@format {this.base_path}/data"
model_path = "@format {this.base_path}/models/{env[USER]}"
```

### 2. æ¢ä»¶è¨­å®š
```python
# æ ¹æ“šç’°å¢ƒå‹•æ…‹èª¿æ•´
if settings.current_env == "production":
    settings.models.lstm.epochs = 500
else:
    settings.models.lstm.epochs = 50
```

### 3. è¨­å®šç¹¼æ‰¿
```toml
# base.toml
[default.base_model]
optimizer = "adam"
loss = "mse"

# settings.toml
[default.models.lstm]
"@extends" = "base_model"
units = [128, 64]
```

## ğŸ¯ å¸¸è¦‹æ¨¡å¼

### å¯¦é©—è¿½è¹¤
```python
# å°‡è¨­å®šä¿å­˜ç”¨æ–¼å¯¦é©—é‡ç¾
import json
from datetime import datetime

experiment_config = {
    'timestamp': datetime.now().isoformat(),
    'settings': settings.as_dict(),
    'git_hash': get_git_hash()
}

with open('experiments/exp_001.json', 'w') as f:
    json.dump(experiment_config, f)
```

### è¶…åƒæ•¸æœå°‹
```python
# å‹•æ…‹æ›´æ–°è¶…åƒæ•¸
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15]
}

for params in ParameterGrid(param_grid):
    settings.update({'models.random_forest': params})
    # è¨“ç·´æ¨¡å‹...
```

## ğŸ› é™¤éŒ¯æŠ€å·§

```python
# æŸ¥çœ‹æ‰€æœ‰è¨­å®š
print(settings.as_dict())

# æŸ¥çœ‹ç‰¹å®šè¨­å®šä¾†æº
print(settings.get_fresh('models.lstm.epochs'))

# åˆ—å‡ºæ‰€æœ‰ç’°å¢ƒ
print(settings.list_envs())

# é©—è­‰è¨­å®š
settings.validators.validate()
```