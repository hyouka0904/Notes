# 1 Introduction

* 統計模型在現代社會中變得無處不在且越來越重要
* 各種類型的預測已融入我們的日常生活：
  - 醫生依據模型識別特定患者群體的風險
  - 航班到達時間的數值預測
  - 律師使用統計模型量化潛在招聘偏見的可能性

## 模型的本質與目的 (Nature and Purpose of Models)

* 模型通過現有數據尋找可接受精確度的數學表示
* 模型用途分為兩類：
  - **推論型**（Inferential）：為了理解自然狀態而得出結論
    * 例如：招聘偏見的估計與「統計顯著性」的判定
  - **預測型**（Estimation）：專注於對特定值的準確預測
    * 例如：航班到達時間的預測

## 模型的重要特性 (Important Model Characteristics)

* **簡約性**（Parsimony/Simplicity）是關鍵考量
  - 簡單模型在推論目的時特別受青睞
  - 簡約性提高模型的可解釋性
  - 例如：教育年限與薪資的線性關係模型易於解釋

* **準確性**（Accuracy）不應為簡約性嚴重犧牲
  - 模型必須保持對數據的可接受忠實度
  - 複雜性通常是解決準確性差的方案
  - 使用額外參數或非線性模型可能提高準確性但降低可解釋性

* 準確性與簡約性之間的權衡是模型建構的關鍵考量

## 預測變數的重要性 (Importance of Predictors)

* 進入模型的變數及其表示方式與模型本身同樣關鍵
* 相關術語：
  - 被建模或預測的量：**結果變數**（Outcome）、**反應變數**（Response）或**因變數**（Dependent Variable）
  - 用於建模的變數：**預測變數**（Predictors）、**特徵**（Features）或**自變數**（Independent Variables）

* 特徵表示的多樣性（以房屋銷售價格為例）：
  - **位置資訊**可以多種方式表示：
    * 鄰里（Neighborhood）
    * 經緯度（Longitude/Latitude）
    * 郵遞區號（ZIP Code）作為學區的代理
  - 從資訊理論角度，經緯度提供最具體的物理位置信息

## 特徵工程 (Feature Engineering)

* **特徵工程**：創建數據表示以提高模型有效性的過程
* 模型有效性受多種因素影響：
  - 如果預測變數與結果沒有關係，其表示方式無關緊要
  - 不同模型有不同的敏感度和需求

## 不同模型對預測變數的要求 (Model Requirements for Predictors)

* 某些模型無法容忍測量相同基本量的預測變數（**多重共線性**或預測變數間相關性）
* 許多模型無法使用具有任何缺失值的樣本
* 某些模型在存在不相關預測變數時性能嚴重下降

## 本書目標 (Book Objective)

* 幫助實踐者通過專注於預測變數來建立更好的模型
* 「更好」取決於問題背景，但可能包括：準確性、簡單性和穩健性
* 理解預測變數與模型類型之間的相互作用至關重要
* 通過更適合模型的數據表示或減少使用的變數來提高準確性和/或簡化模型
## 1.1 簡單的案例分析 (A Simple Example)

* 本案例來自 Hill et al. (2007) 的實驗，展示**特徵工程**（Feature Engineering）如何影響模型效能
* 使用兩個相關的預測變數（標記為 A 和 B）
* 資料點依據其結果分為兩類：「PS」和「WS」
* 圖 1.2a 顯示兩個類別沿對角線有明確的分隔

### 邏輯迴歸模型實施 (Logistic Regression Implementation)

* 使用**邏輯迴歸模型**（Logistic Regression Model）建立預測方程式：
  $$\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1A + \beta_2B$$
  - $p$ 是樣本屬於「PS」類別的機率
  - $\beta$ 值是需要從資料中估計的模型參數

* 資料分割與參數估計：
  - 訓練集：1009 個資料點用於估計參數
  - 測試集：1010 個樣本用於評估性能
  - 使用**最大概似估計**（Maximum Likelihood Estimation）估計參數
  - 得到的參數值：$\hat{\beta}_0 = 1.73$, $\hat{\beta}_1 = 0.003$, $\hat{\beta}_2 = -0.064$

### 模型評估方法 (Model Evaluation)

* 使用**接收者操作特徵曲線**（Receiver Operating Characteristic, ROC）評估模型
  - 避免使用固定的機率閾值（如50%）進行硬性分類
  - ROC 曲線繪製真陽性率與假陽性率的關係
  - 理想的曲線應盡可能靠近左上角
  - 無效模型的曲線會沿著對角虛線

* 使用**曲線下面積**（Area Under the ROC Curve）作為性能摘要
  - AUC = 1.0 代表完美模型
  - AUC ≈ 0.5 表示模型無預測能力
  - 原始邏輯迴歸模型的 AUC = 0.794（中等準確度）
  - 圖 1.2b 展示了原始模型的 ROC 曲線

### 預測變數轉換的影響 (Impact of Predictor Transformation)

* 由於兩個預測變數均大於零且呈右偏分布，可嘗試不同轉換：
  - 使用比率 A/B 作為單一預測因子
  - 對每個預測因子進行簡單轉換

* 利用**Box-Cox轉換**（Box-Cox Transformation）調整預測變數尺度：
  - 轉換方法建議兩個預測因子都使用倒數尺度（如 1/A 代替 A）
  - 轉換後的資料分布如圖 1.3a 所示
  - 視覺上能更清楚區分兩組資料

* 轉換後的變數帶來顯著改善：
  - AUC 從 0.794 提升到 0.848
  - 圖 1.3b 顯示，轉換後的 ROC 曲線全面優於原始結果

### 模型比較與結論 (Model Comparison and Conclusions)

* 同樣數據使用**神經網路**（Neural Network）模型：
  - 不需要進行倒數轉換也能達到 AUC = 0.848
  - 神經網路不易受預測因子分布特性的影響

* 模型選擇考量：
  - 神經網路：完全不可解釋，需要大量參數調整
  - 依據**沒有免費午餐定理**（No Free Lunch Theorem），不應武斷地認為某模型永遠優於另一個
  - 根據模型用途（推論分析 vs. 純預測），選擇適當的模型

* 案例關鍵啟示：
  - 簡單的特徵轉換可以顯著提高模型效能
  - 不同模型對數據特性有不同的敏感度
  - 適合的特徵工程可以簡化模型或提高其性能

## 1.2 重要概念 (Important Concepts)

### 過擬合 (Overfitting)
* 指模型在當前數據上表現良好，但在新樣本上預測失敗的情況
* 典型原因：模型過度依賴當前數據集中的特定模式和趨勢
* 例如：預測房價時，發現「面積在1,267.5至1,277平方呎且有三間臥室的房屋」價格可準確預測，但此模式不具泛化性
* 高彈性（低偏差）模型更容易過擬合
* 變數選擇也可能過擬合，特別是當樣本數（n）小而預測變數數量（p）大時

### 監督式與非監督式程序 (Supervised and Unsupervised Procedures)
* **監督式分析**（Supervised Analysis）：識別預測變數與目標結果之間的模式
* **非監督式分析**（Unsupervised Analysis）：專注於預測變數之間的模式
* 兩種方法通常都使用**探索性數據分析**（Exploratory Data Analysis, EDA）
* 監督式分析更容易發現數據中的錯誤模式
* 需避免「自我實現的預測預言」：使用同一數據進行變數選擇和視覺化會產生「挑揀結果」現象

### 沒有免費午餐定理 (No Free Lunch)
* 在沒有特定問題或數據知識的情況下，沒有單一預測模型是最佳的
* 不同模型針對不同數據特性（如缺失值或共線性預測因子）進行優化
* 研究顯示某些模型平均表現較佳，但勝率不足以支持「總是使用模型X」的策略
* 實務建議：嘗試多種不同類型的模型來確定哪一種最適合特定數據集

### 模型與建模過程 (The Model versus the Modeling Process)
* 發展有效模型的過程是迭代性和啟發式的
* 圖1.4展示典型建模過程：
  * (a) 探索性數據分析（EDA）
  * (b) 初步數據分析
  * (c) 預測因子表達的第一稿
  * (d) 模型調優（超參數調整）
  * (e) 模型評估
  * (f) 對模型結果進行EDA
  * (g) 再一輪特徵工程
  * (h) 更廣泛的模型調優
  * (i) 最終模型評估
  * (j) 最終模型選擇
* 建模過程不僅僅是擬合單一數學模型，還包含多個反饋循環

### 模型偏差與變異 (Model Bias and Variance)
* **變異**（Variance）：模型參數因數據微小變化而改變的程度
  * 低變異模型：線性迴歸、邏輯迴歸、偏最小平方法
  * 高變異模型：決策樹、最近鄰模型、神經網路
* **偏差**（Bias）：模型符合數據基本結構的能力
  * 高偏差模型：線性方法（無法描述非線性模式）
  * 低偏差模型：決策樹、支持向量機、神經網路
* 變異-偏差權衡：低偏差模型傾向於高變異，反之亦然
* 圖1.5展示非線性數據集被兩種方法擬合：
  * 三點移動平均（綠色）：高變異但能較好跟蹤數據趨勢
  * 二次迴歸（紫色）：低變異但不能準確擬合非線性趨勢
* 圖1.6顯示這兩種模型在加入隨機噪聲後的表現：
  * 移動平均：預測有噪聲但平均而言能跟蹤數據模式
  * 二次模型：不受額外噪聲干擾但生成相似（且不準確）的擬合

### 經驗驅動與數據驅動建模 (Experience-Driven and Empirically Driven Modeling)
* **經驗驅動建模**：利用主題專家知識確定變數及其表示方式
* **數據驅動建模**：讓數據決定哪些預測因子及表示方法應被使用
* 數據驅動方法的風險：
  * 過擬合到數據中的虛假模式
  * 可能產生高度複雜且沒有明顯合理解釋的模型
* 最佳方法：結合兩種方法
  * 專家對新特徵更有信心，前提是發現方法嚴謹
  * 數據分析師從專家建議中受益，尤其是在初步篩選或優先順序預測因子時

### 大數據 (Big Data)
* 有效樣本量可能小於實際數據量（例如，嚴重類別不平衡或稀有事件率）
* 大數據的潛在缺點：
  * 如果預測因子與結果間無關係，增加數據量無法解決問題
  * 對複雜模型產生計算挑戰，計算時間可能隨數據量非線性增加
  * 並非所有模型都能從大數據中獲益（如高偏差、低變異模型）
* 可有效利用大數據的模型包括利用未標記數據的模型
* 面對大數據的關鍵問題：
  * 你用它來做什麼？它是否解決了某個未滿足的需求？
  * 它會造成障礙嗎？

  ## 1.2 更複雜的案例 (A More Complex Example)

* 本案例研究預測芝加哥「L」列車的乘客量（特定車站每日進站人數）
* 目的：幫助芝加哥交通管理局適當配置列車和車廂數量

### 案例分析過程 (Analysis Process)

* **初始預測變數集（Set 1）**：
  - 簡單易計算的四個預測因子
  - 與乘客量有強關聯性（通過視覺化確認）
  - 使用**均方根誤差**（Root Mean Squared Error, RMSE）評估模型
  - RMSE值範圍在2331至3248人次/日
  - 樹狀模型表現最佳，線性模型表現最差
  - 同類型模型間RMSE變異很小

* **第二組預測變數（Set 2）**：
  - 增加128個數值型預測變數（不同車站乘客量的滯後版本）
  - 例如：使用今天的乘客量預測一週後的乘客量（七天滯後）
  - 整體效果良好，對線性模型特別有幫助
  - 不同模型和模型類型獲益程度不同

* **第三組預測變數（Set 3）**：
  - 新增8至14天的滯後變數
  - 許多變數與其他預測因子有強相關性
  - 與前一組模型相比，沒有明顯改善，某些甚至更差
  - 某些線性模型因變數間高度相關（**多重共線性**，Multicollinearity）而表現下降
  - 此組滯後變數未顯示總體收益，未進一步考慮

* **第四組預測變數（Set 4）**：
  - 增加18個天氣狀況相關預測因子
  - 與第一、第二組一起使用
  - 天氣變數對預測列車乘客量沒有相關性

* **第五組預測變數（Set 5）**：
  - 開發49個二元預測變數，針對當前最佳模型表現不佳的日子
  - 基於殘差圖的探索性數據分析結果
  - 模型誤差大幅下降
  - 使用第一、第二和第五組特徵集，簡單線性模型的結果與更複雜的模型技術相當

### 關鍵結論 (Key Takeaways)

1. 建模過程幾乎不可能通過單一模型或特徵集立即解決問題，更像是一場反覆試錯的**戰役**（Campaign）
2. 特徵集的影響**可能**遠大於不同模型的影響
3. 模型與特徵之間的相互作用複雜且難以預測
4. 使用正確的預測變數集，多種不同類型的模型可達到相同性能水平
   - 初期表現最差的線性模型，最終展現了最佳性能

## 1.4 特徵選擇 (Feature Selection)

* 前例中，新特徵集被依序衍生以改善模型性能
  - 開發特徵集、加入模型，再用**重抽樣**（Resampling）評估其效用
  - 新預測變數事先未經篩選統計顯著性，這屬於**監督式程序**（Supervised Procedure）
  - 需注意避免過擬合

### 特徵選擇的必要性 (The Necessity of Feature Selection)

* 前例特徵集（如集合1、2和5）能充分預測結果，但可能包含：
  - 非資訊性變數（影響性能）
  - 重要變數被埋沒在集合3和4的非資訊性變數中

* 特徵選擇方法應用場景：
  - **連續法**：當新預測變數逐步加入模型時
  - **非連續法**：所有原始預測變數在建模過程開始時即已知且可用

### 監督式特徵選擇策略 (Supervised Feature Selection Strategies)

* 根據子集衍生方式區分搜尋方法：

* **包裹方法**（Wrapper Methods）
  - 使用外部搜尋程序選擇不同的預測變數子集進行評估
  - 特徵搜尋過程與模型擬合過程分離
  - 例子：向後或逐步選擇、**基因演算法**（Genetic Algorithms）

* **嵌入方法**（Embedded Methods）
  - 特徵選擇程序自然發生於模型擬合過程中
  - 例子：簡單**決策樹**（Decision Tree）
  - 變數在模型用於分割時被選擇
  - 如果預測變數從未用於分割，預測方程與該變數無關，則被排除

* 主要風險：過擬合
  - 使用包裹方法時尤其如此
  - 當訓練集數據點數量相對於預測變數數量較小時風險增加

### 非監督式選擇方法 (Unsupervised Selection Methods)

* 非監督式選擇方法可對模型性能產生積極影響
* 例如Ames住房數據的鄰里變數：
  - 鄰里變數有28個可能值，轉換為27個二元變數
  - 其中2個鄰里只有一或兩個屬性，占總體不到1%
  - 這種低頻率預測變數可能對某些模型（如線性迴歸）產生有害影響
  - 建議在建模前移除這些變數

### 變數子集搜尋的考量 (Considerations for Variable Subset Search)

* 可能不存在唯一的最佳性能預測變數集
* 存在補償效應：
  - 移除某個看似重要的變數時，模型使用剩餘變數進行調整
  - 在解釋變數之間存在相關性或使用低偏差模型時尤其明顯

* 特徵選擇不應作為確定特徵顯著性的正式方法
* 對評估預測變數對基礎模型或數據集的貢獻：
  - 建議使用更傳統的推論統計方法

  ## 1.5 本書大綱 (An Outline of the Book)

* 本書目標：提供有效工具來發現相關且具預測性的預測變數表達方式
* 這些工具將成為預測建模過程的兩端：
  - 過程開始：探索增強預測變數集的技術
  - 過程結束：提供篩選增強預測變數集的方法，以產生更好的模型

### 各章節內容概述 (Chapter Overview)

* **第2章**：模型與特徵工程過程互動的簡短說明
  - 使用特徵工程和特徵選擇方法改善預測缺血性中風風險的模型能力

* **第3章**：預測模型開發過程回顧
  - 資料分割、驗證方法選擇、模型調優、未來預測性能估計
  - 在多個模型的模型建構過程中使用反饋循環的指導

* **第4章**：資料探索性視覺化
  - 理解預測變數之間以及預測變數與響應變數之間關係的視覺化技術
  - 評估個別預測變數特性（偏度、缺失值模式）的視覺化方法
  - 評估模型擬合不良的圖形方法

* **第5章**：**類別型預測變數**（Categorical Predictors）編碼方法
  - 表示類別型預測變數的標準技術
  - 類別型預測變數的特徵工程方法，如**特徵雜湊**（Feature Hashing）
  - 處理類別型預測變數中罕見水平的實用問題
  - 為樹狀和規則型模型創建**虛擬變數**（Dummy Variables）的影響
  - 基於日期的預測變數處理（可視為類別型預測變數）

* **第6章**：**數值型預測變數**（Numeric Predictors）工程
  - 單變量和多變量轉換作為尋找更好形式的第一步
  - 使用**基底展開**（Basis Expansions）如樣條函數（Splines）創建更好的表示
  - 將連續預測變數轉換為類別或序數箱，以減少變異並提高性能
  - 數值預測變數分箱的注意事項

* **第7章**：**交互作用效應**（Interaction Effects）檢測
  - 探索兩個或多個原始預測變數之間交互作用的重要性
  - 確定預測變數之間交互作用的量化工具
  - 評估這些效應重要性的圖形方法
  - 交互作用可估計性的概念探討

* **第8章**：處理**缺失數據**（Missing Data）
  - 探索缺失數據的機制
  - 調查缺失數據模式的視覺化工具
  - 移除或插補缺失數據的傳統和現代工具
  - 評估連續和類別型預測變數的插補方法

* **第9章**：處理**剖面數據**（Profile Data）
  - 處理時間序列（縱向）、細胞-孔板、圖像等特殊結構數據
  - 金融、製藥、情報、交通和天氣預報領域常見的數據類型
  - 介紹**偏最小平方法**（Partial Least Squares）等自然處理此類數據的工具
  - 在建模前總結或壓縮剖面數據的技術

* **第10-12章**：**特徵選擇**（Feature Selection）策略
  - 特徵選擇的目標
  - 無關預測變數的後果
  - 與通過**正則化**（Regularization）進行選擇的比較
  - 如何避免特徵選擇過程中的過擬合

  # 2 說明性案例：預測缺血性中風風險 (Illustrative Example: Predicting Risk of Ischemic Stroke)

* 本章節作為特徵工程的入門，展示類似圖1.4所示的建模過程
* 為說明目的，本例將聚焦於探索、分析擬合和特徵工程，通過單一模型（邏輯迴歸）視角

## 研究背景與假設 (Background and Hypothesis)

* 傳統上，動脈狹窄（阻塞）程度用於識別中風風險患者
* 阻塞充分（>70%）的患者通常建議進行手術干預
* 歷史證據表明阻塞程度本身實際上是未來中風的較差預測因子
* 理論認為：同等大小的阻塞，**斑塊阻塞的組成**也與中風風險相關
  - 大型但穩定且不易破裂的斑塊可能比小型但不穩定的斑塊風險更低

## 研究數據集 (Dataset)

* 選擇了126位具有不同程度頸動脈阻塞的患者歷史數據
  - 其中44位患者阻塞大於70%
* 所有患者接受了**電腦斷層血管造影**（Computed Tomography Angiography, CTA）
* 使用Elucid Bioimaging的vascuCAP™軟件分析圖像，生成解剖結構估計：
  - 狹窄百分比
  - 動脈壁厚度
  - 組織特性（如富含脂質的壞死核心和鈣化）

## 圖像特徵與測量 (Imaging Features and Measurements)

* **圖2.1(a)**：表示嚴重狹窄的頸動脈
  - 軟件可計算面積（MaxStenosisByArea）和直徑（MaxStenosisByDiameter）的最大橫截面狹窄
  - 灰色區域代表提供動脈壁結構支持的大分子，可通過面積量化（MATXArea）

* **圖2.1(b)**：顯示嚴重狹窄和鈣化斑塊（綠色）及富含脂質的壞死核心（黃色）
  - 斑塊和富含脂質的壞死核心被認為會增加中風風險
  - 可通過體積（CALCVol和LRNCVol）和最大橫截面積（MaxCALCArea和MaxLRNCArea）量化

* **圖2.1(c)**：顯示嚴重狹窄和向外的動脈壁生長
  - 頂部箭頭表示最大狹窄的橫截面
  - 底部箭頭表示最大正壁重塑的橫截面
  - **重塑比例**（Remodeling Ratio）：測量動脈壁，比例小於1表示壁收縮，大於1表示壁生長
  - 大比例的冠狀動脈與破裂相關

## 阻塞與中風結果 (Blockage and Stroke Outcome)

* 表2.1顯示阻塞分類與中風結果的關聯
* 基於卡方關聯檢驗（p = 0.42），關聯不具統計顯著性
* 表明阻塞分類本身可能不是中風結果的良好預測因子

## 研究目標與額外數據 (Study Objectives and Additional Data)

* 如果斑塊特性對評估中風風險重要，成像軟件提供的測量可能有助於改善中風預測
* 改善預測能力可幫助醫生做出更好的患者管理或臨床干預決策：
  - 有大型（阻塞>70%）但穩定斑塊的患者可能不需要手術干預
  - 有較小但不穩定斑塊的患者可能需要手術干預或更積極的藥物治療

* 每位患者的數據還包括常見的臨床特徵：
  - 是否有心房顫動
  - 冠狀動脈疾病
  - 吸煙史
  - 性別和年齡等人口統計學資料

## 分析方法 (Analysis Approach)

* 將訓練模型使用風險預測因子、成像預測因子及兩者組合
* 探索這些特徵的其他表示方式，以提取有益的預測信息
* 首先評估容易收集的臨床風險因子（不需要昂貴的成像技術）

## 2.1 資料分割 (Splitting)

* 在建立模型前，需將數據分割為兩部分：
  - **訓練集**（Training Set）：用於開發模型、預處理預測變數和探索關係
  - **測試集**（Test Set）：作為預測變數集/模型組合性能的最終評判

* **分層**（Stratified）分割方法：
  - 在每個結果類別中進行隨機分割
  - 保持中風患者的比例大致相同（見表2.2）
  - 原始數據集的70%分配給訓練集

* **表2.2：按訓練和測試分割的中風結果分布**
  | 數據集 | 中風 = 是 (n) | 中風 = 否 (n) |
  | ------ | ------------- | ------------- |
  | 訓練集 | 51% (45)      | 49% (44)      |
  | 測試集 | 51% (19)      | 49% (18)      |

* 分層抽樣的優點：
  - 確保訓練和測試集具有相似的結果分布
  - 減少樣本偏差的風險
  - 提高最終模型評估的可靠性

  ## 2.2 預處理 (Preprocessing)

* 建模過程的首要步驟是了解預測變數的重要特性：
  - 個別分佈
  - 每個預測變數中缺失值的程度
  - 預測變數中潛在的異常值
  - 預測變數之間的關係
  - 每個預測變數與結果變數的關係

* 當預測變數數量增加時，仔細檢視每個變數的能力迅速下降
  - 自動化工具和視覺化可實施良好實踐
  - 參考 Kuhn (2008) 和 Wickham and Grolemund (2016)

### 處理缺失值 (Handling Missing Values)

* 數據集中僅有4個缺失值（所有受試者和預測變數）
* 許多模型無法容忍任何缺失值，因此必須採取行動
* 使用**中位數插補**（Median Imputation）替換缺失值：
  - 簡單、無偏的方法
  - 適用於相對少量的缺失（但非最佳方案）
  - 更多插補技術將在第8章討論

### 預測變數的探索 (Exploration of Predictors)

* 數據集夠小，可進行手動探索
* 成像預測變數的單變量探索發現許多有趣特性：
  - 預測變數進行了**均值中心化**和**單位方差縮放**以便直接視覺比較
  - 許多成像預測變數具有長尾分佈（正偏分佈）
  
* 範例：富含脂質壞死核心的最大橫截面積（MaxLRNCArea）
  - 圖2.2a 顯示原始偏斜分佈
  - 初看可能認為偏度和高度異常值是由少數患者引起
  - 但偏度通常是數據基本分佈的結果，不是異常值

* 解決方案：數據轉換
  - 簡單的**對數轉換**（Log-transformation）
  - 或更複雜的**Box-Cox**或**Yeo-Johnson轉換**（第6.1節）
  - 使數據在近似對稱的尺度上，消除異常值的外觀（圖2.2b）
  - 對於指數增長的測量特別適用
  - 脂質面積自然地乘法增長（面積計算的定義）

### 處理高相關性預測變數 (Handling Correlated Predictors)

* 移除與其他預測變數高度相關（r² > 0.9）的預測變數
* 圖2.3熱圖顯示成像預測變數之間的相關性
  - 列和行順序由聚類算法決定
  - 三對預測變數顯示不可接受的高相關性（紅色方框）：
    1. 血管壁體積（WallVol）和基質體積（MATXVol）
    2. 最大橫截面壁面積（MaxWallArea）和最大基質面積（MaxMATXArea）
    3. 基於面積的最大橫截面狹窄（MaxStenosisByArea）和基於直徑的最大橫截面狹窄（MaxStenosisByDiameter）
  
* 對最後一對高相關的理解：面積計算是直徑的函數
* 其他接近閾值的中度高相關變數對：
  - 鈣化體積（CALCVol）和最大橫截面鈣化面積（MaxCALCArea）(r = 0.87)
  - 富含脂質壞死核心的最大橫截面積（MaxLRNCArea）和富含脂質壞死核心的體積（LRNCVol）(r = 0.8)

* 相關性閾值是任意的，可能需要根據問題和使用的模型調整
  - 第3章包含更多關於此方法的詳細信息

  ## 2.3 探索 (Exploration)

* 下一步是探索潛在的預測關係：
  - 個別預測變數與結果之間的關係
  - 預測變數對之間與結果的關係

* 確保不過度解釋小數據集中的趨勢：
  - 使用**重抽樣**（Resampling）技術
  - 採用**重複10折交叉驗證**（Repeated 10-fold Cross-validation）
  - 創建訓練集的50個變種，用於評估所有分析
  - 提供防止過擬合的保護機制

### 模型比較方法論 (Model Comparison Methodology)

* 傳統方法缺點：在整個數據集上進行統計假設檢驗，可能導致過擬合
* 替代方法：比較兩個模型（M₁和M₂）的演算法（圖像1）：
  1. 對每個重抽樣樣本，使用90%擬合兩個模型
  2. 用兩個模型預測剩餘10%
  3. 計算兩個模型的ROC曲線下面積
  4. 確定兩個AUC值的差異
  5. 使用單側t檢定測試M₂是否優於M₁
  
* 90%和10%數值源於使用10折交叉驗證

### 風險預測變數分析 (Risk Predictors Analysis)

* 比較兩個邏輯迴歸模型：
  - 簡單模型（僅包含截距項）
  - 複雜模型（包含風險集中的單個預測變數）
  
* 表2.3（圖像2）顯示每個風險預測變數相對於空模型的ROC改善
* 多個預測變數提供顯著但有限的改善：
  - **冠狀動脈疾病**（CoronaryArteryDisease）：0.079改善，p=0.0003
  - **糖尿病病史**（DiabetesHistory）：0.066改善，p=0.0003
  - **高血壓病史**（HypertensionHistory）：0.065改善，p=0.0004
  - **年齡**（Age）：0.083改善，p=0.0011
  - **心房顫動**（AtrialFibrillation）：0.044改善，p=0.0013

### 成像預測變數分析 (Imaging Predictors Analysis)

* 圖2.4（圖像3）展示每個成像預測變數與中風結果的散點圖
* 與中風結果關聯最強的變數：
  - 目標所有橫截面中的最厚壁厚（MaxMaxWallThickness）
  - 最大橫截面壁重塑比率（MaxRemodelingRatio）
  
* MaxRemodelingRatio分析：
  - 中風類別間平均值有顯著差異
  - 但分布仍有相當重疊
  - 圖2.5（圖像4）顯示其ROC曲線
  - 顯示有一定信號，但可能不足以作為單獨的預後工具

### 交互作用探索 (Interaction Exploration)

* 探索預測變數之間的配對交互作用：
  - 數值預測變數的交互作用通過乘法生成
  - 從成像預測變數對中創建了171個潛在交互作用項
  
* 圖2.6（圖像5）展示：
  - x軸：交互作用項導致的ROC改善
  - y軸：改善的負對數p值（值越大越顯著）
  - 點的大小：主效應模型的基線ROC曲線下面積
  
* 171個交互作用項中，18個提供了相對於主效應的改善（p < 0.2）

* MaxRemodelingRatio與MaxStenosisByArea的交互作用：
  - 圖2.7（圖像6）面板(a)：兩個預測變數的散點圖，按中風結果著色
  - 等值線代表兩個預測變數之間的等效乘積值
  - 未發生中風的患者通常具有較低的乘積值
  - 發生中風的患者通常具有較高的乘積值
  - 實際意義：顯著阻塞結合血管壁向外生長增加中風風險
  - 面板(b)：這種交互作用的箱形圖顯示中風結果類別之間的分離比任一單獨預測變數更強

## 2.4 跨集預測建模 (Predictive Modeling Across Sets)

* 此階段可探索至少五種預測變數組合的預測能力：
  - 僅原始風險集
  - 僅成像預測變數
  - 風險和成像預測變數結合
  - 成像預測變數及其交互作用
  - 風險、成像預測變數及其交互作用

### 模型選擇與考量 (Model Selection and Considerations)

* 醫師偏好**邏輯迴歸**（Logistic Regression）模型：
  - 具有內在可解釋性
  - 屬於高偏差、低變異模型
  - 預測性能往往低於低偏差、高變異模型
  - 包含相關或非資訊性預測變數會降低其性能
  - 需識別最相關預測變數來找到最佳子集

### 特徵選擇方法論 (Feature Selection Methodology)

* 使用**遞迴特徵消除**（Recursive Feature Elimination, RFE）確定最佳預測變數集：
  - 簡單的向後選擇程序
  - 初始使用最大模型，並對每個預測變數按重要性排名
  - 使用迴歸係數絕對值判斷重要性（在預測變數被中心化和縮放後）
  - 逐步移除最不重要的預測變數，重新擬合模型並評估性能
  - 每次模型擬合時，預測變數都經過Yeo-Johnson轉換、中心化和縮放

* 處理預測變數間相關性：
  - 相關性可能導致邏輯迴歸係數不穩定
  - 使用額外變數過濾器移除最小預測變數集，使無成對相關性大於0.75
  - 分析使用和不使用此步驟的數據預處理結果

* 結合重抽樣方案與RFE過程：
  - 向後選擇在訓練集的50個不同90%子集上執行
  - 使用剩餘10%評估移除預測變數的效果
  - 在整個訓練集上執行最終RFE，並在最佳大小處停止
  - 目的是減少小數據集過擬合風險
  - 所有預處理步驟在重抽樣步驟內進行，相關性過濾器可能為每個重抽樣選擇不同變數

### RFE應用於不同預測變數集 (RFE Application to Different Predictor Sets)

* 風險預測變數集（8個預測變數）：
  - 僅考慮主效應時，偏好完整8個預測變數集
  - 加入所有28個配對交互作用時，額外交互作用損害模型性能
  - 基於重抽樣，13個預測變數集最佳（其中11個為交互作用）
  - 應用相關性過濾器時，主效應模型不受影響，交互作用模型最多18個預測變數
  - 過濾器對此預測變數集無幫助

* 成像預測變數集（19個預測變數）：
  - 數據集偏好沒有之前發現的交互作用且使用相關性過濾器的模型
  - 似乎反直覺，但非交互作用項可能替代或補償最重要交互作用項提供的信息
  - 迄今為止最佳模型基於7個經過過濾的成像主效應

* 所有預測變數組合：
  - 不使用相關性過濾器時，性能一般，交互作用和主效應模型無明顯差異
  - 應用過濾器後，數據強烈偏好主效應模型（包含所有10個通過相關性過濾的預測變數）

### 最終模型與評估 (Final Model and Evaluation)

* 最終選擇的預測變數集（7個成像預測變數）：
  - MaxLRNCArea
  - MaxLRNCAreaProp
  - MaxMaxWallThickness
  - MaxRemodelingRatio
  - MaxCALCAreaProp
  - MaxStenosisByArea
  - MaxMATXArea

* 表2.4（圖像2）顯示在所有50個重抽樣中，7變數模型中預測變數的選擇頻率：
  - 選擇結果相當一致，特別考慮到訓練集很小
  - 前四個變數（MaxLRNCAreaProp、MaxMaxWallThickness、MaxCALCAreaProp、MaxRemodelingRatio）被選中頻率最高

* 測試集表現：
  - ROC曲線下面積為0.69
  - 低於重抽樣估計的0.72
  - 但高於估計的90%下限（0.674）

## 2.5 其他考量 (Other Considerations)

* 本章所呈現的方法並非處理這些數據的唯一方法
* 存在多種替代方法和值得探討的問題：

### 替代建模方法 (Alternative Modeling Approaches)

* 例如，如果評估邏輯迴歸，可以考慮`glmnet`模型（Hastie, Tibshirani, and Wainwright 2015）
  - 將特徵選擇整合到邏輯迴歸擬合過程中

### 未探索的方向與潛在問題 (Unexplored Directions and Potential Questions)

* 合理的疑問包括：
  - 為何僅預處理成像預測變數？
  - 為何未探索風險預測變數之間或風險預測變數與成像預測變數之間的交互作用？
  - 為何在原始預測變數而非預處理後的預測變數上構建交互作用項？
  - 為何未採用不同的建模技術或特徵選擇程序？

### 替代方法的可能性 (Possibilities with Alternative Approaches)

* 可能有不同的預處理方法、不同的預測變數組合或不同的建模技術能帶來更好的預測性
* 不同的方法可能產生不同的結果和洞見

### 核心要點 (Key Takeaway)

* 本章主要目的：說明花更多時間（有時是大量時間）調查預測變數及其關係可以幫助改善模型預測性
* 這在預測性能的邊際改善能帶來顯著收益時尤為重要
* 特徵工程和選擇過程的投資通常會產生實質性回報

# 3 預測建模過程回顧 (A Review of the Predictive Modeling Process)

* 在深入探討特定的建模方法和技術之前，需要先討論和定義一些必要的主題
* 這些主題對於實證建模而言是相當普遍的，包括：
  - 回歸和分類問題的性能測量指標
  - 最佳數據使用方法（包括數據分割和重抽樣）
  - 模型調優的最佳實踐
  - 比較模型性能的建議

## 章節使用的數據集 (Datasets Used in This Chapter)

* 本章使用兩個數據集來說明相關技術：

1. **艾姆斯房價數據**（Ames Housing Price Data）
   - 首次在第1章介紹
   - 用於回歸問題示例

2. **在線約會網站職業分類數據**（Online Dating Site Profession Classification）
   - 基於在線約會網站信息對人的職業進行分類
   - 用於分類問題示例
   - 將在下一節詳細討論

## 3.1 說明性案例：OkCupid個人資料數據 (Illustrative Example: OkCupid Profile Data)

* **OkCupid**是一個服務國際用戶的在線約會網站
* Kim和Escobedo-Land（2015）描述了一個來自舊金山地區超過50,000個個人資料的數據集
* 數據可在`GitHub`存儲庫中找到

### 數據類型 (Data Types)

* 數據包含幾種類型的變數：
  - **開放式文本**：與個人興趣和個人描述相關的文章
  - **單選欄位**：如職業、飲食習慣和教育程度
  - **多選欄位**：如所說語言和編程語言熟練度

* 原始形式中，幾乎所有數據欄位本質上都是離散的：
  - 僅年齡和上次登錄以來的時間為數值型
  - 分類預測變數在擬合模型前轉換為**虛擬變數**（Dummy Variables）（第5章）

### 預測變數結構 (Predictor Structure)

* 本章分析中將忽略開放式文本數據（將在5.6節探討）
* 使用的212個預測變數中包含以下變數群集：
  - 地理位置（鎮，p=3）
  - 宗教信仰（p=13）
  - 星座（p=15）
  - 孩子（p=15）
  - 寵物（p=15）
  - 收入（p=12）
  - 教育（p=32）
  - 飲食習慣（p=17）
  - 超過50個與口語相關的變數

* 有關數據集及其處理方式的更多信息，請參閱該書的GitHub存儲庫

### 預測目標與數據平衡 (Prediction Goal and Data Balance)

* 目標：預測一個人的職業是否屬於**STEM領域**（科學、技術、工程和數學）
* 數據存在中度類別不平衡：
  - 僅18.5%的個人資料從事這些領域工作
  - 雖然不平衡對分析有顯著影響，但本例將主要通過**下採樣**（Down-sampling）規避此問題
  - 下採樣使每個類別的個人資料數量相等

* 關於處理罕見類別的技術詳細描述，請參閱Kuhn和Johnson（2013）第16章

## 3.2 績效測量 (Measuring Performance)

* 雖然常被忽視，但用於評估模型預測效能的指標非常重要，並可能影響結論
* 選擇的指標取決於結果變數類型

### 3.2.1 迴歸指標 (Regression Metrics)

* **均方根誤差**（Root Mean Squared Error, RMSE）
  - 計算方法：計算觀測值與預測值之間差異（殘差）的平方平均值，然後取平方根
  - 優點：結果以原始測量單位表示
  - 解釋：樣本從其觀測值到預測值的平均距離
  - 規則：RMSE越低，模型預測能力越佳

* **決定係數**（Coefficient of Determination, R²）
  - 計算方法：觀測值與預測值之間的標準相關係數的平方
  - 優點：對線性模型有直觀解釋：可被模型解釋的結果總變異比例
  - 特點：接近1.0表示幾乎完美擬合，接近零表示預測與結果無線性關聯
  - 優勢：無單位，便於比較不同結果

* **R²的問題**：
  - 主要問題：是相關性而非準確性的度量
  - 模型可能與觀測值有強線性關係，但不符合45度一致線
  - 現象舉例：樹狀集成方法（如隨機森林、提升樹等）在結果一端低估而另一端高估
  - 當結果變異大時，可能顯示過於樂觀的結果
  - 少數離群值可能人為增加R²值

* **圖3.1**顯示R²問題：
  - 芝加哥列車乘客數據模型R²估計為0.9，表面看似極佳
  - 但高值主要由於乘客數工作日高、週末低的固有雙峰性質
  - 雙峰結果增加結果變異，進而增加R²
  - 部分殘差大於10,000人次，RMSE為3,853人次
  - 藍線（觀測值與預測值的線性迴歸擬合）與黑線（一致線）比較顯示模型在小值低估、大值高估

* **一致相關係數**（Concordance Correlation Coefficient, CCC）
  - Lawrence和Lin（1989）開發，解決相關係數過於樂觀的問題
  - 定義：普通相關係數與一致線偏差測量的乘積
  - 偏差係數範圍0至1，1表示數據落在一致線上
  - 可視為帶懲罰的相關係數
  - 數據與一致線偏離越遠，偏差係數越小

* **穩健指標**（Robust Metrics）
  - RMSE和R²對極端值非常敏感（基於殘差平方）
  - 穩健技術尋找數據多數部分的數值摘要
  - 方法：降低極端樣本權重或轉換原始值
  - 範例：等級相關、中位數絕對偏差（MAD）、絕對誤差

### 3.2.2 分類指標 (Classification Metrics)

* 兩種類型：
  - 基於定性類別預測（如STEM或其他）
  - 使用預測類別機率（如Pr[STEM]=0.254）

* **混淆矩陣**（Confusion Matrix）
  - 觀測類別與預測類別的簡單交叉表格
  - 表3.1顯示OkCupid數據的混淆矩陣
  - 對角線上為正確預測樣本

* **分類準確度**（Classification Accuracy）
  - 定義：正確預測結果的比例
  - 例子：0.78 = (5134 + 25257)/(5134 + 6385 + 2033 + 25257)
  - 問題：類別不平衡時可能產生誤導（如預測全部非STEM可達0.82）

* **科恩卡帕係數**（Cohen's Kappa）
  - 考慮類別不平衡，將錯誤率標準化為預期偶然發生的錯誤率
  - 值範圍-1至1，1表示完全一致，接近零表示無關聯
  - 可推廣至多類別問題

* **馬賽克圖**（Mosaic Plot）
  - 用於可視化混淆矩陣（圖3.3）
  - 每個單元格表示為面積與數值成比例的矩形

* **二分類特殊指標**：

  * 敏感度（Sensitivity）/真陽性率：
    - 定義：正確預測的事件比例
    - 計算：5,134/7,167 = 0.716
    - 如果檔案確實是STEM，被正確預測的機率是多少？

  * 特異度（Specificity）：
    - 定義：正確預測非事件的比例
    - 計算：25,257/31,642 = 0.798
    - 假陽性率 = 1 - 特異度（0.202）

  * 精確度（Precision）：
    - 定義：所有預測事件中正確預測的比例
    - 計算：5,134/11,519 = 0.446
    - 如果檔案被預測為STEM，實際是STEM的機率是多少？

* **條件統計與貝葉斯法則**：
  - 敏感度、特異度、精確度均為條件統計量
  - 真正想知道的：Pr[Y = STEM|P = STEM]
  - 貝葉斯法則：Pr[Y|P] = Pr[Y] × Pr[P|Y] / Pr[P] = 先驗 × 似然 / 證據

* **陽性預測值**（Positive Predictive Value, PPV）和**陰性預測值**（Negative Predictive Value, NPV）：
  - PPV：無條件敏感度類比
  - NPV：無條件特異度類比
  - 使用盛行率（先驗機率）進行計算
  - OkCupid資料的PPV = 0.45，NPV = 0.93（假設盛行率0.18）
  - 若全美STEM盛行率為5%，則PPV = 0.16，NPV = 0.98

* **基於機率的指標**：

  * **二項式對數似然統計量**（Binomial Log-likelihood Statistic）：
    - 計算公式：log ℓ = ∑ᵢ₌₁ⁿ ∑ⱼ₌₁ᶜ yᵢⱼlog(pᵢⱼ)
    - 目標：最大化對數似然
    - 當所有樣本以高機率被正確預測時達到最大值

  * **基尼準則**（Gini Criterion）：
    - 計算公式：G = ∑ᵢ₌₁ⁿ ∑ⱼ≠ⱼ′ pᵢⱼpᵢⱼ′
    - 類別機率變異或不純度的測量
    - 應最小化

  * **熵**（Entropy）：
    - 計算公式：H = -∑ᵢ₌₁ⁿ ∑ⱼ₌₁ᶜ pᵢⱼlog₂pᵢⱼ
    - 類別機率變異或不純度的測量
    - 應最小化

* **表3.2**比較：
  - 對數似然使用真實類別信息，以監督方式懲罰不良模型
  - 基尼和熵只懲罰不確定模型（產生大致相等的類別機率）
  - 好模型與壞模型在基尼和熵上值相等，但在對數似然上差異明顯

* **ROC曲線**（圖3.4a）：
  - 考慮所有可能閾值，追蹤敏感度和特異度的變化
  - 繪製假陽性率（1-特異度）與真陽性率
  - 最佳模型緊貼y軸直接前往左上角
  - ROC曲線下面積（AUC）：1為完美，約0.5為無效
  - OkCupid數據AUC為0.839，表示適中良好擬合

* **精確度-召回曲線**（圖3.4b）：
  - 從信息檢索角度更適合
  - 貧弱模型的曲線接近觀測盛行率（此處為0.18）的水平線
  - 曲線下面積用於總結模型性能，最佳值為1.0，最差為盛行率
  - OkCupid數據的曲線下面積為0.603

### 3.2.3 特定情境指標 (Context-Specific Metrics)

* 前述指標可用於開發有效模型，但可能無法回答核心問題
* 例如：廣告點擊預測模型可能需要回答「使用此模型公司將賺多少錢？」
* 貸款例子：可計算預期利潤或損失
* 應讓感興趣的問題引導選擇適當的評估指標
* 可能使用現有通用指標，或需開發特定情境的自定義指標

## 3.3 數據分割 (Data Splitting)

* 建模專案起始的首要決策之一是如何利用現有數據
* 常見技術：將數據分為兩組，稱為**訓練集**（Training Set）和**測試集**（Testing Set）

### 訓練集與測試集的用途 (Purpose of Training and Testing Sets)

* **訓練集**：
  - 用於開發模型和特徵集
  - 作為估計參數的基礎
  - 比較模型和進行其他達成最終模型所需的活動

* **測試集**：
  - 僅在結論階段使用
  - 用於估計模型性能的最終、無偏評估
  - 關鍵點：在此之前不應使用測試集
  - 提前查看測試集結果會產生偏差，因為測試數據將成為模型開發過程的一部分

### 測試集大小考量 (Test Set Size Considerations)

* 很難制定統一的指導方針
* 影響因素包括：
  - 原始樣本池的大小
  - 預測變數的總數量
  - 樣本數（n）與預測變數數（p）的比率
  
* 當樣本池較大時：
  - 一旦訓練集包含「足夠」樣本，分割決策的重要性降低
  - 簡單初始分割的替代方案可能是好主意（見3.4.7節）

* 樣本數與預測變數數比率（n:p）很重要：
  - n遠大於p時，分割數據更有彈性
  - 當n小於p時，即使n看似較大，仍可能遇到建模困難

### 數據分割方法 (Data Splitting Methods)

* **完全隨機抽樣**（Completely Random Sampling）：
  - 最常見的方法
  - 直接實施且通常避免對數據特性產生偏差
  - 問題：當結果變數分布不均勻時可能有問題

* **分層隨機抽樣**（Stratified Random Sampling）：
  - 基於結果變數進行
  - 分類模型：在每個類別內隨機選擇樣本
  - 確保結果變數的頻率分布在訓練集和測試集中大致相等
  - 數值結果：可基於四分位數構建人工分層
    * 例如：艾姆斯房價數據可分為四組，每組約230個房屋
    * 在四個組內進行訓練/測試分割，然後合併

* **非隨機抽樣**（Non-random Sampling）：
  - 當有充分理由時使用
  - 例如：數據具有重要時間特性時
  - 可能明智的做法是使用最新數據作為測試集
  - 應用案例：第4.1節討論的芝加哥交通數據

## 3.4 重抽樣 (Resampling)

* 重抽樣方法用於在不使用測試集的情況下評估模型效能
* 產生訓練集的不同版本，用於模擬模型在新數據上的表現

### 重抽樣的基本概念 (Basic Concepts of Resampling)

* 每種重抽樣技術產生數據的兩個子集：
  - **分析集**（Analysis Set）：用於建立模型
  - **評估集**（Assessment Set）：用於衡量性能
  - 類似於訓練集和測試集的概念（圖3.5）

* 重抽樣的基本單位是**獨立實驗單元**（Independent Experimental Unit）
  - 例如：艾姆斯數據每棟房屋都是獨立的
  - 若每個客戶有多行數據，則整個客戶及其所有數據一起分配

### 3.4.1 V折交叉驗證及其變體 (V-Fold Cross-Validation and Its Variants)

* **基本V折交叉驗證**：
  - 創建V個訓練集的不同版本，每個大小大致相同
  - 每個評估集包含訓練集的1/V，分析集包含其餘部分
  - 循環方式計算V個性能估計值，最終取平均值
  - 圖3.6展示20個樣本的10折交叉驗證

* **OkCupid數據案例**：
  - 使用分層10折交叉驗證
  - 訓練集38,809個檔案，每個評估集3,880個不同檔案
  - ROC曲線下面積範圍從0.83到0.854，平均值為0.839

* **重複V折交叉驗證**：
  - 減少基本V折交叉驗證的變異性
  - 使用R次重複，創建RV個重抽樣
  - 最終方差減少約√R

* **特殊變體**：
  - **留一交叉驗證**（Leave-one-out）：V等於訓練集大小，僅適用於極小訓練集
  - **分組V折交叉驗證**：按組（如客戶）而非單個觀測值進行重抽樣

### 3.4.2 蒙特卡洛交叉驗證 (Monte Carlo Cross-Validation)

* 與V折交叉驗證不同，評估集可能有重疊
* 每次重抽樣，π比例的訓練集進入分析集，剩餘樣本進入評估集
* 重複B次，取B次結果的平均值
* 圖3.6顯示10次重抽樣和π=0.90的蒙特卡洛交叉驗證

### 3.4.3 自助法 (The Bootstrap)

* 自助法重抽樣：與訓練集大小相同的**有放回**簡單隨機樣本
* 特點：
  - 訓練集成員在自助樣本中至少出現一次的機率為63.2%
  - 自助樣本作為分析集
  - 評估集（袋外樣本）由未包含在自助樣本中的訓練集成員組成
* 圖3.7顯示20個樣本數據集的10個自助樣本

### 3.4.4 滾動原點預測 (Rolling Origin Forecasting)

* 專門用於時間序列數據或具有強時間成分的數據集
* 適用場景：數據存在季節性或其他慢性趨勢
* 方法：
  - 第一個分析集由前M個訓練集點組成
  - 評估集包含接下來的N個訓練集樣本
  - 每次保持數據集大小相同，但增量分析集
* 圖3.8展示使用10個數據點進行分析，隨後2個樣本用於評估的重抽樣
* 變體：
  - 分析集大小可累積增長
  - 可跳過迭代以減少重抽樣次數
  - 對於不均勻抽樣的數據，可按時間增量而非數據集行增量移動

### 3.4.5 驗證集 (Validation Sets)

* 介於訓練集和測試集之間的概念
* 源自神經網絡領域，用於確定訓練中的錯誤率
* 通常是訓練集的小隨機子集，用於估計模型訓練時的性能
* 不能替代測試集（或評估集），因為驗證數據引導訓練過程

### 3.4.6 重抽樣的變異與偏差 (Variance and Bias in Resampling)

* **變異**（Variance）：
  - 衡量重複同一重抽樣方案多次時結果的分散程度
  - 固定訓練集大小和重抽樣次數，V折交叉驗證變異性最大，自助法變異性最小

* **偏差**（Bias）：
  - 重抽樣方案能夠接近真實性能參數的能力
  - 當分析集數據量減少時，重抽樣估計的偏差增加
  - 自助法偏差最大（約36.8%的訓練集用於評估），且通常是悲觀偏差
  - V折交叉驗證偏差較低，特別是V≥10時

* 圖3.9圖形表示了重抽樣方案的變異和偏差
* 建議：對於不大的訓練集，使用5次左右重複的10折交叉驗證

### 3.4.7 重抽樣應包含什麼？ (What Should Be Included Inside of Resampling?)

* 重抽樣必須包含建模過程中所有可能顯著影響模型效能的步驟：
  - 預測變數轉換
  - 預處理或過濾步驟
  - 特徵選擇方法
  
* 可豁免的操作：
  - 小範圍的中位數插補
  - 中心化和縮放
  
* **信息洩露**（Information Leakage）問題：
  - 定義：測試集數據直接或間接用於訓練過程
  - 導致過於樂觀的結果，無法在未來數據點上複製
  - 例如：時間序列移動平均平滑處理可能不經意引入測試集信息
  - 解決方法：僅使用訓練數據點開發預處理技術，然後應用於未來數據

* 大數據的替代數據使用方案：
  - 可創建多個分割用於特定目的
  - 例如：使用特定數據分割確定相關預測變數，再用訓練集建模
  - 可減少在重抽樣中包含昂貴特徵選擇步驟的需要

## 3.5 調整參數與過擬合 (Tuning Parameters and Overfitting)

* 許多模型包含**調整參數**（Tuning Parameters）或**超參數**（Hyperparameters）
  - 這些參數無法直接從數據估計
  - 對模型非常重要，通常控制模型複雜度
  - 影響模型的變異-偏差權衡

### K最近鄰模型範例 (K-Nearest Neighbor Example)

* **K最近鄰模型**（K-nearest Neighbor）工作原理：
  - 儲存訓練集數據
  - 預測新樣本時，找出K個距離最近的訓練集點
  - 使用這些鄰居的結果做出預測

* **K值的影響**：
  - 控制模型複雜度
  - 類似於第1.2.5節中移動平均討論，控制模型的變異和偏差
  - K很小時：過擬合風險最大，因為僅使用極少值進行預測，對數據變化最敏感
  - K過大時：使用過多潛在不相關的數據點進行預測，導致欠擬合

### 實際案例演示 (Practical Demonstration)

* 圖3.10展示艾姆斯一個測試集房屋樣本（藍色）與其訓練集中五個最近鄰居（紅色）
* 測試集樣本售價：$176,000
* 鄰居價格（從最近到最遠）：$175,000、$128,000、$100,000、$120,000、$125,000

* **不同K值的影響**：
  - K = 1：模型與真實房價相差$900（僅使用最近的$175,000鄰居）
    * 展示了第1.2.1節引入的過擬合概念
    * 模型過於激進地使用訓練集模式預測新數據點
  - K = 5：使用所有五點平均值預測，大幅減少誤差至$-46,400
    * 註：計算方式為(175K+128K+100K+120K+125K)/5 = 129.6K，與實際176K相比的誤差

### 調整參數的複雜性 (Complexity of Tuning Parameters)

* 一些模型可能有多個調整參數
* K最近鄰模型的其他可能參數：
  - 不同距離度量
  - 不同加權方案（使遠點對預測影響較小）

### 選擇最佳參數值 (Selecting Optimal Parameter Values)

* 需要搜索程序找到合適的調整參數值
* 需要獲得良好、可泛化的性能測量方法
* 重複使用測試集進行這些問題是有問題的（會失去其公正性）
* 通常使用**重抽樣**（Resampling）方法確定最佳參數值
* 下一節將描述確定這些類型參數最佳值的幾種方法

## 3.6 模型最佳化與調優 (Model Optimization and Tuning)

* 尋找最佳調整參數值的方法可分為兩大類：
  - 預先定義要評估的值
  - 增量確定值

### 網格搜索方法 (Grid Search)

* **網格搜索**（Grid Search）是最著名的預定義值評估程序：
  - 指定一組候選調整參數值並進行評估
  - 多個調整參數時，候選參數組合為多維的
  - 建議使用重抽樣評估每個不同參數值組合
  - 選擇「最佳」調整參數組合後，用整個訓練集擬合最終模型
  - 通常選擇實證結果最佳的候選方案

* **K最近鄰模型範例**（OkCupid資料）：
  - 模型需要確定鄰居數量K
  - 預定義候選集為K = 1, 3, ..., 201
  - 結合10折交叉驗證，使用380個臨時模型確定良好的K值
  - 圖3.11顯示K = 201時ROC曲線下面積最大（0.806）
  - 圖中每個黑點代表10個不同模型（使用訓練集不同90%）的平均性能

### 多調整參數處理 (Handling Multiple Parameters)

* 當模型有多個調整參數時的處理方法：
  - **多維網格搜索**：評估候選參數組合的網格（可能效率低）
  - **隨機搜索**（Random Search）：為每個參數定義可能值範圍，並隨機抽樣多維空間
  - 隨機搜索在參數數量大且無先驗知識時特別有益
  - 適用於**神經網絡**（Neural Networks）、**梯度提升機**（Gradient Boosting Machines）等模型

* **神經網絡模型範例**（OkCupid數據）：
  - 使用單層前饋神經網絡預測STEM領域概率
  - 主要調整參數包括：
    1. **隱藏單元數量**（Number of Hidden Units）：控制複雜度（範圍2-20）
    2. **激活函數**（Activation Function）：連接網絡不同部分的非線性函數（選項：S型曲線、tanh、ReLU）
    3. **丟棄率**（Dropout Rate）：訓練迭代中隨機將係數設為零的比率（範圍0-80%）
  
  - 神經網絡係數擬合的相關參數：
    1. **批量大小**（Batch Size）：每個迭代中隨機暴露給優化過程的訓練集數據點數量（範圍10-40,000）
    2. **學習率**（Learning Rate）：控制參數估計迭代中下降率（範圍0-1）
    3. **衰減率**（Decay Rate）：隨時間降低學習率（範圍0-1）
    4. **均方根梯度縮放因子**（Root Mean Square Gradient Scaling Factor, ρ）：控制梯度被近期平方梯度值歸一化的程度（範圍0-1）

  - 隨機創建20個七維調整參數組合，每個都使用10折交叉驗證評估
  - 表3.3顯示最佳設置ROC曲線下面積為0.837

### 自適應搜索方法 (Adaptive Search Methods)

* 不同於預先指定參數的網格搜索和隨機搜索，還有其他自適應方法：
  - **非線性搜索方法**（Nonlinear Search Methods）：
    * **Nelder-Mead單純形搜索**（Nelder-Mead Simplex Search）
    * **模擬退火**（Simulated Annealing）
    * **遺傳算法**（Genetic Algorithms）
  - 這些方法徹底搜索網格空間但計算成本高

* **貝葉斯優化**（Bayesian Optimization）：
  - 首先評估初始樣本池（使用網格或隨機搜索）
  - 創建單獨模型預測性能作為調整參數的函數
  - 推薦下一個要評估的候選集
  - 評估新點後，更新模型並繼續迭代

### 重抽樣結果解釋的注意事項 (Interpreting Resampling Results)

* **優化偏差**（Optimization Bias）風險：
  - 基於結果選擇最佳設置並用這些值表示模型性能可能導致偏差
  - 可能會高估模型的真實性能
  - 嵌套重抽樣程序可用於減輕這些偏差

* 參考 Boulesteix 和 Strobl (2009) 獲取更多信息

## 3.7 使用訓練集比較模型 (Comparing Models Using the Training Set)

* 在多個模型競爭時，需要進行正式評估以了解性能差異是否超出隨機預期範圍
* 建議對所有評估的模型使用相同的重抽樣，以便進行公平比較
* 這允許在使用測試集之前對模型進行正式比較

### 配對比較方法 (Paired Comparison Method)

* **OkCupid資料案例**：比較邏輯迴歸與神經網路模型
  - 兩個模型使用相同的重抽樣進行擬合和評估
  - 形成10個配對比較（10折交叉驗證）
  - 表3.4顯示每個重抽樣的具體ROC結果及其配對差異
  - 兩組值之間的相關性為0.96，表明結果中可能存在重抽樣到重抽樣的效應

* **表3.4：預測OkCupid數據的兩個模型的匹配重抽樣結果**
  |      | ROC估計值 |          |        |
  | ---- | --------- | -------- | ------ |
  |      | 邏輯迴歸  | 神經網絡 | 差異   |
  | 折1  | 0.830     | 0.827    | -0.003 |
  | 折2  | 0.854     | 0.853    | -0.002 |
  | 折3  | 0.844     | 0.843    | -0.001 |
  | 折4  | 0.836     | 0.834    | -0.001 |
  | 折5  | 0.838     | 0.834    | -0.004 |
  | 折6  | 0.840     | 0.833    | -0.007 |
  | 折7  | 0.839     | 0.838    | -0.001 |
  | 折8  | 0.837     | 0.837    | 0.000  |
  | 折9  | 0.835     | 0.832    | -0.003 |
  | 折10 | 0.838     | 0.835    | -0.003 |

### 統計推斷方法 (Statistical Inference Methods)

* 給定配對差異集，可以進行正式統計推斷比較模型
* 簡單方法：
  - 考慮兩個模型之間的**配對t檢定**（Paired t-test）
  - 或對差異進行普通**單樣本t檢定**（One-sample t-test）
  
* OkCupid案例結果：
  - ROC值的估計差異為-0.003
  - 95%置信區間為(-0.004, -0.001)
  - 顯示模型間存在真實但微小的性能差異

* 此方法曾在第2.3節用於預測中風結果的潛在變數排名

### 方法價值 (Value of the Method)

* 這種技術的價值有兩方面：
  1. 防止測試集在模型開發過程中被使用
  2. 使用多次評估（通過評估集）來衡量差異

* 第二點更為重要：
  - 通過使用多個差異，可以測量性能統計的變異性
  - 單一靜態測試集雖有優勢，但僅代表模型性能的單一實現
  - 無法了解此值的精確度

### 多模型比較 (Multiple Model Comparison)

* 也可以比較兩個以上的模型
* 分析必須考慮重抽樣內相關性：
  - 使用**貝葉斯階層模型**（Bayesian Hierarchical Model）(McElreath 2015)
  - 或**重複測量模型**（Repeated Measures Model）(West and Galecki 2014)

* 方法論來源：
  - 最初由Hothorn等人(2005)提出
  - Benavoli等人(2016)提供了模型間和數據集間重抽樣結果分析的貝葉斯方法

## 3.8 無過擬合特徵工程 (Feature Engineering Without Overfitting)

* 第3.5節討論了使用重抽樣調優模型的方法，核心是在未用於建模的數據上評估參數值
* 這一概念同樣適用於特徵工程活動：
  - 工程新特徵/編碼
  - 決定是否將新項納入模型
  - 應始終有獨立數據評估設計選擇的適當性

### 芝加哥列車數據案例 (Chicago Train Data Example)

* 某些日子的乘客數量被嚴重高估
  - 問題根源：假日和附帶趨勢未在預測變數中得到處理
  - 例如：「週五的聖誕節」等情況
  
* 圖3.12展示了這些新特徵對**支持向量機**（Support Vector Machine, SVM）的影響：
  - 面板顯示重抽樣的兩年評估集預測
  - 虛線表示添加假日預測變數前的預測
  - 實線對應添加這些特徵後的模型
  - 清楚表明這些預測變數在這些時間段內改善了性能
  - 雖有明顯的常識性理由，但仍使用評估集確認結果

### 使用重抽樣驗證趨勢 (Validating Trends with Resampling)

* 假設性情境：探索性數據分析清晰顯示當芝加哥公牛隊和熊隊有客場比賽時乘客量減少
  - 即使這種假設模式在訓練集中有明確信號
  - 重抽樣仍是最佳方法來確定在保留部分訓練數據作為無偏對照時是否成立

### OkCupid數據案例 (OkCupid Data Example)

* 第5.6節中，OkCupid數據用於從文本論文數據中派生關鍵詞特徵
* 方法：
  - 使用訓練集中隨機選擇的小部分發現關鍵詞特徵
  - 這個小子集保留在訓練集中並在模型擬合時正常重抽樣
  - 一些發現數據集包含在訓練中，預期重抽樣估計會略微樂觀偏差
  - 但仍有相當大量未參與特徵發現的數據用於確定其有效性

### 最佳實踐建議 (Best Practice Recommendations)

* 最佳做法是對探索性數據分析中發現的趨勢進行「壓力測試」：
  - 使用重抽樣獲得趨勢或模式是否真實的更客觀感知
  - 當數據豐富時，可以分配單獨的數據集用於發現（與訓練和測試集分開）
  - 這種方法在特徵工程過程中有效降低過擬合風險

# 4 探索性視覺化 (Exploratory Visualizations)

* 建模過程的主要目標是尋找可複製的方法來解釋反應變數的變異
* 前一章討論了發現與反應變數相關的預測變數模式的過程：
  - 選擇重抽樣方案防止過擬合
  - 選擇性能指標
  - 調優和訓練多個模型
  - 比較模型性能以識別最佳模型

## 視覺化探索的重要性 (Importance of Visual Exploration)

* 面對新數據集時，容易直接跳入預測建模過程：
  - 嘗試快速開發符合性能期望的模型
  - 或使用建模結果識別與反應變數相關的最重要預測變數
  
* 然而，如圖1.4所示，應花費足夠時間探索數據
* 本章將介紹視覺化探索數據的方法，展示如何指導特徵工程

## 探索性數據分析的步驟 (Steps in Exploratory Data Analysis)

* 首先創建視覺化以幫助闡明對反應變數的了解：
  - 使用**直方圖**（Histogram）或**箱形圖**（Box Plot）了解反應變數
  - 這些簡單視覺化揭示反應變數的變異量和可能的異常特性
  
* 然後探索預測變數之間以及預測變數與反應變數之間的關係：
  - **散點圖**（Scatter Plots）：個別預測變數與反應變數
  - **成對相關圖**（Pairwise Correlation Plot）：預測變數間
  - **高維預測變數的低維投影**（Projection）
  - **時間序列線圖**（Line Plots）：基於時間的預測變數
  - **迴歸或分類樹的前幾個層級**
  - **熱圖**（Heat Map）：跨樣本和預測變數
  - **馬賽克圖**（Mosaic Plots）：檢查分類變數間的關聯

## 視覺化的價值 (Value of Visualization)

* 這些視覺化提供的洞見應用於指導初始模型
* 最有用的視覺化不一定複雜或難以創建
* 一個簡單的散點圖可能揭示模型無法發現的洞見
* 可能導致創建新預測變數或轉換現有預測變數或反應變數，從而改善模型性能
* 挑戰在於發展如何視覺化探索數據以提取改進信息的直覺

## 持續的視覺化過程 (Continuous Visualization Process)

* 探索性數據分析不應在初始階段停止，而應在建立初始模型後繼續
* 模型建立後，視覺工具可用於：
  - 評估模型擬合不足
  - 評估原始模型中不存在的新預測變數的潛在效力

## 本章內容 (Chapter Content)

* 本章將深入探討建構初始模型前的各種有用視覺化工具
* 部分工具可在模型建立後用於識別可改善模型性能的特徵
* 按照圖1.4的輪廓，將查看建模前和建模過程中的視覺化
* 推薦閱讀Tufte (1990)、Cleveland (1993)和Healy (2018)的優秀數據視覺化資源

## 案例數據集 (Example Datasets)

* 芝加哥列車乘客數據（Chicago Train Ridership）：用於數值視覺化
* OkCupid數據：用於分類視覺化

## 4.1 芝加哥列車乘客數據簡介 (Introduction to the Chicago Train Ridership Data)

* 本節使用芝加哥交通管理局（Chicago Transit Authority, CTA）「L」列車系統的乘客數據（圖4.1）來說明：
  - 探索性視覺化如何成為理解數據集的關鍵部分
  - 視覺化如何用於識別和發現有助於改善模型預測能力的預測變數表達方式

### 數據的實際應用價值 (Practical Value of the Data)

* **短期預測價值**：
  - 預測未來一到兩週的乘客量，確保最佳數量的車廂可用
  - 大都市區工作日乘客量通常強於週末
  - 可能的需求誤解錯誤：
    * 工作日車廂過少：延誤乘客、過度擁擠、產生緊張
    * 週末車廂過多：效率低下、運營成本高、盈利能力低
  - 良好的需求預測有助於CTA更接近最佳滿足需求

* **長期預測價值**：
  - 預測何時需要線路服務
  - 更改停靠頻率以最佳服務乘客
  - 隨著人口變化和需求增強或減弱，增加或取消停靠站

### 數據內容與範圍 (Data Content and Scope)

* **收集範圍**：
  - 2001年1月22日至2016年9月11日
  - 涵蓋126個車站的每日乘客量
  - 乘客量測量：每個車站所有閘機入口的數量
  - 此期間各站每日乘客量差異大，範圍在0至36,323人次/天
  - 為便於展示，乘客量以千人次為單位

* **特定關注點**：Clark/Lake站
  - 位於市中心環路
  - 全系統中乘客量最高的車站之一
  - 服務四條不同線路，覆蓋芝加哥北部、西部和南部

### 預測變數構造 (Predictor Construction)

* **時間序列滯後變數**（Lagged Variables）：
  - 預測目標：未來乘客量
  - 僅有歷史數據可用於預測時
  - 創建滯後14天的預測變數：預測D天時，使用D-14天的數據
  - 必要時可添加其他滯後

* **區域因素變數**（Regional Factors）：
  - **天氣數據**：
    * 與乘客數據同期收集
    * 每小時記錄多種條件：陰天、凍雨、雪等
    * 創建反映雨、雪/冰、雲、風暴和晴天相關條件的預測變數
    * 每個類別由更細粒度的記錄條件合併而成
    * 新預測變數通過計算一天內觀察到條件的百分比來編碼
    * 例如：2012年12月20日，條件記錄為：雪（12.8%）、多雲（15.4%）、風暴（12.8%）和雨（71.8%）
    * 其他每小時天氣數據：溫度、露點、濕度、氣壓、降水量和風速
    * 溫度：總結為每日最低、中位數和最高值，以及每日變化
    * 壓力變化：類似計算
    * 多數情況下，使用每日中位數值總結每小時記錄
    * 如同乘客數據，使用滯後版本天氣數據
    * 共有18個天氣相關預測變數

  - **汽油價格數據**：
    * 2001年至2016年芝加哥地區平均每週汽油價格
    * 來源：美國能源信息管理局（U.S. Energy Information Administration）

  - **失業率數據**：
    * 同期月度失業率
    * 來源：美國人口普查局（United States Census Bureau）

* 這些預測變數的潛在用途將在下文討論
* 第9章將進一步討論此類數據的摘要方法

## 4.2 數值數據的視覺化：探索列車乘客數據 (Visualizations for Numeric Data: Exploring Train Ridership Data)

### 4.2.1 箱形圖、小提琴圖和直方圖 (Box Plots, Violin Plots, and Histograms)

* **單變量視覺化**（Univariate Visualizations）用於理解單一變數的分布
* 常見單變量視覺化工具：
  - **箱形圖**（Box-and-whisker Plots/Box Plots）
  - **小提琴圖**（Violin Plots）
  - **直方圖**（Histograms）

* 理解反應變數分布的重要性：
  - 模型主要目標是理解反應變數的變異
  - 必須了解分布是否對稱、偏斜、多峰或含異常值
  - 分布特性提供模型性能期望的下限
  - 可能指示反應變數在分析前需要轉換
  - 可能提供有關包含或創建特定特徵的線索

* **箱形圖**（圖4.2）：
  - 由John Tukey開發，用於快速評估變數分布
  - 包含最小值、下四分位數、中位數、上四分位數和最大值
  - 對稱分布在四分位數間有相等間距
  - 無法有效識別多峰分布

* **直方圖與小提琴圖**（圖4.3）：
  - 直方圖將數據分入等區間並計算樣本頻率
  - 能看到Clark/Lake站乘客量有兩個峰值
  - 小提琴圖由Hintze和Nelson (1998)開發
  - 通過生成數據密度及其鏡像創建
  - 比箱形圖更能保留分布特性，顯示多峰模式

* 乘客數據分析考量：
  - 是否應在自然單位或對數尺度上建模
  - 自然單位使解釋更容易（RMSE以乘客數計）
  - 對數轉換可確保不會預測負乘客量
  - 數據的雙峰性質使決定困難
  - 最終模型使用自然單位，表現略佳

### 4.2.2 通過分面、顏色和形狀增強視覺化 (Augmenting Visualizations through Faceting, Colors, and Shapes)

* 可通過多種方式向圖形添加額外維度：
  - **分面**（Faceting）：基於某變數將同類圖分割成不同面板
  - **顏色**（Colors）：用不同顏色表示類別或數值
  - **形狀**（Shapes）：用不同形狀區分數據點

* Clark/Lake站乘客量分布案例（圖4.5）：
  - 使用分面和顏色按工作日/週末分割乘客量分布
  - 揭示工作日乘客量明顯高於週末
  - 工作日分布左側有長尾，表明某些工作日乘客量異常低
  - 啟發我們進一步探索異常低值的原因，可能是新特徵的來源

### 4.2.3 散點圖 (Scatter Plots)

* **散點圖**：直接在二維空間展示兩個數值變數關係的圖形
* 用途：
  - 評估預測變數與反應變數之間的關係
  - 發現預測變數對之間的關係
  - 了解新預測變數的潛在用處

* 14天滯後乘客量與當日乘客量的散點圖（圖4.6）：
  - 顯示兩者之間存在強烈線性關係
  - 呈現兩個不同的點群（由於工作日/週末差異）
  - 顯示一些點遠離整體模式
  - 表明14天滯後變數將是預測當日乘客量的關鍵因素
  - 進一步解釋離群點可能導致新的有用特徵

### 4.2.4 熱圖 (Heatmaps)

* **熱圖**（Heatmap）：通用視覺化工具，可顯示兩個分類預測變數的關係
  - x軸和y軸形成網格
  - 網格填充可以是連續變數（使用顏色梯度）或分類變數（使用不同顏色）

* 低工作日乘客量熱圖案例（圖4.7）：
  - 創建月日、年份和工作日乘客量<10,000的指標
  - x軸表示年份，y軸表示月日
  - 紅色方框表示乘客量<10,000的工作日
  - 揭示明確模式：低乘客量出現在美國主要假日期間
    * 年初、1月中旬、2月中旬（直到2007年）
    * 5月底、7月初、9月初、11月底、12月底
  - 兩個不遵循年度模式的異常日子：
    * 2011年2月2日：芝加哥創紀錄低溫-16°F
    * 2014年1月6日：暴風雪傾倒21.2英寸雪

* 修改後的散點圖（圖4.8）：
  - 排除主要美國假日後的14天滯後vs當日乘客量
  - 大部分離群點消失，但仍有少數異常值
  - 2010年6月11日：芝加哥慶祝黑鷹隊贏得斯坦利杯
  - 儘管這類慶祝活動不頻繁，但工程化特徵預測這些異常事件可減少模型誤差

### 4.2.5 相關矩陣圖 (Correlation Matrix Plots)

* **相關矩陣圖**：顯示變數對之間相關性的矩陣形式圖
  - 每個變數在外部x軸和y軸上表示
  - 相關強度由矩陣中相應位置的顏色表示

* 2016年非假日工作日14天滯後乘客量相關矩陣（圖4.9）：
  - 幾乎所有車站乘客量對都呈正相關（紅色）
  - 一個車站低乘客量對應另一車站相對低乘客量
  - 大多數車站對之間相關性極高：
    * 18.7%的預測變數對相關性>0.90
    * 3.1%的相關性>0.95
  - 高相關性表明車站間信息冗餘，可通過過濾技術（第3章）或降維（第6章）處理

* 相關圖組織結構：
  - 基於**層次聚類分析**（Hierarchical Cluster Analysis）
  - 目標：將相似樣本在測量空間中排列在一起
  - 車站間距離基於相關值向量
  - 相關向量相似的車站排列相近
  - **樹狀圖**（Dendrogram）連接基於相關向量近似性的樣本
  - 組織結構有助於發現車站的視覺區分分組
  
* 異常車站案例：
  - x軸最左側的車站組顯示低相關甚至負相關
  - 其中一個站的中位數相關性僅為0.23
  - 這是服務奧黑爾機場的車站，受飛機時刻表影響
  - 與同線路的UIC-Halsted站負相關(-0.46)
  - 第二個最不相似的站是Addison站，受比賽出席率驅動

### 4.2.6 線圖 (Line Plots)

* **線圖**（Line Plot）：散點圖的延伸，適用於時間序列數據
  - x軸為時間，y軸為變數值
  - 相鄰時間點的值以線連接
  - 有助於識別時間趨勢，引導特徵工程

* 芝加哥乘客量時間趨勢（圖4.10）：
  - 計算工作日和週末每月平均乘客量
  - 自2001年以來，工作日和週末乘客量穩步增加
  - 與芝加哥大都市區人口增長一致
  - 每年內部模式：1月至10月增加，然後至12月下降
  - 表明最近（一週或一個月內）的乘客量信息對預測未來乘客量更有用

* 週末乘客量與汽油價格關係（圖4.11和4.12）：
  - 週末乘客量顯示出年度趨勢但某些年份內部變異更大
  - 2008年夏季週末乘客量和汽油價格均達高峰
  - 計算月平均2週滯後汽油價格與Clark/Lake站乘客量的幾何平均值
  - 發現正相關：2001-2014年，汽油價格越高，乘客量越高
  - 2015-2016年因石油供應增加而價格下降，趨勢略有不同
  - 挖掘原始線圖特性發現可解釋乘客量變異的新特徵

### 4.2.7 主成分分析 (Principal Components Analysis)

* 多維數據視覺化挑戰：
  - 可使用顏色、形狀和分面視覺化5-6個維度
  - 但現實數據集往往維度更高
  - 投影技術可將多維壓縮至2-3維：
    * **主成分分析**（Principal Components Analysis, PCA）
    * **偏最小平方法**（Partial Least Squares, PLS）
    * **多維縮放**（Multidimensional Scaling, MDS）

* **主成分分析**（PCA）：
  - 找到最能總結原始數據變異性的變數組合
  - 提供數據的簡化表示
  - 經常識別有助於特徵工程的潛在數據特性
  - 第6.3節將更詳細介紹降維技術

* 14天滯後車站乘客量的PCA（圖4.13）：
  - 第一主成分捕捉76.7%的總體變異性
  - 前兩個主成分捕捉83.1%的變異性
  - 表明車站乘客量信息冗餘，可以更濃縮的方式總結

* PCA分析摘要（圖4.13）：
  - (a)部分：前50個成分累積變異量
  - (b)部分：前兩個主成分的散點圖
    * 第一主成分專注於工作日/週末差異
    * 第二主成分專注於時間變化
  - (c)和(d)部分：主成分與影響最大的潛在變數的關係
  - 確認工作日/週末和年份相對於反應變數的重要性
  - 支持創建簡化數據同時保留關鍵預測信息的新特徵

  ## 4.3 分類數據的視覺化：探索OkCupid數據 (Visualizations for Categorical Data: Exploring the OkCupid Data)

* 本節使用OkCupid數據來說明定性數據的不同視覺化技術
* 數據在3.1節首次介紹：
  - 訓練集包含38,809個檔案
  - 目標是預測檔案作者是否在STEM領域工作
  - 事件率為18.5%
  - 大多數預測變數為分類性質

### 4.3.1 視覺化結果與預測變數之間的關係 (Visualizing Relationships between Outcomes and Predictors)

* **條形圖**（Bar Charts）的應用與局限：
  - 傳統用於表示分類值的計數
  - 圖4.14(a)顯示按結果類別分割和著色的宗教頻率
  - 優點：容易看出最頻繁和最不頻繁的類別
  - 問題：
    * 需要視覺判斷每個宗教類別STEM與非STEM的比率
    * 間接顯示我們感興趣的數據特性（STEM與非STEM的頻率比）
    * 不提供比例中的不確定性感

* **改進的視覺化方法**：
  - 圖4.14(b)：顯示各宗教內STEM檔案的百分比
    * 改進：STEM檔案比例成為焦點
    * 清楚顯示宗教的排序方式和與基準率18.4%的偏差
    * 仍未顯示不確定性和各宗教的頻率

  - 圖4.14(c)：帶95%置信區間的比例圖
    * 解決上述所有問題
    * 計算每種宗教STEM檔案的比例並顯示置信區間
    * 清楚顯示哪些宗教偏離隨機性
    * 誤差條寬度幫助理解數值的可信度
    * 類別較多時可使用**火山圖**（Volcano Plot）

* 關鍵啟示：
  - 圖形應有明確定義的假設
  - 假設應以允許讀者基於數據做出快速和有信息判斷的方式簡潔顯示
  - 宗教與結果之間存在關係，因為STEM職業比率在各組之間呈現梯度變化

* **分類結果與數值預測變數的視覺化**：
  - 使用檔案文章總長度（字符數）為例
  - 訓練集1,197個檔案未填寫任何開放文本欄位
  - 文章長度分布左偏，中位數1,853字符，最長約59,000字符
  - 圖4.15(a)顯示對數轉換後的字符計數分布，未填文章的檔案顯示為零
  - 類別間分布極為相似，表明此預測變數（單獨使用）不太重要

* **迴歸樣條平滑器**（Regression Spline Smoother）：
  - 使用基底展開（Basis Expansion）建立邏輯迴歸模型
  - 將原始因子（對數文章長度）用於創建人工特徵
  - 圖4.15(b)顯示結果：
    * 黑線表示邏輯迴歸模型的類別機率
    * 帶狀區域表示95%置信區間
    * 水平紅線表示訓練集中STEM檔案的基準機率
    * 長度約10^1.5之前的檔案STEM可能性略低於隨機
    * 更大檔案顯示機率增加，但最多僅增加約3.5%
    * 置信帶在10^1.75附近迅速擴大，主要由於此範圍內數據點減少
    * 此預測變數可能值得包含在模型中，但單獨影響不大

### 4.3.2 探索分類預測變數之間的關係 (Exploring Relationships Between Categorical Predictors)

* 在使用非數值數據的預測變數前，理解其特性和與其他預測變數的關係至關重要
* 不幸的做法：僅依賴對二維表的基本統計分析（如卡方檢定）

* **馬賽克圖**（Mosaic Plot）應用：
  - 可用於理解分類變數間關係
  - 圖4.16顯示OkCupid數據中藥物和酒精使用的馬賽克圖
  - 大多數數據表明社交飲酒，而絕大多數藥物反應為「從不」或缺失

* **對應分析**（Correspondence Analysis）：
  - 用於分析列聯表中的關係（Greenacre 2017）
  - 使用頻率分布確定預期單元計數（如果兩個變數無關）
  - 傳統卡方（χ²）檢定使用與預期值的偏差評估變數間關聯
  - 兩個變數強關聯時，總體χ²統計量較大
  - 對應分析分析殘差以確定解釋這些統計量最大部分的新變數
  - 新變數稱為**主坐標**（Principal Coordinates）

* 對應分析圖的特點：
  1. 每個軸評估主坐標解釋原始表信息的程度
  2. 靠近原點的類別表示數據的「平均」值
  3. 單一變數中主坐標接近的類別表示冗餘，可能可以合併
  4. 不同變數中在主坐標空間接近的類別表示這些類別間存在關聯

* OkCupid藥物和酒精使用的對應分析（圖4.17）：
  - 卡方統計量非常大（4043.8），自由度為18，p值極小（0）
  - 表明兩個變數間存在強關聯
  - x軸成分解釋超過一半卡方統計量：
    * 零附近的值表示未做選擇或偶爾使用物質
    * 遠離零的右側是較少發生的值
    * 一個小集群表明偶爾使用藥物和頻繁飲酒傾向有特定關聯
    * 另一個更極端的集群顯示經常使用酒精和藥物有關聯
  - y軸主要受缺失數據驅動（26%的表至少有一個缺失回應）
  - 解釋另外三分之一的卡方統計量
  - 結果表明這兩個變數有相似結果，可能測量相同的潛在特性

## 4.4 模型後探索性視覺化 (Post Modeling Exploratory Visualizations)

* 如第3.2.2節所述，重抽樣過程中對評估數據的預測可用於：
  - 理解模型性能
  - 指導建模者通過視覺化和分析了解下一組可能的改進

### 迴歸診斷工具 (Regression Diagnostic Tools)

* **多元線性迴歸**（Multiple Linear Regression）具有豐富的基於模型殘差的診斷工具
  - 這些診斷工具有助於理解模型擬合並識別可能有用的關係
  - 雖然預測性能不及其他建模技術，但其診斷能力不應被低估
  - 能揭示對更複雜建模技術有益的預測變數和預測變數關係

* **部分迴歸圖**（Partial Regression Plot）：
  - 使用兩個不同線性迴歸模型的殘差揭示預測變數的潛在用處
  - 步驟1：擬合包含現有預測變數的模型並計算殘差（ε）：
    * $y_i = \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon_i$
  - 步驟2：選擇新預測變數並擬合：
    * $x_{new_i} = \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \eta_i$
  - 步驟3：繪製兩組殘差（ε和η）的散點圖
  - 殘差間的線性或曲線關係表明新預測變數可能是有用的模型補充

* 殘差分析最佳實踐：
  - 避免僅通過預測訓練集數據來檢查模型擬合
  - 更好的策略是使用重抽樣創建的各種評估集產生的殘差

### 芝加哥數據重抽樣方案 (Chicago Data Resampling Scheme)

* 採用滾動預測原點方案（第3.4.4節）
* 訓練集包含5,697個數據點（每點代表一天）
* 重抽樣細節：
  - 基本樣本集：2014年9月1日前的數據
  - 分析/評估分割從此日期開始
  - 分析集累積增長：評估集評估後加入下次迭代的分析集
  - 每個評估集包含分析集最後一個值之後的14天
  - 共52個重抽樣，每個評估集互斥包含最新日期
  - 模擬真實數據分析方式：新數據作為測試集，之前數據用於訓練模型

* 圖4.18說明前幾個重抽樣：
  - 左側箭頭表示完整分析集始於2001年1月22日
  - 右側紅條表示滾動兩週評估集

* 使用這種方案擬合模型的優點：
  - 14組殘差集合可用於理解模型優缺點，過擬合風險最小
  - 因評估集在時間上移動，分析師可了解模型在特定時間表現不佳的情況

### 案例應用：Clark/Lake站乘客量預測 (Case Application: Clark/Lake Station Ridership Prediction)

* 迴歸模型：
  - 反應變數：Clark/Lake站乘客量
  - 初始預測變數：週、月和年

* 殘差分析（圖4.19）：
  - (a)部分：來自此模型的保留集殘差分布
    * 分布有兩個峰值，由一週中的部分（工作日與週末）造成
  
  - (b)部分：週部分的部分迴歸圖
    * 將基本預測變數對一週部分進行迴歸並計算保留集殘差
    * 殘差集之間的非隨機關係表明一週部分包含額外預測信息
    * 包括一週部分進一步減少殘差分布（直方圖標記為「Base + Part of Week」）
  
  - (c)部分：Clark/Lake站14天滯後乘客量的重要性
    * 數據主流中存在強烈線性關係
    * 少數日子落在整體模式之外，這些是假日
  
  - (d)部分：假日的潛在預測重要性
    * 一個假日遠離其他假日樣本：2015年7月4日
    * 這是訓練數據中唯一既是假日又是週末的日子
    * 因模型已考慮到一週部分，這一天是假日的額外信息對預測值幾乎沒有影響

## 4.5 總結 (Summary)

* 進階預測建模和機器學習技術提供了誘人的前景：以分析師很少的努力提取預測變數與反應變數之間的複雜關係
* 然而，這種完全自動化的建模方法只會使分析師處於不利地位

* 花時間視覺化以下內容只會帶來對數據更好的理解：
  - 反應變數本身
  - 預測變數特性
  - 預測變數之間的關係
  - 預測變數與反應變數之間的關係

* 此類知識的價值：
  - 提供關於數據中可能缺少特徵的關鍵洞見
  - 識別可能需要包含哪些特徵來改善模型的預測性能
  - 引導更有效的特徵工程決策

* **數據視覺化是特徵工程的基礎工具**
* 下一章將以此為基礎，開始開發分類預測變數的特徵工程

# 5 編碼分類預測變數 (Encoding Categorical Predictors)

* **分類**或**名義預測變數**（Categorical/Nominal Predictors）是包含定性數據的變數
* 例如：
  - OkCupid數據：教育程度（高中、二年制學院、大學等）和飲食習慣（素食、純素等）
  - Ames數據：房屋類型和鄰里
  - 郵遞區號：雖為數值，但實質上是定性預測變數，因為數值沒有連續意義

## 分類預測變數的來源與類型 (Sources and Types of Categorical Predictors)

* **來源多樣性**：
  - 結構化數據：預定義類別（如教育程度、宗教信仰）
  - 非結構化或開放文本：需要特殊處理的信息豐富數據
  - 文本處理方式：
    * 提取關鍵詞：單個詞或詞串？
    * 詞幹提取：使用詞根（如「comput」代表「computer」、「computers」等）
    * 正則表達式模式匹配：例如 ^comput

* **類型分類**：
  - **有序分類變數**（Ordered）：類別間存在有意義的順序
    * 例如：「壞」、「好」、「更好」顯示明確的值進展
    * 雖然差異可能無法精確量化，但順序有意義
  - **無序分類變數**（Unordered）：類別間無有意義的順序
    * 例如：「法國」、「印度」或「秘魯」
  - 有序和無序因子可能需要不同的方法來在模型中包含其信息

## 模型對分類預測變數的處理 (Model Handling of Categorical Predictors)

* **大多數模型要求所有預測變數為數值型**
* 例外情況：
  - **樹狀模型**（Tree-based Models）：
    * 可自然處理數值或分類預測變數的分割
    * 使用一系列if/then語句將數據順序分割為組
    * 例如，芝加哥數據中，樹狀模型可能包含組件：
      ```
      if day in {Sun, Sat} then ridership = 4.4K
      else ridership = 17.3K
      ```
  - **樸素貝葉斯模型**（Naive Bayes Model）（第12.1節）：
    * 可在分類預測變數與結果類別間創建交叉表
    * 頻率分布納入模型的概率計算
    * 類別可以以自然格式處理

* 本章主要關注將分類數據編碼為數值的方法

## 數據分析考量 (Data Analysis Considerations)

* 本章許多分析使用OkCupid數據（第3.1節介紹，前一章討論）
* 過擬合問題：
  - OkCupid數據集包含大量潛在分類預測變數，許多具有低流行度
  - 如果在整個訓練集上檢查特定趨勢和變數，然後基於這些分析添加特徵到模型，可能有問題
  - 即使使用交叉驗證，仍可能過擬合並找到不可泛化的預測關係
  - 訓練集不小（38,809個數據點）
  - 為發現新預測變數，從訓練集選取5,000個STEM檔案和5,000個非STEM檔案的子樣本（與第3.8節一致）

## 5.1 為無序類別創建虛擬變數 (Creating Dummy Variables for Unordered Categories)

* **虛擬變數**（Dummy Variables）或**指標變數**（Indicator Variables）是將分類值表示為數值數據的最基本方法
* 這些人工數值變數捕捉分類值的某些方面

### 對比函數與參數化 (Contrast Functions and Parameterization)

* 將分類值轉換為二元虛擬變數的數學函數稱為**對比函數**（Contrast Function）或**參數化函數**（Parameterization Function）
* 常用的對比函數類型：

#### 參考單元/處理對比 (Reference Cell/Treatment Contrast)

* 一種對比函數，其中預測變數的一個值在結果虛擬變數中未被考慮
* 範例：星期幾（使用星期日作為參考單元）

    | 原始值 | 虛擬變數 |       |       |       |       |       |  Sun  |
    | :----: | :------: | :---: | :---: | :---: | :---: | :---: | :---: |
    |        |   Mon    | Tues  |  Wed  | Thurs |  Fri  |  Sat  |  Sun  |
    |  Sun   |    0     |   0   |   0   |   0   |   0   |   0   |   1   |
    |  Mon   |    1     |   0   |   0   |   0   |   0   |   0   |   0   |
    |  Tues  |    0     |   1   |   0   |   0   |   0   |   0   |   0   |
    |  Wed   |    0     |   0   |   1   |   0   |   0   |   0   |   0   |
    | Thurs  |    0     |   0   |   0   |   1   |   0   |   0   |   0   |
    |  Fri   |    0     |   0   |   0   |   0   |   1   |   0   |   0   |
    |  Sat   |    0     |   0   |   0   |   0   |   0   |   1   |   0   |

* 這六個數值預測變數將取代原始分類變數

### 為何使用C-1個虛擬變數 (Why Use C-1 Dummy Variables)

* 兩個相關原因：
  1. 若已知六個虛擬變數的值，則可直接推斷第七個
  2. 技術性原因：設計矩陣的滿秩問題
     - 擬合線性模型時，創建設計矩陣X
     - 當模型有截距時，包括一列全為1的額外初始列
     - 線性模型參數估計涉及矩陣(X'X)的求逆
     - 若模型包含截距和所有七天的虛擬變數，七天列的行總和等於截距列
     - 這種線性組合會阻止矩陣求逆（因為矩陣奇異）
     - 此時設計矩陣被稱為**非滿秩**（Less than Full Rank）或**過確定**（Overdetermined）

* 當預測變數有C個可能值且僅使用C-1個虛擬變數時：
  - 可以計算矩陣逆
  - 對比方法被稱為**滿秩參數化**（Full Rank Parameterization）

* **單熱編碼**（One-hot Encoding）：
  - 非滿秩編碼的一種稱呼
  - 生成完整的指標變數集
  - 對於不敏感於線性依賴的模型可能有優勢（如第7.3.2節描述的glmnet模型）

### 虛擬變數的解釋 (Interpretation of Dummy Variables)

* 解釋取決於所使用的模型類型
* 使用只有星期幾的線性模型（芝加哥交通數據）：
  - 截距值估計參考單元的平均值（訓練集中星期日乘客的平均數）：3.84千人
  - 星期一的第二個模型參數估計為12.61千人
  - 在參考單元模型中，虛擬變數表示超出參考單元平均值的平均值
  - 星期一的乘客總估計值 = 截距 + 虛擬變數 = 16.45千人次

* 當有多個分類預測變數時：
  - 參考單元變為多維
  - 例如加入天氣預測變數（「晴朗」、「多雲」、「雨」和「雪」）
  - 如果「晴朗」為參考單元，截距對應於晴朗星期日的平均值
  - 每組虛擬變數的解釋不變
  - 「多雲星期一」的平均乘客量 = 晴朗星期日平均值 + 多雲的平均增量效應 + 星期一的平均增量效應

### 其他對比函數 (Other Contrast Functions)

* **單元平均參數化**（Cell Means Parameterization）：
  - 為星期的每一天創建一個虛擬變數
  - 不包括截距以避免設計矩陣的奇異性問題
  - 每個虛擬變數的估計值對應於該類別的平均值
  - 例如，星期一虛擬變數的參數估計值為16.45千人次

* 還有其他構建虛擬變數的對比方法
* 另一種基於多項式函數的方法將在有序數據章節中討論

## 5.2 編碼具有多類別的預測變數 (Encoding Predictors with Many Categories)

* 當分類預測變數的類別數 $C$ 非常大時會出現的問題
* 例如：美國郵遞區號（ZIP code）可能是重要預測變數，但有超過40,000個可能值
* 這可能產生過多虛擬變數（相對於數據點數量）
* 另外，人口稠密區的郵遞區號在數據中可能出現率更高，導致不常見位置的「長尾」分布

### 稀有類別的問題 (Issues with Rare Categories)

* **重抽樣問題**：
  - 重抽樣可能排除分析集中一些較罕見的類別
  - 導致全為零的虛擬變數列，對許多模型造成數值問題
  - 模型無法為包含這類預測變數的新樣本提供相關預測
  - 當預測變數只有單一值時，稱為**零變異預測變數**（Zero-variance Predictor）

* **處理方法一**：創建完整虛擬變數集並移除零變異預測變數
  - 簡單有效，但難以事先知道哪些項將在模型中
  - 在重抽樣過程中可能有不同數量的模型參數
  - 這可能是好的副作用，捕捉因省略罕見值造成的變異

* **處理方法二**：識別**近零變異預測變數**（Near-zero Variance Predictors）
  - 這些預測變數具有很少的唯一值，且在數據中出現頻率低
  - 計算最常見值頻率與第二常見值頻率的比率
  - 對於虛擬變數，這是零與一的數量比率
  - 例如：990個零和10個一的虛擬變數，比率為99
  - 建議粗略閾值為19來宣告變數「太罕見」

* **處理方法三**：重新定義預測變數類別
  - 創建「其他」（Other）類別，將罕見類別合併
  - 基於訓練集頻率指定應合併哪些類別的閾值
  - 這應在重抽樣過程中進行，以反映預測變數值包含在「其他」組中的變異性

### 特徵雜湊 (Feature Hashing)

* **雜湊函數**（Hash Functions）將一組值映射到另一組值
  - 通常用於數據庫和密碼學
  - 原始值稱為**鍵**（Keys），映射到較小的人工**雜湊值**（Hash Values）集
  - 可能雜湊數由用戶設定，通常為2的冪

* **特徵雜湊**（Feature Hashing）或**雜湊技巧**（Hash Trick）的特點：
  - 只需要被雜湊的值和雜湊結果數量
  - 轉換過程完全確定性
  - 傳統上應使映射到雜湊的鍵分布相對均勻
  - 雜湊函數是單向的：創建後無法知道原始值
  - 存在**碰撞**（Collision）：某些原始類別將映射到相同雜湊值

* **OkCupid位置數據雜湊示例**（表5.1）：
  - 雜湊值是僅從每個城市文本值派生的整數
  - 使用模運算將整數轉換為特徵列：$column = (integer \mod N) + 1$
  - 例如：「mountain view」映射到第15個虛擬變數特徵（16個特徵時）
  - 使用16個特徵時，某些位置具有碰撞（如雜湊14編碼了Menlo Park和San Leandro）

* **簽名雜湊**（Signed Hash）：
  - 不僅產生二進制指標，還可能產生-1、0或+1
  - 0表示類別與特定特徵無關
  - ±1值可表示原始數據的不同值
  - 可減少碰撞，例如將一個類別編碼為+1，另一個為-1

### 碰撞與別名結構 (Collisions and Alias Structures)

* **碰撞率研究**（圖5.1）：
  - 模擬10^4個預測變數類別，每類20個唯一值
  - 產生不同大小的二進制和簽名雜湊
  - 簽名值的碰撞率低於二進制編碼
  - 對於此數據，簽名特徵提供最佳別名結果

* **特徵雜湊的局限性**：
  - 對預測變數別名概念無感
  - 雜湊函數的均勻性在數據分析中可能成為劣勢
  - 減少別名是統計學中的重要概念

* **低別名的優勢**：
  - 更好解釋結果：如果預測變數對模型有顯著影響，理解原因很重要
  - 碰撞涉及的類別沒有有意義的關係（如San Leandro和Menlo Park不是因相似性而別名）
  - 雜湊函數不考慮每個鍵的出現概率
  
* **建議改進**：
  - 統計意識雜湊函數應讓頻繁出現的值不與其他別名
  - 這能為最可能暴露於對比函數的類別提供高度特異性
  - 可能導致不常見值相互別名，減少這些虛擬變數的稀疏性

## 5.3 處理新型類別的方法 (Approaches for Novel Categories)

* 假設已建立一個預測個人在STEM專業工作機率的模型
* 該模型依賴地理位置（如城市）作為預測變數
* 問題：當新個體居住在原始數據中未表示的城市時，模型預測會發生什麼？

### 新類別的預測挑戰 (Prediction Challenges with New Categories)

* 如果模型僅基於虛擬變數，則無法生成對新類別的預測
* 這是因為模型從未見過這些信息，沒有相應的參數估計

### 解決策略 (Solution Strategies)

* **「其他」類別策略**：
  - 使用之前提到的「其他」（Other）類別來捕獲新值
  - 雖然這種方法可能無法最有效地提取關於此特定類別的預測信息
  - 但它能讓原始模型應用於新數據，而無需完全重新擬合
  
* **與特徵雜湊結合**：
  - 新類別轉換為唯一「其他」類別，按與其餘預測變數值相同的方式進行雜湊
  - 需確保訓練/測試數據中存在「其他」類別
  - 或確保「其他」類別與另一個雜湊類別碰撞，使模型能預測新樣本

* 更多處理新類別的方法將在下一節給出

### 訓練/測試階段的注意事項 (Training/Testing Phase Considerations)

* 預測變數具有新類別的概念在模型的訓練/測試階段不是問題
* 在此階段，建模時已知現有數據中所有預測變數的類別
* 如果特定預測變數類別僅出現在訓練集（或測試集），仍可為兩個數據集創建虛擬變數
* 不過，它在其中一個數據集中將是零變異預測變數


## 5.4 監督式編碼方法 (Supervised Encoding Methods)

* 監督式方法：使用結果數據作為指導，將分類預測變數編碼為數值
* 適用情況：
  - 預測變數有許多可能的值
  - 模型訓練後可能出現新水平

### 效應編碼/似然編碼 (Effect/Likelihood Encoding)

* **基本概念**：
  - 測量因子水平對結果的影響，並使用此效應作為數值編碼
  - 例如：計算Ames數據中每個鄰里的平均或中位數房價，用作該因子水平的表示

* **分類問題應用**：
  - 使用邏輯迴歸模型測量分類結果與分類預測變數間的效應
  - 事件勝算定義為：$p/(1-p)$，其中$p$是事件發生率
  - 例如：Mountain View加州STEM檔案率為0.53，勝算為1.125
  - 邏輯迴歸模型將結果的對數勝算作為預測變數的函數
  - 可為每個預測變數值計算對數勝算，用作編碼

* **估計效應的方法**：
  - 使用廣義線性模型（線性或邏輯迴歸）：快速但有缺點
    * 例如：當因子水平只有單一值時，對數勝算理論上應無限大
  
  - **收縮方法**（Shrinkage Methods）：
    * 當因子水平數據質量差時，將效應估計偏向整體估計
    * 「質量差」可能由於小樣本量或大方差
    * 可將極端估計值向分布中間移動
    
  - **貝葉斯分析**（Bayesian Analysis）：
    * 使用先驗分布指定估計的理論分布
    * 將觀察數據與先驗分布混合，形成後驗分布
    * 數據質量差的類別，其後驗估計更接近先驗分布中心
    * OkCupid數據案例：使用標準差較大($\sigma=10$)的對數勝算的正態先驗分布
    * 結果顯示在表5.2的「Raw」和「Shrunk」列
    * 圖5.2展示：極端值（如Mountain View從0.118降至0.011）被收縮

  - **經驗貝葉斯方法**：
    * 以線性（和廣義線性）混合模型形式
    * 非貝葉斯估計但也納入收縮
    * 比完全貝葉斯方法快但靈活性較低

* **效應編碼的問題**：
  - 增加過擬合可能性
  - 從一個模型提取估計效應，放入另一模型（作為變數）
  - 如果兩模型基於相同數據，可能成為自我實現的預言
  - 若編碼與未來數據不一致，會導致無法檢測的過擬合
  - 使用摘要統計作為預測變數可能低估數據變異
  - 解決方案：使用不同數據集估計編碼和預測模型，或在重抽樣內進行推導

### 詞嵌入/實體嵌入方法 (Word/Entity Embedding)

* **來源**：深度學習文獻中分析文本數據的方法
* **基本思想**：
  - 估計較小的數值特徵集，充分表示分類預測變數
  - 類似第6章的維度減少方法
  - 可能估計單詞間語義關係，使相似主題單詞（如「狗」、「寵物」）有相似編碼值
  - 不限於文本數據，可用於任何定性變數
  
* **工作原理**：
  - 指定新特徵數量
  - 將傳統指標變數隨機分配給新特徵
  - 優化指標到特徵的分配及特徵參數係數
  - 模型中結果可與預測模型相同（如售價或STEM概率）
  - 損失函數通常用均方根誤差（數值結果）或交叉熵（分類結果）
  - 儲存每個觀察值的嵌入特徵值作為查詢表
  - 為原始預測變數分配額外水平，作為新值的佔位符

* **OkCupid數據案例**：
  - 訓練集有52個位置，使用嵌入特徵表示（加一個新位置插槽）
  - 模型結構包含位置數據降維的專用部分（綠色節點）和其他預測變數
  - 使用**整流線性單元**（Rectified Linear Unit, ReLU）連接
  - 使用10個隱藏節點，總計估計678個參數
  - 使用交叉熵擬合，30次迭代後收斂
  - 結果見表5.2「Feat 1/2/3」列和圖5.3
  - 特徵與原始對數勝算的秩相關為-0.01、-0.58和-0.69
  - 部分新特徵相互關聯（絕對相關值0.01至0.69）
  - 這些嵌入特徵可用於其他模型預處理位置值

## 5.5 有序數據的編碼 (Encodings for Ordered Data)

* 前面討論的無序預測變數編碼方法（虛擬變數或雜湊版本）無法反映類別間的相對順序
* 當預測變數類別有相對順序時（如「低」、「中」、「高」），需要不同的編碼方法

### 多項式對比編碼 (Polynomial Contrast Encoding)

* **基本概念**：
  - 在統計學中稱為**多項式對比**（Polynomial Contrast）
  - 對比特徵：是單一比較（一個自由度），其係數和為零
  - 目的：呈現有序類別與反應變數間的關係（線性或非線性）

* **線性對比**：
  - 當類別與反應變數有線性關係時使用
  - 例如：「低」、「中」、「高」編碼為-0.71、0、0.71
  - 反映反應變數在各類別間的均勻增加（如每階增加約5單位）

* **二次對比**：
  - 當類別與反應變數關係最好用二次趨勢描述時使用
  - 例如：「低」、「中」、「高」編碼為0.41、-0.82、0.41
  - 表5.3顯示三水平有序分類預測變數的線性和二次多項式對比

* **正交性要求**：
  - 多種關係（線性、二次等）可同時包含在同一模型中
  - 新預測變數應包含唯一信息
  - 對比向量的點積必須為0（正交）

### 多項式對比的局限性 (Limitations of Polynomial Contrasts)

* **模式適配問題**：
  - 多項式對比可能無法有效描述某些關係模式
  - 例如：「低」和「中」樣本反應大致相同，但「高」樣本反應明顯不同

* **高階多項式問題**：
  - 當有序預測變數有C個水平時，虛擬變數編碼使用高達C-1次的多項式
  - 高階多項式（如八次模式）不太可能建模重要趨勢
  - 實踐中，很少探索二次多項式以上的有效性

### 替代方法 (Alternative Approaches)

* **將預測變數視為無序因子**：
  - 允許多項式特徵集未涵蓋的模式
  - 缺點：如果真正的潛在模式是線性或二次，無序虛擬變數可能無法有效發現

* **基於特定情境信息的數值轉換**：
  - 將有序類別轉換為單一數值得分集
  - 例如：專家可按整數尺度對計算機硬件故障模式的嚴重性進行排名
  - 輕微故障可評為「1」，災難性故障模式可能得到「10」分

* **決策輔助**：
  - 簡單視覺化和特定情境專業知識可幫助了解這些方法是否合適

## 5.6 從文本數據創建特徵 (Creating Features from Text Data)

* 文本數據來源豐富：問卷、文章、評論、推文等
* OkCupid數據包含九個開放文本問題的回答，如「自我總結」和「我絕對離不開的六件事」
* 開放回答可能包含與結果相關的重要信息：STEM領域人士可能使用不同於人文學科人士的詞彙

### 文本數據預處理 (Text Data Preprocessing)

* 案例分析：OkCupid檔案中的文本處理
  - 初始分析：10,000個檔案子樣本中有63,440個不同詞
  - 清理步驟：
    * 移除HTML標記（如`<br />`和`<a href= \"link\">`)
    * 移除標點符號、換行符和其他符號
    * 移除無意義的重複字符（如`***`或`aaaaaaaaaa`）

* 計算文本相關特徵：
  - 計算逗號、標籤、提及、感嘆號等數量
  - 創建13個新的「文本相關」特徵

### 超連結分析案例 (Hyperlink Analysis Example)

* **假設**：超連結存在可能與職業相關
* **分析結果**（表5.4）：STEM檔案超連結比率為21%，非STEM檔案為12.4%
* **勝算比分析**：
  - STEM檔案包含超連結的勝算：0.21/0.79 = 0.27
  - 非STEM檔案勝算：0.142
  - 勝算比：1.9，表示含超連結的檔案是STEM職業的勝算幾乎高1.9倍
  - 95%置信區間下限為1.7，表明增加不太可能源於隨機噪聲
  - 結論：超連結指標可能有利於模型，應被包含

### 關鍵詞分析 (Keyword Analysis)

* **過濾步驟**：
  - 僅分析在10,000個檔案中至少出現50次的詞彙
  - 此約束將潛在關鍵詞數量從63,440減少到4,918

* **統計方法**：
  - 計算每個詞的勝算比和相關p值
  - p值本身有局限性：
    * 只回答「兩組間勝算有差異嗎？」而非「差異有多大？」
    * 多重檢驗問題：4,918個檢驗會指數級增加假陽性率
  - 使用**假發現率**（False Discovery Rate, FDR）校正
    * Benjamini-Hochberg校正方法
    * FDR值0.30意味著FDR值小於0.30的關鍵詞集合有30%的假發現率

* **火山圖**（Volcano Plot）分析（圖5.4）：
  - x軸：估計勝算比
  - y軸：FDR值的負對數（較大值表示更高統計顯著性）
  - 點大小與樣本數據集中的事件數量相關
  - 上左和上右落入點表示類別間強烈差異
  - 更多關鍵詞在STEM檔案中出現可能性更高（x軸1右側上部的點）

* **重要關鍵詞選擇標準**：
  - 勝算比至少2（任一方向）
  - FDR值小於10^-5
  - 結果：52個關鍵詞
  - 僅7個在非STEM檔案中富集：im、lol、teacher、student、law、alot、lawyer
  - STEM富集關鍵詞多與職業相關（如engin、startup和scienc）或極客文化相關（如firefli、neal和stephenson）

### 情感和語言特徵 (Sentiment and Language Features)

* 計算9個與情感和語言相關的特徵：
  - 基於詞彙情感值（如「horrible」負面，「wonderful」正面）
  - 包括觀點（第一、第二或第三人稱文本）和其他語言元素測量

### 特徵集評估 (Feature Set Evaluation)

* 使用不同特徵集計算一系列邏輯迴歸模型：
  1. 基本檔案特徵集（160個預測變數）：ROC曲線下面積0.77
  2. 添加簡單文本特徵（173個預測變數）：性能基本不變（AUC=0.776）
  3. 基本特徵加關鍵詞：性能大幅提升（AUC=0.839）
  4. 添加情感和語言特徵：微小提升（AUC=0.841）
  - 結論：關鍵詞和基本特徵模型是最佳版本

### 其他文本預處理方法 (Other Text Preprocessing Methods)

* **停用詞移除**（Stop Words Removal）：
  - 移除常用詞如「is」、「the」、「and」等
  - SMART詞庫停用詞過濾後剩下62,928個唯一結果

* **詞幹提取**（Stemming）：
  - 使用Porter算法將相似詞轉換為共同詞根
  - 例如：「teach」、「teacher」、「teaches」等簡化為「teach」、「teacher」、「teachabl」
  - 處理後剩下45,486個唯一詞
  - 潛在缺點：可能降低文本特異性

* **詞頻-逆文檔頻率**（tf-idf）：
  - 目標：找到對當前文檔集合中個別文檔重要的詞或術語
  - 計算：tf（詞頻）× idf（逆文檔頻率）
  - 例如：「feynman」在特定檔案出現2次，在10,000個檔案中總共出現55次
    * tf-idf = 2 × log₂(10000/55) = 15
  - 相比之下，「internet」出現同樣次數但更常見（1529個檔案）
    * tf-idf = 2 × log₂(10000/1529) = 5.4
  - 這樣，根據總體數據集中的豐富度，相同原始計數被不同程度降權
  - 預測模型中的潛在問題：新樣本預測時如何處理（無集合）
  - 解決方案：使用訓練集的整體idf值對新樣本中的詞頻加權

* **n-gram考慮**：
  - 分析連續詞序列而非單個詞
  - n-gram是n個連續詞的序列
  - 例如：「I hate computers」可能得分不同於單個詞「computer」

## 5.7 決策樹模型中的因子與虛擬變數 (Factors versus Dummy Variables in Tree-Based Models)

* 某些類型的模型能夠直接使用分類數據的自然形式（無需轉換為虛擬變數）
* 芝加哥數據的簡單**迴歸樹**（Regression Tree）使用星期幾預測變數的簡單分割：
  ```
  if day in {Sun, Sat} then ridership = 4.4K
  else ridership = 17.3K
  ```

### 虛擬變數轉換的影響 (Impact of Dummy Variable Conversion)

* 若星期幾被轉換為虛擬變數，模型會變得稍微複雜：
  ```
  if day = Sun then ridership = 3.84K
    else if day = Sat then ridership = 4.96K
      else ridership = 17.30K
  ```
* 非虛擬變數模型也可以產生相同結構，若它發現星期六和星期日的結果差異足夠大

* **核心問題**：在樹狀模型中，預測變數的編碼方式是否重要？

### 實驗設計與數據集 (Experimental Design and Datasets)

* 進行了一系列實驗來比較不同編碼方式
* 使用多個公開分類數據集進行比較（表5.5摘要）：
  - **離職數據**（Attrition）
  - **汽車評估**（Cars）
  - **客戶流失**（Churn）
  - **德國信用**（German Credit）
  - **HPC數據**

* **實驗方法**：
  - 每次迭代使用75%的數據作為訓練集
  - 採用10折交叉驗證調整模型
  - 二元結果變數使用**ROC曲線下面積**（Area Under ROC Curve）最大化
  - 多類別結果使用**多項式對數似然**（Multinomial Log-likelihood）優化
  - 類別不平衡時使用**下採樣**（Downsampling）補償

* **數據編碼方式**：
  - 原始形式數據
  - （無序）虛擬變數
  - 對於有序數據，額外創建有序虛擬變數（多項式對比）

### 測試的模型類型 (Tested Model Types)

* **單一CART樹**（Single CART Trees）：使用「一標準誤差法則」選擇複雜度參數
* **袋裝CART樹**（Bagged CART Trees）：每個模型包含50個成分樹
* **單一C5.0樹**和**單一C5.0規則集**（Single C5.0 Trees/Rulesets）
* **單一條件推論樹**（Single Conditional Inference Trees）：評估10個p值閾值
* **提升CART樹**/**隨機梯度提升**（Boosted CART Trees/Stochastic Gradient Boosting）：
  - 優化樹數量、學習率、樹深度和要求附加分割的樣本數
  - 使用隨機搜索評估25個參數組合
* **提升C5.0樹**和**提升C5.0規則**：針對迭代次數調優
* **隨機森林使用CART樹**（Random Forests using CART Trees）：
  - 每個森林包含1,500棵樹
  - 調優分割選擇的變數數量
* **隨機森林使用條件（無偏）推論樹**（Random Forests using Conditional Inference Trees）：
  - 每個森林包含100棵樹
  - 調優分割選擇的變數數量

### 性能評估結果 (Performance Evaluation Results)

#### ROC曲線下面積比較 (AUC Comparison)

* 對於三個二元分類數據集的ROC曲線下面積比較（圖5.6）：
  - 計算性能差異百分比：$\%Difference = \frac{Factor - Dummy}{Factor} \times 100$
  - 正值表示因子編碼性能更好
  - 結果顯示：對於這些數據集，編碼方法之間幾乎沒有實質差異
  - 在40種情況中，只有2種情況顯示分布主流未覆蓋零
  - **隨機梯度提升**和**袋裝CART樹**（都是集成方法）在使用因子而非虛擬變數時，一個數據集的ROC曲線下降了2%-4%

#### 準確率比較 (Accuracy Comparison)

* 整體準確率評估結果（圖5.7）：
  - 結果不一致
  - 比較因子與無序虛擬變數時，兩個模型顯示編碼差異
  - 客戶流失數據結果類似於ROC曲線指標
  - 汽車評估數據顯示因子編碼普遍優於虛擬變數
    * 可能原因：汽車數據有四個類別，所有預測變數都是分類型
  - 多項式對比生成的虛擬變數情況下，兩個數據集都沒有顯示編碼間差異

#### 性能結論 (Performance Conclusions)

* 兩種編碼之間的差異很少（但可能發生）
* 雖然汽車數據（全部為分類變數）中因子編碼表現較好，但其他高比例分類預測變數的數據集卻沒有顯示差異
* 編碼效果可能取決於分類預測變數對結果的重要性及其影響方式
* 很難預測何時會出現差異

### 訓練時間比較 (Training Time Comparison)

* 計算每次模擬的模型訓練時間（圖5.8）：
  - 顯示使用因子相較於虛擬變數的速度提升
  - 強烈趨勢表明：基於因子的模型訓練效率高於虛擬變數對應模型
  - 可能原因：生成虛擬變數擴增的預測變數數量需要更多計算時間
  - 例外：使用**條件推論樹**（Conditional Inference Trees）的模型

### 編碼對摘要指標的影響 (Impact on Summary Measures)

* 質性預測變數編碼方式會影響摘要指標（如變數重要性評分）：
  - 許多技術（特別是樹狀模型）計算變數重要性評分，表示預測變數影響結果的相對程度
  - 樹會測量特定分割對模型性能改進的影響
  - 如果分割涉及預測變數的所有值（如週六與其他六天），整個變數的重要性評分可能比單個水平（如週六或非週六）的類似評分大得多
  - 在後一種情況下，每個水平的分散評分可能不會比反映所有水平的類似評分排名更高
  - 特徵選擇（第10至12章）和交互作用檢測（第7章）中也會出現類似問題

### 實用建議 (Practical Recommendations)

* 建議使用不轉換為虛擬變數的預測變數
* 如果模型表現良好，也可嘗試使用虛擬變數重新擬合
* 第12章對**樸素貝葉斯模型**（Naive Bayes Model）進行了類似但規模較小的分析

## 5.8 總結 (Summary)

* 分類預測變數在待建模的數據中可以呈現多種形式
* 除了樹狀模型外，分類預測變數必須首先轉換為數值表示，以便其他模型使用其信息

### 基本轉換策略 (Basic Conversion Strategies)

* 分類預測變數最簡單的特徵工程技術是將每個類別轉換為單獨的二元虛擬預測變數
* 這種基本轉換有一個主要注意事項：某些模型需要的虛擬預測變數數量比類別數量少一個
* 創建虛擬預測變數可能不是從分類預測變數提取預測信息的最有效方法
  - 例如：如果預測變數具有有序類別，則線性或多項式對比等其他技術可能與結果更好地關聯

### 文本數據處理 (Text Data Processing)

* 文本字段也可以視為分類預測變數的集合，必須轉換為數值
* 近期出現了許多將文本轉換為數值的方法
* 這些方法通常必須包括過濾步驟，以移除高頻率、無描述性的詞語

### 結論 (Conclusion)

* 若做得好，分類預測變數的特徵工程可以釋放與結果相關的重要預測信息
* 下一章將專注於使用特徵工程技術來揭示連續預測變數中的額外預測信息

# 6 數值型預測變數的工程 (Engineering Numeric Predictors)

* 上一章提供了熟練修改定性預測變數的方法
* 本章目標：開發將連續型預測變數轉換為模型可更好利用形式的工具

## 連續型預測變數的挑戰 (Challenges with Continuous Predictors)

* 連續型預測變數可能面臨許多潛在問題
* 某些問題可透過模型選擇來緩解：
  - **基於排序的模型**（如決策樹）：對偏斜分布或異常值不敏感
  - **K最近鄰**（K-nearest Neighbors）和**支持向量機**（Support Vector Machines）：對偏斜分布或異常值更敏感
  - **偏最小平方法**（Partial Least Squares）：專為直接處理高度相關的預測變數而設計
  - **多元線性迴歸**（Multiple Linear Regression）或**神經網路**（Neural Networks）：在高度相關預測變數情況下性能受損

* 若希望利用和探索更多類型模型的預測能力，需要透過特徵工程解決預測變數問題

## 常見的數值型預測變數問題 (Common Issues with Numeric Predictors)

* 預測變數可能：
  - 處於極其不同的尺度
  - 呈現偏斜分布，少數樣本比大多數數據大幾個數量級（即**偏度**，Skewness）
  - 包含少量極端值（**離群值**，Outliers）
  - 在範圍低端和/或高端受到**審查**（Censoring）
  - 與響應變數有複雜關係，真正具有預測性但無法用簡單函數充分表示
  - 包含相關但過度冗餘的信息（即信息可通過更小、更集中的新預測變數集更有效表示）

## 本章技術組織 (Organization of Techniques)

* 本章將特徵工程技術分為三大類：

1. **單變量轉換**（1:1 Transformations）：
   - 針對個別預測變數的問題特性（第6.1節）
   - 處理單一預測變數的問題

2. **一對多轉換**（1:Many Transformations）：
   - 將單個預測變數擴展為多個預測變數（第6.2節）
   - 目的：更好地表示複雜的預測變數-響應變數關係
   - 促進預測信息的提取

3. **多對多轉換**（Many:Many Transformations）：
   - 整合多個預測變數中的冗餘信息的方法（第6.3節）
   - 目的：減少維度並提取有用特徵

* 所有方法的最終目標：將現有連續預測變數轉換為任何模型都能利用的形式，呈現最有用的信息

## 應用考量 (Application Considerations)

* 這些技術的需求高度依賴數據和模型
* 例如：解決偏度或離群值的轉換對某些模型而言非必要，但對其他模型的良好性能至關重要
* 本章將提供指南，說明哪些模型會從特定預處理方法中受益

## 6.1 單變量轉換 (1:1 Transformations)

* 對單個預測變數的修改可以提高其在模型中的實用性
* 首先討論改變數據尺度的轉換方法（如第1章圖1.3所示）

### 冪轉換 (Power Transformations)

* **Box-Cox轉換**（Box-Cox Transformation）是常用的冪轉換方法：
  - 最初用於轉換模型的結果變數
  - 使用**最大似然估計**（Maximum Likelihood Estimation）估計轉換參數λ
  - 轉換公式：
    $$x^* = \begin{cases} 
    \frac{x^\lambda - 1}{\lambda \tilde{x}^{\lambda-1}}, & \lambda \neq 0 \\
    \tilde{x}\log x, & \lambda = 0
    \end{cases}$$
  - 其中$\tilde{x}$是預測變數數據的幾何平均值
  
* **λ值對應的常見轉換**：
  - λ = 1：無轉換
  - λ = 0：對數轉換
  - λ = 0.5：平方根轉換
  - λ = -1：倒數轉換

* **Box-Cox轉換的限制**：只能應用於嚴格為正的數據
* **Yeo-Johnson轉換**（Yeo-Johnson Transformation）：
  - 由Yeo和Johnson (2000)開發
  - 類似Box-Cox但可用於任何數值數據

* **轉換的無監督性質**：
  - 這些轉換是無監督的，因為未使用結果變數進行計算
  - 可能改善預測變數分布，但不保證改進模型
  - 對於使用預測變數多項式計算的模型特別有用：
    * 線性模型
    * 神經網路
    * 支持向量機
  - 這些模型中，偏斜分布的尾部可能主導底層計算

### 範圍受限變數的轉換 (Transformations for Bounded Variables)

* **Logit轉換**（Logit Transformation）：
  - 適用於範圍在0和1之間的變數（如比例）
  - 解決問題：模型預測可能超出原始邊界
  - 公式：$logit(\pi) = log(\frac{\pi}{1-π})$
  - 此轉換將尺度從0-1之間的值轉換為負無窮到正無窮
  - 極端值（如絕對0或1）可添加/減去小常數避免除以零
  - 進行預測後，可使用**反logit轉換**（Inverse Logit Transformation）將值恢復至原始尺度

* **反正弦轉換**（Arcsine Transformation）：
  - Logit轉換的替代方法
  - 主要用於比例的平方根：$y^* = arcsine(\sqrt{\pi})$

### 標準化技術 (Standardization Techniques)

* **中心化**（Centering）：
  - 從預測變數的個別值中減去其訓練集平均值
  - 結果：所有變數具有共同的平均值（即零）

* **縮放**（Scaling）：
  - 將變數除以訓練集標準差
  - 確保變數具有標準差為1

* **範圍縮放**（Range Scaling）：
  - 使用訓練集最小值和最大值
  - 將數據轉換到任意範圍（通常是0和1之間）

* **重要注意事項**：
  - 轉換所需的統計量（如平均值）從訓練集估計
  - 應用於所有數據集（如測試集或新樣本）
  
* 這些轉換通常在模型需要預測變數採用共同單位時需要：
  - 當使用預測變數間的距離或點積時（如K最近鄰或支持向量機）
  - 當變數需要在共同尺度上應用懲罰時（如第7.3節的lasso或ridge迴歸）

### 數據平滑 (Data Smoothing)

* **適用情境**：包含時間或序列效應的數據
* **運行均值**（Running Mean）：
  - 可在建模前減少預測變數或結果數據中的過多噪聲
  - 例如：5點運行均值將每個數據點替換為其自身及其前後兩個數據點的平均值
  - 移動窗口大小很重要：過大可能消除重要趨勢（如非線性模式）

* **運行中位數**（Running Median）：
  - 特別適用於存在顯著離群值的情況
  - 當離群值落入移動窗口時，中位數對異常值不敏感
  - 優勢：改變的原始數據點較少
  - 案例（圖6.2）：3點運行中位數平滑器可緩解離群值而不影響非線性趨勢

* **其他平滑器**：
  - 平滑樣條（本章稍後描述）等
  - 短運行中位數的簡單性和穩健性使其成為具吸引力的方法

* **平滑操作的應用**：
  - 可應用於結果數據和/或任何序列預測變數
  - 重要性：確保測試集預測變數數據單獨平滑，避免訓練集影響測試集值
  - 減少噪聲的預測變數數據平滑在第9.4節中有更多探討

## 6.2 一對多轉換 (1:Many Transformations)

* 上一章說明了從單個定性預測變數創建多個數值指標列的過程
* 類似地，可以對單個數值預測變數進行轉換，將其擴展為多個預測變數
* 這些一對多轉換可用於提高模型性能

### 6.2.1 通過基底展開和樣條的非線性特徵 (Nonlinear Features via Basis Expansions and Splines)

* **基底展開**（Basis Expansion）：
  - 從預測變數$x$派生一系列函數$f_i(x)$，可使用線性組合結合
  - 例如：連續預測變數$x$的三次基底展開：
    $$f(x) = \sum_{i=1}^3 \beta_i f_i(x) = \beta_1 x + \beta_2 x^2 + \beta_3 x^3$$
  - 應用：原始列增加兩個新特徵（原始值的平方和立方）
  - $\beta$值可使用基本線性迴歸估計

* **全局基底展開的局限性**：
  - 將模式全局應用於預測變數，通常可能不足
  - 例如：Ames住房數據中的地塊大小變數與銷售價格關係（圖6.3）
    * 數據主流（在$10^{3.75}$和$10^{4.25}$之間）有線性增長趨勢
    * 但在數據主體兩側，模式可忽略不計

* **多項式樣條**（Polynomial Spline）：
  - 替代創建全局基底函數的方法
  - 使用**節點**（Knots）劃分預測變數空間的不同區域
  - 在每個區域內使用多項式函數（通常是三次）表示數據
  - 特殊函數確保節點處的整體連續性
  - 節點數量控制區域數量和函數潛在複雜度：
    * 節點數量低（也許3個或更少）：表示相對簡單的趨勢
    * 節點數量更多：更適應但更可能過擬合

* **節點選擇**：
  - 通常使用數據的百分位數，使每個區域包含相對相等的數據量
  - 例如：具有3個區域的樣條通常在33.3%和66.7%百分位數處放置節點
  - 確定節點後，可創建基底函數並用於迴歸模型

* **自然三次樣條**（Natural Cubic Spline）：
  - 多項式樣條的常見類型
  - 第一個函數通常作為截距
  - 見圖6.4(a)：顯示了由此過程生成的$x$的其他函數
    * 藍線表示節點
    * 前三個特徵包含基底函數值為零的區域
    * 其他三個特徵對數據主流外的區域賦予最大權重
  - 圖6.4(b)顯示最終迴歸形式：
    * 數據點云中間有強線性趨勢，外部關係弱
    * 左側擬合不佳，可能需要更多節點

* **節點數量選擇**：
  - 這是可通過網格搜索或平滑器結果視覺檢查確定的調優參數
  - 一維中，過擬合會有視覺證據
  - 一些樣條函數使用**廣義交叉驗證**（Generalized Cross-Validation, GCV）估計適當的樣條複雜度

* **監督方法**：
  - 基底函數可在接觸結果數據前創建，但這不是必需的
  - 平滑樣條方法：最初將每個訓練集點指定為潛在節點，使用正則化迴歸模型確定哪些實例應被視為節點
  - **廣義加性模型**（Generalized Additive Models, GAMs）：
    * 擴展了廣義線性模型（包括線性和邏輯迴歸）
    * 為單個預測變數包含非線性項（不能建模交互作用）
    * 可自適應地為不同變數建模不同基底函數
    * 為每個變數估計複雜度
  
* **單一固定節點樣條**（Single, Fixed Knot Spline）：
  - 與**多變量自適應迴歸樣條**（Multivariate Adaptive Regression Spline, MARS）模型相關
  - 使用**鉸鏈函數轉換**（Hinge Function Transformation）：
    $$h(x) = x I(x > 0)$$
    其中$I$是指示函數，當$x$大於零時為$x$，否則為零
  - 例如：如果地塊面積的對數由$x$表示：
    * $h(x-3.75)$生成一個新特徵，當對數地塊面積小於3.75時為零，否則等於$x-3.75$
    * 相反的特徵$h(3.75-x)$在值高於3.75時為零
  - 圖6.5(a)說明了一對鉸鏈函數如何隔離預測變數空間的某些區域

* **分段迴歸模型**（Segmented Regression Models）：
  - 這些特徵可添加到模型中，創建具有不同趨勢的不同部分
  - 圖6.5(b)顯示了這種策略相關的模型擬合
  - 這種特徵生成函數在神經網絡和深度學習領域中被稱為**修正線性單元**（Rectified Linear Unit, ReLU）激活函數

* **基底函數的價值**：
  - 有效幫助建立更好的模型，表示特徵的非線性模式
  - 對探索性數據分析非常有效
  - 可視化（如圖6.4(b)和6.5）可用於了解預測應該具有的潛在函數形式（例如對數線性、二次、分段等）

### 6.2.2 將預測變數離散化作為最後手段 (Discretize Predictors as a Last Resort)

* **分箱**（Binning）/**分類**（Categorization）/**離散化**（Discretization）：
  - 將定量變數轉換為兩個或更多定性類別的過程
  - 例如：將變數轉換為分位數（如數據的前25%、25%至中位數等）

* **離散化的表面原因**：
  1. 簡化分析和/或結果解釋（例如，年齡分為40歲以上和以下）
  2. 避免需要指定預測變數與結果之間的關係
  3. 給人減少數據變異的印象

* **分箱方法**：
  - **無監督**：基於用戶驅動的截斷或估計百分位數（如中位數）
  - **監督**：優化切點位置（和數量）以提高性能
    * 例如：如果需要單一分割，可使用ROC曲線找到適當的靈敏度和特異度交換

* **將連續數據轉換為分類數據的問題**：
  1. 潛在趨勢極不可能與新模型一致
  2. 當存在真實趨勢時，離散化可能使模型更難有效工作，因為數據中的所有細微差別都被移除
  3. 可能沒有特定切點的客觀理由
  4. 當結果與預測變數之間沒有關係時，發現錯誤趨勢的概率大大增加

* **錯誤趨勢的模擬示例**（圖6.6）：
  - 圖6.6(a)顯示具有線性趨勢（係數為1.0）和標準差為4.0的正態分布誤差的模擬數據集
  - 線性迴歸顯示遞增趨勢，估計$R^2$為30.3%
  - 圖6.6(b)：將數據切割為10個等間距分箱並計算每個分箱的平均結果
    * 由於使用平均值，估計的$R^2$更大（64.2%），因為模型認為數據更精確（實際上並非如此）
  - 圖6.6(c)：模擬結果，其中預測變數無信息
    * 這種人為減少變異可能導致高假陽性率
    * 原始數據：在1000個數據集中看到的最大$R^2$值為18.3%
    * 離散化數據：$R^2$值往往更大（最大值為76.9%），19%的模擬$R^2 \geq 20\%$

* **使用離散化程序的建議**：
  1. 不應作為常規操作程序；分類預測變數應作為最後手段
  2. 分箱確定必須包含在重抽樣過程內：
     * 有助於診斷非存在關係是否由程序引起
     * 減輕使用信息豐富的預測變數時性能過度估計
     * 防止「目測」數據選擇切點的典型方法
     * 基於數據純視覺檢查的分箱策略更容易過擬合

## 6.3 多對多轉換 (Many:Many Transformations)

* 從多個預測變數創建新特徵時，可能解決多種問題：
  - 離群值
  - 共線性
  - 降低預測變數空間的維度
  - 改進性能和減少計算時間

### 6.3.1 線性投影方法 (Linear Projection Methods)

* **現代數據集特點**：
  - 通常包含大量預測變數，有時甚至超過樣本數量
  - 傳統思路認為：更多預測變數可能提高模型找到信號的機會
  - 實際情況：包含無關預測變數可能對最終模型產生負面影響（詳見第10章）
  - 負面影響包括：
    * 增加模型訓練計算時間
    * 降低預測性能
    * 使預測變數重要性計算複雜化

* **線性投影方法的作用**：
  - 有效識別原始預測變數的有意義投影
  - 具體技術包括：
    * **主成分分析**（Principal Components Analysis, PCA）
    * **核主成分分析**（Kernel PCA）
    * **獨立成分分析**（Independent Component Analysis, ICA）
    * **非負矩陣分解**（Non-negative Matrix Factorization, NNMF）
    * **偏最小平方法**（Partial Least Squares, PLS）
  - 前四種為無監督技術，不考慮響應變數
  - 最後一種使用響應變數指導降維過程

* **線性投影方法的工作原理**：
  - 將數值預測變數矩陣($X$)轉換為線性組合的新成分
  - 這些成分通常稱為**得分**（Scores）
  - 若有$n$個訓練集點和$p$個預測變數，得分可表示為$n \times p$矩陣$X^*$：
    $$X^* = XA$$
  - 其中$p \times p$矩陣$A$稱為**投影矩陣**（Projection Matrix）
  - 數據點$i$的第一個得分計算為：
    $$x^*_{i1} = a_{11}x_{i1} + a_{21}x_{i2} + \ldots + a_{p1}x_{ip}$$

* **維度降低**：
  - 可能只使用部分$A$，將得分數量從$p$減少到$k < p$
  - 例如：$A$的最後幾列可能對描述預測變數集沒有益處
  - 不應與特徵選擇混淆：
    * 如果預測變數空間可用$k$個得分變數描述，模型輸入的特徵更少
    * 但每個這些列都是所有原始預測變數的函數

* **不同投影方法的區別**：
  - 各種方法在如何確定投影值方面有所不同
  - 每種無監督技術有自己的目標，將原始預測變數投影到較低維度子空間
  - 壓縮信息可減少預測變數冗餘，從噪聲中提取潛在信號
  - 關鍵益處：減少訓練模型所需的計算時間
  - 局限性：無監督投影方法不保證改進預測性能
  - 主要假設：識別的子空間對響應變數有預測性（可能不成立）

* **監督技術優勢**：
  - PLS改進了無監督技術，專注於找到與響應變數直接相關的原始預測變數子空間
  - 通常找到更小、更高效的原始預測變數子空間
  - 缺點：需要更大的數據集進行測試和驗證，以避免過擬合

#### 6.3.1.1 主成分分析 (Principal Component Analysis)

* **PCA簡介**：
  - 在第4.2.7節中作為探索性視覺化技術介紹
  - 由Karl Pearson於一個多世紀前開發
  - 仍是最廣泛使用的降維工具之一
  - 優秀參考資料：Abdi和Williams (2010)

* **PCA目標**：
  - 找到原始預測變數的線性組合，總結原始預測變數空間中的最大變異量
  - 從統計角度看，變異與信息同義
  - 通過找到捕捉變異的原始預測變數組合，找到包含與預測變數相關信息的數據子空間
  - 新的PCA得分要求相互**正交**（即不相關）
  - 正交性使預測變數空間變異能以不重疊的方式整齊劃分
  - 所有主成分總結的變異恰好等於原始預測變數的變異
  - 實務中，計算足夠的新得分變數以解釋原始預測變數中預先指定的變異量（如95%）

* **PCA關鍵優勢**：
  - 結果PCA得分不相關
  - 此屬性對需要相對不相關預測變數的建模技術非常有用：
    * 多元線性迴歸
    * 神經網路
    * 支持向量機等

* **PCA適用場景**：
  - 數據由一個或多個包含冗餘信息的預測變數集群組成（高度相關的預測變數）
  - 高度相關的預測變數可視為存在於比原始數據更低的維度空間中
  - 範例（圖6.7）：
    * (a)部分：兩個線性相關的預測變數散點圖
    * 雖然數據在二維中測量，但只需一個維度（線）即可充分總結樣本位置
    * (b)部分：這兩個變數的組合顯示
    * 紅點是原始數據到第一主成分得分的正交投影
    * 新特徵用一個數值表示原始數據（從原點到投影值的距離）
    * 藍線長度表示「剩餘」信息（即第二個成分）

* **主成分迴歸**（Principal Component Regression）：
  - 描述先降維再對新成分得分進行迴歸的分步過程
  - 兩個步驟不連接，表示降維步驟與響應變數無直接關聯
  - 因此，與數據變異相關的主成分得分可能與響應變數相關，也可能無關

* **芝加哥乘客數據的PCA應用**：
  - 第4章首次探索的數據，許多車站乘客量高度相關（圖4.9）
  - 125個車站中，94個與另一車站的成對相關性大於0.9
  - 這意味著：已知一個車站的乘客量，可高精度估計許多其他車站
  - 乘客量信息可以更簡單地包含在這些數據的更低維度中
  - 本分析專注於1628天的週末數據，使用14天滯後預測變數

* **PCA結果分析**（圖6.9左側列）：
  - 前五個成分及其與結果的關係
  - 第一成分解釋滯後預測變數變異的60.4%，與乘客量存在強線性關係
  - 對應此線性組合的投影矩陣$A$所有係數均為正
  - 五個最有影響的車站：35th/Archer、Ashland、Austin、Oak Park、Western
  - 五個最不重要的車站：Central、O'Hare、Garfield、Linden、Washington/Wells
  - 第二成分僅解釋5%的變異性，與結果的相關性較低
  - 剩餘成分捕捉的原始預測變數信息越來越少
  - 覆蓋95%的原始信息需要共36個主成分

* **PCA視覺化**：
  - 圖6.10使用熱圖可視化前五個成分
  - 熱圖中，點的顏色表示$A$元素的符號和大小
  - 行基於使用五個成分的聚類程序重新排序
  - 行標籤顯示相應車站的線路
  - 第一列顯示：每個車站對第一成分的相對權重可靠地為正且小
  - 其他成分似乎分離特定線路（或線路內子集）

* **週末數據地理趨勢的PCA分析**（圖6.11）：
  - 尤其聚焦第二PCA成分的旋轉值（正負兼有）
  - 其符號和大小有助於了解哪些車站驅動此成分
  - 沿海岸線向北的Red和Purple線車站對第二成分有顯著正面影響
  - 沿Pink線的西南車站有較大負面影響（紅色標記較大）
  - O'Hare車站（極西北）也有負面影響
  - 靠近Clark和Lake的許多車站對第二車站影響較小或中等負面
  - 有趣的是：相距較近的兩個車站（Illinois Medical District和Polk）對成分有實質性影響但符號相反
  - 這兩個車站位於不同線路（分別為Blue和Pink），可能進一步表明數據中存在線路效應

* **PCA的視覺化方法**：
  - 常見做法是將前幾個得分相互繪製，尋找類別間分離、離群值或其他有趣模式
  - **雙標圖**（Biplot）(Greenacre 2010)：結合顯示得分和$A$係數的信息圖
  - 注意事項：由於每個PCA成分捕捉的變異性越來越少，得分的比例可能越來越小
  - 為避免過度解釋，應使用考慮所有數據的通用軸尺度繪製成分得分

#### 6.3.1.2 核主成分分析 (Kernel Principal Component Analysis)

* **傳統PCA的局限性**：
  - 僅在預測變數線性相關且結果得分與響應相關時有效
  - 預測變數空間的正交劃分可能與響應沒有良好的預測關係
  - 特別是當預測變數與響應間的真實關係為非線性時

* **非線性關係案例**：
  - 假設響應$y$與兩個預測變數$x_1$和$x_2$的關係為：
    $$y = x_1 + x_1^2 + x_2 + x_2^2 + \epsilon$$
  - 其中$\epsilon$表示隨機噪聲，且$x_1$和$x_2$高度相關
  - 傳統PCA會將$x_1$和$x_2$的關係總結為一個主成分
  - 但這種方法會忽略預測響應所必需的重要二次關係

* **核PCA原理**（Kernel PCA）：
  - 由Schölkopf, Smola和Müller (1998)及Shawe-Taylor和Cristianini (2004)開發
  - 將PCA的特定數學視角與核函數和核「技巧」結合
  - 使PCA能夠擴展進行降維的預測變數空間維度（類似基底展開）
  - 核心思想：PCA提出的方差總結問題可通過找到預測變數協方差矩陣的特徵值和特徵向量來解決
  - 協方差矩陣的特徵分解可等效地用樣本內積表示（「對偶」表示）
  - 內積稱為**線性核**（Linear Kernel）：
    $$k(x_1, x_2) = \langle x_1, x_2 \rangle = x_1^T x_2$$
  - 對於一組$i=1...n$個樣本和兩個預測變數，對應於：
    $$k(x_1, x_2) = x_{11} x_{12} + ... + x_{n1} x_{n2}$$

* **常見核函數**：
  - **多項式核**（Polynomial Kernel）：有一個參數$d$
    $$k(x_1, x_2) = \langle x_1, x_2 \rangle^d$$
    兩個預測變數且$d=2$時：
    $$k(x_1, x_2) = (x_{11} x_{12} + ... + x_{n1} x_{n2})^2$$
  
  - **徑向基函數核**（Radial Basis Function Kernel, RBF Kernel）：有一個參數$\sigma$
    $$k(x_1, x_2) = \exp\left(-\frac{\|x_1 - x_2\|^2}{2\sigma^2}\right)$$
    也稱為**高斯核**（Gaussian Kernel）

* **核PCA的優勢**：
  - 使用核方法擴展原始數據維度為可能有益的高維表示
  - 透過核技巧避免在$X$矩陣中創建這些項的麻煩
  - 使用核函數直接探索高維空間，解決優化問題

* **PCA與核PCA的比較**（圖6.12）：
  - 使用模擬數據演示核PCA的效用
  - 基於本節前面的模型生成200個樣本，其中150個作為訓練集
  - 圖6.12(a)：訓練數據中$x_1$和$x_2$的關係（線性）
  - 圖6.12(b)：揭示$x_1$和$y$之間的二次關係
  - 圖6.12(c)：PCA找到一個維度總結$x_1$和$x_2$，但與響應擬合較差
  - 圖6.12(d)：使用二次多項式核的核PCA能更好找到預測變數間的底層結構
  - 圖6.12(e)：測試集上的性能，核PCA模型的殘差明顯小於PCA模型

* **芝加哥數據的核PCA應用**：
  - 使用徑向基函數核獲得新PCA成分
  - 核的唯一參數($\sigma$)使用Caputo等人(2002)的方法分析估計
  - 合理值範圍為0.002至0.022，使用中間值0.006計算成分
  - 圖6.9中左側第二列顯示結果
  - 類似於普通PCA，第一個kPCA成分與乘客量有強關聯
  - 但基於核的成分在x軸上有更大的動態範圍
  - 圖中顯示的其他成分與常規PCA有所不同
  - 雖然這些得分與結果（單獨）關聯較少，但同時輸入迴歸模型時可能具有預測效用

#### 6.3.1.3 獨立成分分析 (Independent Component Analysis)

* **PCA與ICA的比較**：
  - PCA尋求解釋數據變異並產生相互正交的成分
  - 結果是成分不相關，但可能不是統計獨立的
  - 相關性是兩個變數間線性獨立性的度量
  - 例外：當底層值遵循高斯分布時，不相關數據也是統計獨立的

* **ICA基本原理**（Lee 1998）：
  - 類似PCA，創建原始變數的線性組合
  - 但目標是使成分盡可能相互統計獨立
  - 使ICA能夠建模比專注於正交性和線性關係的PCA更廣泛的趨勢
  - 有多種方法使ICA滿足統計獨立性約束
  - 通常目標是最大化結果成分的「非高斯性」
  - fastICA方法使用信息理論中的**負熵**（Negentropy）測量非高斯性（Hyvarinen和Oja 2000）
  - 除非預測變數表現出顯著的多變量正態性或嚴格線性趨勢，否則ICA創建的成分與PCA不同
  - 與PCA不同，成分沒有唯一排序

* **ICA數據預處理**：
  - 通常在運行ICA計算前對數據進行標準化和白化
  - **白化**（Whitening）：將原始值轉換為完整的PCA成分集
  - 這不會負面影響ICA目標並大幅降低計算複雜度（Roberts和Everson 2001）
  - ICA計算通常使用隨機參數值初始化，最終結果可能對這些值敏感
  - 為獲得可重複結果，手動初始化值有助於以後重現相同結果

* **芝加哥數據的ICA應用**：
  - 在估計成分前，對預測變數進行縮放，使白化前的ICA步驟使用中心化數據計算初始PCA數據
  - 未應用縮放時，ICA成分受不同個別車站強烈驅動
  - ICA成分顯示在圖6.9第三列，與其他方法顯著不同
  - 第一個成分與結果關係很小，包含一組離群值
  - 在此成分中，大多數車站對得分幾乎沒有貢獻
  - 有效果的許多車站靠近Clark和Lake
  - 42個車站對成分有負面影響（最強的是Chicago、State/Lake和Grand），但大多數有正係數（如Clark/Division、Irving Park和Western）
  - 其他重要車站沿Brown線向北以及Rosemont車站（O'Hare機場前）
  - 圖6.9所示的其他ICA成分與結果關係很弱
  - 對於這些數據，PCA尋找數據適當旋轉的方法更好地定位預測關係

#### 6.3.1.4 非負矩陣分解 (Non-negative Matrix Factorization)

* **NNMF簡介**（Gillis 2017; Fogel等人 2013）：
  - 一種線性投影方法，專用於大於或等於零的特徵
  - 算法找到$A$的係數，使其值也為非負（確保新特徵具有相同性質）
  - 流行應用：
    * 文本數據（預測變數為詞計數）
    * 圖像處理
    * 生物測量（如特定基因的RNA表達量）
  - 也適用於芝加哥數據

* **NNMF方法原理**：
  - 概念簡單：找到最佳係數集，使得分盡可能「接近」原始數據，同時滿足非負性約束
  - 接近度可使用**均方誤差**（Mean Squared Error，跨預測變數聚合）或其他測量方法
  - 另一種方法：**庫爾巴克-萊布勒散度**（Kullback–Leibler Divergence）(Cover和Thomas 2012)
  - 後者是測量兩個概率分布間距離的信息理論方法
  - 與ICA類似，數值求解器使用隨機數初始化，最終解決方案可能對這些值敏感
  - 成分順序是任意的

* **芝加哥數據的NNMF應用**：
  - 圖6.9所示的前五個非負矩陣分解成分與Clark和Lake車站的乘客量有適中關係
  - 在估計的20個NNMF成分中，第11個成分與乘客量關聯最大
  - 圖6.13顯示此成分的係數
  - 該成分受Pink線沿線車站（如54th/Cermak、Damen）、Blue線和Green線強烈影響
  - O'Hare機場的係數也做出很大貢獻

#### 6.3.1.5 偏最小平方法 (Partial Least Squares)

* **監督降維技術介紹**：
  - 迄今討論的技術都是無監督的，僅基於預測變數進行降維
  - 監督降維技術使用響應變數指導預測變數降維，使新預測變數與響應最佳相關
  - **偏最小平方法**（PLS）是PCA的監督版本，以與響應最佳相關的方式降低維度（Stone和Brooks 1990）

* **PLS目標**：
  - 找到與響應具最佳協方差的預測變數線性函數（稱為**潛變數**，Latent Variables）
  - 響應指導降維，使得分與訓練數據中的響應具有最高可能相關性
  - 典型實現要求每個新維度中的數據不相關（類似PCA優化問題中的約束）
  - 作為監督技術，PLS通常比PCA需要更少的成分達到相同性能水平（Kuhn和Johnson 2013）
  - 優勢：PLS可更高效地降低維度，節省內存和計算時間
  - 缺點：由於PLS使用響應確定最佳潛變數數量，必須使用嚴格的驗證方法確保方法不過擬合

* **芝加哥週末數據的PLS成分**（圖6.9）：
  - 第一個成分實際上與第一個PCA成分相同
  - 原因：PLS嘗試平衡預測變數降維與最大化成分與結果間相關性
  - 在這種情況下，PCA成分與乘客量有強線性關係，實現了PLS的兩個目標
  - 對於圖6.9所示的其他成分，第二個成分也與相應的PCA版本非常相似（直至符號）
  - 但其他兩個成分不同，與乘客量關聯略有改善

* **性能比較**：
  - 為量化每種方法估計滯後預測變數與結果關係的優劣，使用每種方法擬合線性模型
  - 每種情況下，計算20個成分並輸入簡單線性迴歸模型
  - 使用類似基於時間的重抽樣方案
  - 初始分析集為800個週末，兩組週末用作評估集（加入下次迭代的分析集）
  - 共25次重抽樣，每次重新計算投影方法
  - 圖6.14顯示每種方法的RMSE值的置信區間
  - 雖然置信區間重疊，但有跡象表明PLS和核PCA有潛力優於原始值
  - 對於對預測變數間相關性非常敏感的其他模型（如神經網路），這些轉換方法可能提供實質性更好的性能

### 6.3.2 自編碼器 (Autoencoders)

* **自編碼器簡介**：
  - 計算複雜的多變量方法，用於尋找預測變數數據的表示
  - 常用於深度學習模型（Goodfellow、Bengio和Courville 2016）
  - 基本思想：創建原始預測變數數據與一組人工特徵（通常相同大小）之間的非線性映射
  - 這些新特徵（可能沒有任何合理解釋）用作模型預測變數
  - 與前述投影方法有許多相似之處，但在新特徵導出方式和潛在益處方面非常不同

* **自編碼器的適用情境**：
  - 當有大量未標記數據（即結果尚未知）時
  - 例如：製藥公司可能擁有數百萬化合物，但只有很小比例具有新藥發現項目所需的相關結果數據
  - 化學中的常見任務：創建使用化合物化學結構預測其作為藥物好壞的模型
  - 可以為大多數化學數據庫計算預測變數，這些數據（儘管缺乏實驗室結果）可用於改進模型

* **自編碼器原理示例**：
  - 考慮擬合簡單線性迴歸模型的情境
  - 矩陣$X$表示預測變數數據，向量$y$表示結果
  - 使用最小二乘法估計迴歸係數：$\hat{\beta} = (X'X)^{-1}X'y$
  - 注意：結果數據僅在等式右側部分使用（$X'y$）
  - 未標記數據可用於生成$(X'X)^{-1}$的很好估計並保存
  - 當結果數據可用時，可以使用相對少量的標記數據計算$X'y$
  - 通過在大量數據上創建$(X'X)^{-1}$，仍可使用上述等式產生更好的參數估計

* **自編碼器的架構**：
  - 可通過神經網絡結構創建：
    * 輸入層到隱藏單元以及隱藏單元之間的函數是非線性的
    * 常用函數包括簡單S型曲線和ReLU函數
    * 到輸出特徵的連接通常是線性的
  - 一種策略：在隱藏單元中使用相對較少的節點，迫使自編碼器學習預測變數中的關鍵模式
  - 也可能需要多層有許多節點來最佳表示預測變數值

* **自編碼器的實際考量**：
  - 網絡圖中的每條線代表需要估計的參數，數量可能非常大
  - 控制過擬合的成功方法：
    * 正則化
    * **丟棄法**（Dropout Methods）
    * 高效的深度學習庫減輕計算負擔
  - 最大益處發生在數據量大的情況
  - 神經網絡和深度學習模型擬合細節超出本書範圍
  - 技術細節參考：Bishop (2011)和Goodfellow、Bengio、Courville (2016)
  - 擬合這類模型的優秀指南：Chollet和Allaire (2018)
  - 衡量性能：通常使用觀測值與預測值（預測變數）之間的**均方根誤差**（RMSE）

* **自編碼器在建模過程中的整合**：
  - 常見做法：將自編碼器模型估計的關係視為「已知」值，在重抽樣過程中不重新估計
  - 深度學習（影像和文本數據）中常見做法：在新項目中使用預訓練網絡
  - 預訓練部分可作為網絡的前端
  - 如果有大量數據用於此模型，且模型擬合過程收斂，此方法可能具有一定統計有效性

* **藥物發現案例應用**：
  - 使用Karthikeyan、Glen和Bender (2005)的數據
  - 使用化學描述符建模化合物的**熔點**（固態到液態的轉變）
  - 包含4401個化合物和202個數值預測變數
  - 模擬剛起步的藥物發現項目：
    * 隨機選取50個數據點作為訓練集
    * 另25個點作為測試集
    * 剩餘4327個數據點視為無標記數據
  - 數據預處理步驟：
    * 消除126個預測變數，確保無成對相關性大於0.75
    * 使用Yeo-Johnson轉換改變剩餘76個預測變數的尺度（減少偏斜）
    * 中心化和縮放所有預測變數
  - 使用具有兩個隱藏層（每層512個單元）的自編碼器建模預測變數
  - 使用**雙曲正切激活函數**（Hyperbolic Tangent Activation Function）
  - 總模型參數數量：341,068
  - 使用隨機20%的無標記數據組成的驗證集RMSE衡量性能
  - 模型訓練500次迭代，圖6.15(a)顯示RMSE在前50次迭代大幅下降，約100次迭代達最小值，之後緩慢增加
  - 將基於第100次迭代係數的自編碼器模型應用於訓練集和測試集

* **自編碼器結果分析**：
  - 對原始和編碼版本的訓練集擬合K最近鄰模型
  - 測試1至20個鄰居，使用五次重複的10折交叉驗證測量的RMSE結果選擇最佳模型
  - 重抽樣配置文件顯示在圖6.15(b)
  - 編碼數據的最佳設置（7個鄰居，估計RMSE為55.1°）
  - 原始數據的最佳設置（15個鄰居，RMSE = 58°）
  - 差異為3度，95%置信區間為(0.7, 5.3)度
  - 雖然這是中等程度的改進，但對藥物發現項目而言，如果模型能更好預測新化合物的有效性，可能產生實質性影響
  - 使用兩個KNN模型預測測試集：
    * 基於原始訓練集預測變數的模型表現很差（RMSE為60.4度）
    * 具有編碼值的KNN模型產生與重抽樣結果一致的結果（RMSE值為51）
  - 使用不同隨機數種子重複整個過程，驗證結果非偶然產生

### 6.3.3 空間符號變換 (Spatial Sign)

* **空間符號變換原理**：
  - 取一組預測變數並進行轉換，使新值與分布中心的距離相同
  - 本質上，數據被投影到多維球體上，使用以下公式：
    $$x^*_{ij} = \frac{x_{ij}}{\sqrt{\sum_{j=1}^p x_{ij}^2}}$$
  - 也稱為**全局對比度標準化**（Global Contrast Normalization）
  - 經常用於圖像分析

* **空間符號變換的應用考量**：
  - 需要計算數據的標準，因此應在空間符號之前進行預測變數的中心化和縮放
  - 如果任何預測變數有高度偏斜分布，可能受益於誘導對稱性的轉換（如Yeo-Johnson技術）

* **案例應用**（Reid 2015）：
  - 使用動物糞便特徵預測真實動物種類
  - 圖6.16(a)顯示兩個預測變數，樣本按真實物種著色
  - 錐度指數在y軸上顯示顯著離群值
  - 將轉換應用於這兩個預測變數後，圖6.16(b)中的新值顯示數據與中心（零）等距
  - 雖然轉換數據在新模型中看起來可能尷尬，但此轉換對減輕極端離群值的損害非常有效

### 6.3.4 距離和深度特徵 (Distance and Depth Features)

* **半監督特徵在分類中的應用**：
  - 在分類中，從數據創建半監督特徵可能有益
  - 這意味著使用結果類別創建新預測變數，但不是為了優化預測準確性
  - 例如：從訓練集計算每個類別的預測變數平均值（通常稱為**類別中心**，Class Centroids）
  - 對於新數據，可以計算到每個類別中心的距離，作為特徵
  - 基本上將準最近鄰技術嵌入到使用這些特徵的模型中

* **類別中心距離的應用**（圖6.17(a)）：
  - 顯示使用對數碳氮比和$\delta^{13}C$特徵的糞便數據的類別中心
  - 基於這些多維平均值，可以直接計算新樣本到每個類別中心的距離
  - 如果新樣本更接近特定類別，其距離應該很小
  - 圖6.17(a)顯示這種方法的示例，新樣本（*表示）相對接近山貓和郊狼中心
  - 可以使用適當的不同距離測量方法
  - 在模型中使用此方法時，為每個類別創建新特徵，測量每個樣本到相應中心的距離

* **數據深度概念**（Ghosh和Chaudhuri 2005; Mozharovskyi、Mosler和Lange 2015）：
  - 數據深度測量數據點與其分布中心的接近程度
  - 有多種計算深度的方法
  - 通常類別距離與數據深度之間存在反比關係
  - 圖6.17(b)顯示假設每個類別數據的二元正態性的概率分布
  - 在這種情況下，使用概率分布測量深度提供比面板(a)中顯示的簡單歐幾里德距離略高的靈敏度
  - 在此例中，被預測的新樣本位於郊狼的概率區域內，程度高於其他物種

* **深度測量的多樣性和應用**：
  - 可以使用各種深度測量方法
  - 許多方法不需要關於預測變數分布的特定假設
  - 可能對離群值具有穩健性
  - 類似距離特徵，可以計算特定於類別的深度並添加到模型中
  - 這些類型的特徵可以增強某些模型預測結果的能力
  - 例如：如果真實類別邊界是對角線，大多數分類樹將難以模擬此模式
  - 在這種情況下，使用距離或深度特徵補充預測變數集可以使此類模型受益

## 6.4 總結 (Summary)

* 數值特徵在原始形式中，可能無法有效地使模型找到與響應變數的良好關係
* 例如：某預測變數可能被測量，但其平方版本才是真正與響應相關的

## 基本轉換的必要性 (Necessity of Basic Transformations)

* 一些簡單但必要的轉換對某些模型發現預測信號至關重要：
  - **中心化**（Centering）
  - **縮放**（Scaling）
  - 將分佈轉換為對稱性（Transforming a Distribution to Symmetry）

* 其他更複雜的轉換可以將原始尺度的預測變數轉換為可能更具資訊性的非線性尺度：
  - **基底展開**（Basis Expansions）
  - **樣條函數**（Splines）

### 計算效率與維度考量 (Computational Efficiency and Dimensional Considerations)

* 擴展預測變數空間會帶來計算成本
* 探索預測變數與響應間非線性關係的另一種方式：
  - 結合**核函數**（Kernel Function）和**PCA**（Principal Component Analysis）
  - 這種方法計算效率高
  - 能夠探索更大維度的空間

* 有時需要降低預測變數的維度，可通過多種技術實現：
  - **無監督技術**（Unsupervised Techniques）：
    * **主成分分析**（PCA）
    * **獨立成分分析**（ICA）
    * **非負矩陣分解**（NNMF）
  - **監督方法**（Supervised Approach）：
    * **偏最小平方法**（PLS）

### 創新特徵工程方法 (Novel Engineering Approaches)

* 其他創新方法提供了獨特的特徵工程途徑：
  - **自編碼器**（Autoencoders）：
    * 利用未標記數據中的信息
  - **空間符號轉換**（Spatial Sign Transformation）：
    * 減弱極端樣本的影響
  - **距離和深度測量**（Distance and Depth Measures）：
    * 提供新的特徵創建方式

# 7 檢測交互作用效應 (Detecting Interaction Effects)

* 在以預測為主要目的的問題中，響應變數中的大部分變異可以由重要個別預測變數的累積效應解釋
* 對許多問題而言，響應變數中的額外變異可以通過兩個或更多預測變數協同工作的效應來解釋

## 交互作用的本質 (Nature of Interactions)

* **交互作用的定義**：
  - 當兩個或更多預測變數的組合效應不同於（小於或大於）分別考慮每個效應時加總的預期效應
  - 交互作用始終在預測變數如何與結果相關的背景下定義
  - 預測變數之間的相關性與是否存在交互作用效應無直接關係
  - 在交互作用之外的個別變數（如肥料和水）被稱為**主效應項**（Main Effect Terms）

* **交互作用的概念性例子**：水和肥料對玉米產量的影響
  - 有肥料但無水：作物無產量，因為水是植物生長的必要條件
  - 有足夠的水但無肥料：作物將產生一些產量
  - 足夠的水和足夠的肥料：產量最優，超過單獨使用任一因素

## 交互作用的視覺示例 (Visual Examples of Interactions)

* **缺血性中風風險預測案例**（第2章）：
  - 動脈壁重塑程度與動脈壁最大厚度之間的交互作用
  - 圖2.7(a)顯示這些預測變數之間的關係，等值線表示預測變數間的等效乘法效應
  - 圖2.7(b)說明這些預測變數的組合如何增強中風與非中風患者之間的分離

* **Ames住房數據例子**（圖7.1）：
  - 連續預測變數與分類預測變數間的交互作用
  - 圖7.1(a)：房屋年齡（x軸）與售價（y軸）的關係，按是否有空調分類
    * 有空調的房屋：年齡與售價呈正相關
    * 無空調的房屋：年齡與售價無明顯關係
  - 圖7.1(b)：不存在交互作用的例子 - 房屋年齡與整體狀況
    * 房屋狀況不影響年齡與售價的關係
    * 缺乏交互作用通過每種狀況類型的近乎平行線表現出來

## 交互作用的數學表示 (Mathematical Representation of Interactions)

* **兩個預測變數交互作用的數學表示**：
  $$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + error$$
  
  - $\beta_0$：整體平均響應
  - $\beta_1$和$\beta_2$：分別代表由於$x_1$和$x_2$的平均變化率
  - $\beta_3$：超出$x_1$和$x_2$單獨解釋能力的$x_1$和$x_2$組合效應的增量變化率
  - error項：真實數據中無法用方程決定性部分解釋的隨機變異

* **參數估計方法**：
  - 連續響應：**線性迴歸**（Linear Regression）
  - 分類響應：**邏輯迴歸**（Logistic Regression）

## 交互作用的四種類型 (Four Types of Interactions)

1. **加性**（Additive）：
   - $\beta_3$與零無顯著差異
   - $x_1$和$x_2$之間的關係是加性的
   - 交互作用對解釋響應變異無用

2. **拮抗性**（Antagonistic）：
   - $\beta_3$顯著為負，而$x_1$和$x_2$單獨也影響響應
   - 組合效應小於單獨效應之和

3. **協同性**（Synergistic）：
   - $\beta_3$為正，而$x_1$和$x_2$單獨也影響響應
   - 組合效應大於單獨效應之和

4. **非典型**（Atypical）：
   - $\beta_3$與零顯著不同，但$x_1$或$x_2$（或兩者）不影響響應
   - $x_1$在$x_2$各值（或反之）的平均響應的變化率基本為零
   - 但$x_1$在每個$x_2$值（或在$x_2$的條件下）的平均值與零不同
   - 這種情況在真實數據中很少見
   - 視覺表現：分類預測變數情況下的交叉模式（圖7.3）

## 模擬數據示例 (Simulated Examples)

* **前三種交互作用類型的模擬**（圖7.2）：
  - 使用上述公式生成數據，係數如下：$\beta_0=0$，$\beta_1=\beta_2=1$
  - $\beta_3=-10$：拮抗性交互作用
  - $\beta_3=0$：無交互作用
  - $\beta_3=10$：協同性交互作用
  - 生成200個樣本，$x_1$和$x_2$在0至1之間均勻分布，誤差服從正態分布
  - 使用線性迴歸模型估計係數並生成各模型預測值的等高線圖
  - 結果顯示：
    * 加性情況：等高線平行，響應從左下到右上增加
    * 協同情況：等高線彎曲，表明需要較低的預測變數值產生相同響應
    * 拮抗情況：等高線彎曲，響應從左下到右上降低

* **非典型交互作用的模擬**（圖7.3）：
  - 兩個分類預測變數具有「低」和「高」值
  - 響應由$y = 2x_1x_2 + error$生成，$x_1$和$x_2$的低/高值分別用-1/+1表示
  - $x_1$在低水平和高水平的平均響應約為零
  - 但當以$x_2$為條件時（由兩種不同顏色表示），響應在$x_1$的兩個水平上相反
  - 每個預測變數的個別效應不顯著，但條件效應強烈顯著

## 交互作用檢測的重要性 (Importance of Detecting Interactions)

* **複雜建模技術中的交互作用**：
  - 多項實證研究表明，複雜模型可發現交互作用
  - Elith、Leathwick和Hastie (2008)：基於樹的模型通過後續遞歸分割本質上建模預測變數之間的交互作用
  - García-Magariños等人(2009)：隨機森林有效識別單核苷酸多態性之間的未知交互作用
  - Lampa等人(2014)：提升樹模型可揭示流行病學問題中的重要交互作用
  - Chen等人(2008)：搜索技術結合支持向量機有效識別與人類疾病相關的基因-基因交互作用

* **為何需要明確識別交互作用**：
  - 如果複雜模型已能發現交互作用，為何還需特別識別？
  - 關鍵在於特徵工程的基本目標：創建包含預測相關信息以提高模型有效性的特徵
  - 通過識別和創建相關交互作用項，可以提高具有更好可解釋性的模型的預測能力
  - 改進單個預測變數形式後，重點應轉向搜索可能有助於解釋響應中額外變異並提高模型預測能力的預測變數交互作用

## 本章焦點 (Chapter Focus)

* 探索如何搜索和識別能改善模型預測性能的預測變數交互作用
* 主要使用Ames住房數據
* 基礎模型包含多個變數：
  - 連續預測變數：一般生活區域、地塊面積和正面、建造和銷售年份、泳池面積、經緯度和浴室數量
  - 定性預測變數：鄰里、建築類型、中央空調、MS子類、基礎類型、屋頂樣式、小巷類型、車庫類型和土地輪廓
* 重點在於發現這些預測變數的有益交互作用

## 7.1 交互作用搜索的指導原則 (Guiding Principles in the Search for Interactions)

### 專家知識的重要性 (Importance of Expert Knowledge)

* 在研究交互作用時，系統的專家知識是關鍵
* Neter等人(1996)強調專家知識的重要性，建議：
  - 「在可能的情況下，事先識別那些最可能以重要方式影響響應變數的交互作用」
* 專家選擇的交互作用應首先被探索
* 許多情況下，專家指導可能無法獲得，或研究領域太新，沒有先驗知識
* 在這種情況下，需要合理的指導原則來引導交互作用項的選擇

### 統計實驗設計的指導原則 (Guidance from Statistical Experimental Design)

* **統計實驗設計**（Statistical Experimental Design）領域提供了解決這一問題的方法
* 實驗設計採用**控制**（Control）、**隨機化**（Randomization）和**重複**（Replication）原則
* 建立框架收集證據，確立自變數（預測變數）與依變數（響應）之間的因果關係
* **篩選設計**（Screening Designs）：以結構化方式生成信息，識別重要因素及其交互作用

* 實驗數據雖然能估計所有可能的交互作用，但不是所有交互作用都能解釋響應變異的顯著部分
* 挑戰在於確定哪些交互作用真正重要

### Wu和Hamada框架 (Wu and Hamada Framework)

* Wu和Hamada (2011)提供了識別顯著交互作用的框架
* 該框架基於三個核心概念：
  1. **交互作用層級原則**（Interaction Hierarchy Principle）
  2. **效應稀疏性原則**（Effect Sparsity Principle）
  3. **效應遺傳原則**（Effect Heredity Principle）
* 這些原則在實驗設計生成的數據中有效識別最相關效應
* 可擴展到預測建模框架

#### 1. 交互作用層級原則 (Interaction Hierarchy Principle)

* 交互作用的階次越高，解釋響應變異的可能性越小
* 搜索順序應為：
  - 首先搜索**成對交互作用**（Pairwise Interactions）
  - 然後是**三因素交互作用**（Three-way Interactions）
  - 再是**四因素交互作用**（Four-way Interactions）等
* 圖7.4通過三個因素的數據展示了效應層級的圖示，節點大小表示項預測響應的可能性
* Neter等人(1996)建議謹慎包括三因素及更高階交互作用：
  - 高階項很少解釋響應變異的顯著部分
  - 幾乎不可能解釋

#### 2. 效應稀疏性原則 (Effect Sparsity Principle)

* 只有一小部分可能的效應真正解釋了響應變異的顯著部分（圖7.4）
* 在搜索優化模型預測能力的項時，可大幅減少可能的主效應和交互作用項

#### 3. 效應遺傳原則 (Effect Heredity Principle)

* 基於遺傳學原則
* 主張：只有當交互作用之前的有序項有效解釋響應變異時，才能考慮交互作用項
* 有兩種可能的實現方式：

##### 強遺傳 (Strong Heredity)

* 要求：要考慮交互作用，所有較低級別的前置項必須解釋響應變異的顯著部分
* 例如：只有當$x_1$的主效應和$x_2$的主效應各自解釋顯著量的響應變異時，才考慮$x_1 \times x_2$交互作用
* 如果只有一個因素顯著，則不考慮交互作用

##### 弱遺傳 (Weak Heredity)

* 放寬要求：可考慮與顯著因素的任何可能交互作用
* 圖7.5說明了三因素實驗中強遺傳和弱遺傳的區別：
  - 若$x_1$主效應被發現能預測響應
  - 弱遺傳原則會探索$x_1x_2$和$x_1x_3$交互作用項
  - 但會排除$x_2x_3$交互作用項
  - 強遺傳原則不考慮任何二階項，因為只有一個主效應顯著

### 原則的局限性 (Limitations of Principles)

* 這些原則在尋找相關且有預測性的項時雖然有效，但並非總是適用
* 自然界中存在複雜的高階交互作用：
  - Bairey、Kelsic和Kishony (2016)表明：物種間的高階交互作用可影響生態系統多樣性
  - 人體是複雜的化學和生物交互作用的溫床：多個系統同時協同工作對抗病原體並維持生命
* 這些例子強調：識別重要交互作用項的最佳方法應結合：
  - 對所建模系統的專家知識
  - 基於可靠統計原則的仔細設計的搜索程序

## 7.2 實際考量 (Practical Considerations)

* 在尋找預測性交互作用時，需要解決幾個實際問題，特別是當缺乏專家知識來建議優先篩選的項
* 在討論搜索方法前需要思考的關鍵問題：
  - 現有數據是否可能列舉並評估所有可能的預測性交互作用？
  - 如果可以，是否應該評估所有交互作用？
  - 應該在預處理原始預測變數之前還是之後創建交互作用項？

### 列舉所有交互作用項的問題 (Question of Enumerating All Interaction Terms)

* **交互作用層級原則**和**效應稀疏性原則**影響這一決策
* 雖然某些問題中可能存在高階交互作用（三因素及以上），但它們：
  - 很少發生（效應稀疏性）
  - 可能只對模型預測能力提供小幅改進（交互作用層級）
* 建議：
  - 只有專家知識建議時才篩選高階交互作用
  - 更可能的情況是：一小部分成對交互作用含有預測相關信息

* **列舉成對交互作用的規模問題**：
  - 對於$p$個預測變數，存在$(p)(p-1)/2$個成對交互作用項
  - 隨著預測變數數量增加，交互作用項數量呈指數增長
  - 僅100個原始預測變數就需要搜索4,950個項
  - 500個預測變數需要評估近125,000個成對項！
  - 對於含有中等數量預測變數的數據，需要更策略性的搜索方法（將在後續章節討論）

### 交互作用項創建的時機 (Timing of Interaction Term Creation)

* **關鍵問題**：相對於預處理步驟，何時應創建交互作用項？
* 第6章回顧：數值預測變數的預處理步驟可能包括：
  - 中心化（Centering）
  - 縮放（Scaling）
  - 維度擴展（Dimension Expansion）
  - 維度降低（Dimension Reduction）
* 這些步驟有助於模型更好地發現預測變數與響應之間的預測關係
* 需要了解：預處理和創建交互作用項的操作順序是否影響發現預測重要交互作用的能力

### 中風數據案例分析 (Stroke Data Case Study)

* 使用第2章中風數據的最大重塑比與面積最大狹窄度之間的交互作用為例
* 數據預處理步驟包括：中心化、縮放和單獨轉換
* 圖7.6比較了交互作用項在預處理步驟前後創建時中風組的分布：
  - (a)部分：先創建交互作用項，再進行預處理步驟
  - (b)部分：先預處理原始預測變數，再創建交互作用項

* **關鍵發現**：
  - 當先創建交互作用項再進行預處理時，交互作用的信號（由組分布間的移位捕獲）得以保留
  - 然而，當在創建交互作用項之前先預處理原始預測變數時，交互作用預測信號幾乎完全丟失
  - 這個案例表明應該仔細考慮在哪個步驟創建交互作用項

### 實用建議 (Practical Recommendations)

* 一般而言，交互作用在原始測量尺度上最為合理且易於解釋
* 建議：交互作用項應該在任何預處理步驟之前創建
* 同時檢查這些步驟順序的影響可能是明智的

## 7.3 識別預測性交互作用的暴力方法 (The Brute-Force Approach to Identifying Predictive Interactions)

* 對於具有少量到中等數量預測變數的數據集，可以評估所有可能的成對交互作用
* 警告：評估的交互作用項越多，發現僅由於隨機機會而與響應相關的交互作用的概率越高
* **假陽性發現**（False Positive Findings）：僅由於隨機機會而非真實關係而具有統計顯著信號的項
* 統計學中有專門研究控制假陽性發現機會的子領域（Dickhaus 2014）
* 假陽性發現可能大幅降低模型的預測性能，需要防止選擇這類發現

### 7.3.1 簡單篩選 (Simple Screening)

* **傳統篩選方法**：使用**嵌套統計模型**（Nested Statistical Models）
* 線性迴歸模型示例：
  - 主效應模型：$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + error$
  - 主效應加交互作用模型：$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2 + error$
  - 這兩個模型被稱為「嵌套」，因為第一個模型是第二個模型的子集

* **統計比較**：
  - 比較兩個模型捕獲的額外信息量
  - 線性迴歸：比較殘差誤差，評估調整自由度後的誤差改進是否足夠真實
  - p值反映由於隨機機會而捕獲額外信息的概率
  - 小p值（如小於0.05）表示額外信息由於隨機性的機會小於5%
  - 5%是假陽性發現率，為歷史性經驗法則
  - 如果願意承擔更多假陽性風險，可設置更高的閾值

* **替代方法**（第2.3節）：
  - 使用重抽樣創建不同版本訓練集的多個嵌套模型對
  - 使用評估集評估目標函數（避免過擬合問題）
  - 可使用任何性能指標比較嵌套模型，不限於統計上可處理的指標
  - 第2.3節使用ROC曲線下面積作為目標函數
  - 這提供了極大的多樣性，使篩選能與適當的建模技術和性能指標匹配

* **兩種方法的差異**：
  - 兩種評估嵌套模型的方法不保證產生一致結果
  - 不同目標函數回答不同問題，沒有背景不能斷言哪個客觀更好
  - Friedman (2001)展示了度量之間的潛在差異
    * 使用二項式似然和錯誤率選擇提升樹的最佳迭代次數
    * 似然作為目標函數時，選擇的迭代次數明顯少於使用錯誤分類率時
    * 過擬合損害似然但改進錯誤率：這些指標衡量擬合質量的不同方面

* **控制假陽性發現**：
  - **不控制**：可在早期探索潛在交互作用時使用，但可能發現一個或多個假陽性
  - **Bonferroni校正**（Shaffer 1995）：使用嚴格的指數懲罰最小化假陽性，但不適用於多次統計檢驗
  - **假發現率**（FDR）（第5.6節）：介於兩者之間，為進行大量檢驗時最小化假陽性而開發
  - 對於全面列舉的成對交互作用，FDR是不調整和Bonferroni校正之間的實用折衷方案

* **方法局限性**：
  - 第2.3節的交互作用檢測方法過於簡化，僅包括交互作用中涉及的項
  - 當將發現的交互作用包含在包含其他（可能相關）預測變數的更廣泛模型中時，其重要性可能降低
  - 本章中，交互作用被添加到包含所有潛在相關預測變數的模型中
  - 這可能減少被認為重要的預測變數數量（剩餘自由度較小），但發現的交互作用可能更可靠地對更大模型重要

* **Ames數據示例**：
  - 使用傳統方法和通過重抽樣得出的p值比較嵌套模型
  - 基本模型中許多預測變數為定性，在群組級別進行主效應和交互作用分析
  - 圖7.7顯示每種計算方法和調整類型下p值分布
  - 傳統方法識別幾乎所有交互作用效應為顯著（即使在FDR或Bonferroni調整後）
  - 交叉驗證方法的顯著率較低，更受兩種校正影響
  - 可能是由於使用不同估計方法；傳統方法重新預測原始訓練集，許多顯著發現可能是過擬合交互作用的結果

* 顯著交互作用列表可作為探索性數據分析的起點
* 例如：使用重抽樣，最小p值的潛在交互作用是經度和緯度之間
* 應使用可視化確定效應是否具有實際顯著性和潛在意義
* 然後建模者可確定編碼交互作用的最佳方法

### 7.3.2 懲罰迴歸 (Penalized Regression)

* **一次一項評估的局限性**：
  - 無法評估交互作用項在完整模型（包含所有個體預測變數和成對交互作用項）中的重要性
  - 如果全面列舉可行，另一方法是創建並添加所有交互作用項到數據
  - 結果：數據可能包含比樣本更多的預測變數（原始預測變數加交互作用項）

* **不同模型的適用性**：
  - 適合：**樹**（Trees）或其集成、**神經網路**（Neural Networks）、**支持向量機**（Support Vector Machines）和**K最近鄰**（K-nearest Neighbors）
  - 不直接適用：**線性迴歸**（Linear Regression）和**邏輯迴歸**（Logistic Regression）
  - 特徵工程目標：識別改進模型性能同時提高可解釋性的特徵
  - 需要方法在預測變數多於樣本的情況下使用線性和邏輯迴歸

* **懲罰模型**（Penalized Models）：
  - 開發用於處理預測變數多於樣本但需要可解釋模型的情況
  - 線性迴歸目標：找到最小化觀測值與預測值間平方距離（誤差）總和的係數$\beta$
  - 平方誤差和（SSE）： $SSE = \sum_{i=1}^n (y_i - \hat{y}_i)^2$
  - 傳統解法需要求逆預測變數的協方差矩陣
  - 當預測變數多於樣本或某預測變數可寫為其他預測變數的組合時，無法求逆
  - 當預測變數相互高度相關時，估計的迴歸係數變大（膨脹）並不穩定

* **嶺迴歸**（Ridge Regression）(Hoerl 1970)：
  - 優化問題修改為：$SSE_{L2} = \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \lambda_r \sum_{j=1}^P \beta_j^2$
  - $\lambda_r$項稱為懲罰
  - 當迴歸係數變大時，懲罰增加，促使迴歸係數變小並趨向零
  - 可獲得合理參數估計，但許多參數估計可能不為真零，使結果模型解釋更困難

* **LASSO迴歸**（Least Absolute Shrinkage and Selection Operator）(Tibshirani 1996)：
  - 優化問題修改為：$SSE_{L1} = \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \lambda_{\ell} \sum_{j=1}^P |\beta_j|$
  - 通過修改懲罰項$\lambda_{\ell}$，LASSO方法迫使迴歸係數為零
  - 實際上選擇模型項至最佳預測變數子集
  - 可用於整個預測變數和交互作用數據集，選擇產生最佳模型性能的項
  - Friedman、Hastie和Tibshirani (2010)擴展了此技術，使其適用於分類設置

* **懲罰類型的關聯任務**：
  - 嶺懲罰主要用於對抗預測變數間的共線性
  - LASSO懲罰用於消除預測變數
  - 某些數據集可能需要兩種效應創建有效模型

* **彈性網**（Elastic Net）(Zou和Hastie 2005; Friedman、Hastie和Tibshirani 2010)：
  - 結合兩種懲罰類型
  - glmnet模型具有總懲罰參數$\lambda = \lambda_r + \lambda_{\ell}$和與LASSO懲罰相關的$\lambda$比例（通常表示為$\alpha$）
  - $\alpha = 1$：完全LASSO懲罰模型
  - $\alpha = 0.5$：均勻混合
  - 回歸模型優化：$SSE = \sum_{i=1}^n (y_i - \hat{y}_i)^2 + (1-\alpha)\lambda_r \sum_{j=1}^P \beta_j^2 + \alpha\lambda_{\ell} \sum_{j=1}^P |\beta_j|$

* **Ames數據應用**（圖7.8）：
  - 使用基本模型加所有可能的交互作用通過重抽樣調優模型
  - 每條線表示給定$\alpha$值的估計RMSE與$\lambda$之間的關係
  - 隨著模型更類似LASSO，RMSE降低
  - 最佳：純LASSO模型，$\lambda = 0.003$
  - 估計RMSE：0.076對數單位
  - 從1,033個預測變數中，模型選擇了115個項（僅3個為主效應）
  - LASSO似乎主要偏好模型中的交互作用項
  - 此技術不強制執行層級原則，但Bien、Taylor和Tibshirani (2013)為該模型開發了一種方法

* **主要發現的交互作用**：
  - 係數最大的交互作用項：Year Built（建造年份）× Living Area（居住面積）
  - 圖7.8(b)顯示此預測變數具有較大的對數線性效應
  - 表7.1顯示前15個交互作用項，某些模型項多次出現（如neighborhood、living area等）

## 7.4 當完全列舉實際上不可能時的方法 (Approaches when Complete Enumeration is Practically Impossible)

* 隨著預測變數數量增加，需要探索的交互作用項數量呈指數級增長
* 僅僅中等數量的幾百個預測變數（在如今的數據中可能算小）就會產生太多無法從實際或統計角度考慮的交互作用
* 計算負擔同樣呈指數級增長
* 即使有足夠的計算資源可能評估大量交互作用，從統計角度這也不明智，因為可能發現假陽性結果
* 由於重要交互作用通常很少，篩選的無意義交互作用越多，評估這些交互作用的懲罰越大
* 如果計算所有可能的交互作用並進行適當調整，真正顯著的交互作用可能被遺漏

* 當有大量交互作用項需要評估時，需要其他方法使搜索更有效率，同時仍能有效發現重要交互作用
* 本節將介紹幾種已開發的方法，用於發現預測變數之間的交互作用

### 7.4.1 指導原則與兩階段建模 (Guiding Principles and Two-stage Modeling)

* **兩階段建模方法**：
  - 第一階段：使用不直接考慮交互作用效應的模型評估預測變數解釋響應變異的能力
    * 例如：線性或邏輯迴歸
  - 第二階段：確定重要預測變數後，計算此模型的殘差
    * 這些殘差含有個別預測變數本身無法解釋的信息
    * 未解釋的變異可能來自隨機測量誤差、未獲得或未測量的預測變數，或模型中不存在的觀測預測變數間的交互作用

* **兩階段方法理論基礎**：
  - 假設觀測數據由方程式生成：$y = x_1 + x_2 + 10x_1x_2 + error$
  - 但收集數據時只觀測到響應和$x_1$
  - 最佳模型估計將限制為：$y = \beta_1x_1 + error^*$
  - 由於真實響應依賴於$x_2$和$x_1$與$x_2$之間的交互作用，不完美模型的誤差($error^*$)包含這些重要項的信息
  - 如果能獲取$x_2$並創建$x_1 \times x_2$交互作用項，則可通過第二個模型分離其貢獻：
    $error^* = \beta_2x_2 + \beta_3x_1x_2 + error$

* **搜索範圍縮小的原則**：
  - **層級原則**：首先尋找成對交互作用幫助解釋殘差變異
  - **稀疏性原則**：若存在活躍交互作用，只有相對較少的交互作用重要
  - **遺傳原則**：在第一階段識別的預測變數中搜索交互作用
  - 此階段選擇的遺傳類型（弱或強）將決定評估的交互作用項數量

* **Ames數據案例**：
  - 第一階段：僅使用基本預測變數集（僅主效應）的glmnet模型選擇了12個預測變數
  - 這12個預測變數之間有66個成對組合
  - 創建適當的虛擬變數後，第二階段glmnet模型包含719個預測列作為輸入，其中660個為交互作用
  - 遵循相同調優過程，結果模式與圖7.8(a)所示非常相似
  - 最佳模型：純lasso，最佳懲罰估計為0.002，RMSE為0.081對數單位
  - 從整個預測變數集中，此glmnet模型選擇了7個主效應和128個交互作用項

* **分類結果的誤差計算**：
  - 連續結果與分類結果的誤差計算方式不同
  - 對於分類結果（如中風數據），必須使用**皮爾森殘差**（Pearson Residual）：
    $$\frac{y_i - p_i}{\sqrt{p_i(1-p_i)}}$$
  - 其中，$y_i$是第$i$個樣本的響應類別的二元指標，$p_i$是第$i$個樣本的預測概率

### 7.4.2 基於樹的方法 (Tree-based Methods)

* **基於樹的方法優勢**：
  - 迄今討論的方法對中等數量預測變數的數據有效
  - 但隨著預測變數數量增加，這些方法失去實際效果
  - 基於樹的方法如**遞歸分割**（Recursive Partitioning）或其集成模型可高效建模大量預測變數的數據
  - 已被證明能發現預測變數之間的潛在交互作用（Breiman等，1984）
  - 使用基於樹的方法識別重要交互作用已成為理論和實證研究的熱門領域

* **遞歸分割基本概念**：
  - 目標：找到預測變數的最佳分割點，使樣本相對於響應分為更均質的組
  - 分割後，相同程序應用於每個後續節點的樣本子集
  - 過程遞歸繼續，直到達到節點中的最小樣本數或滿足優化標準
  - 當數據遞歸分割時，每個後續節點可視為表示前一節點與當前節點之間的局部交互作用
  - 同一對預測變數的後續節點出現頻率越高，預測變數間交互作用在預測變數觀測範圍內全局性的可能性越大

* **協同關係樹模型示例**（圖7.9）：
  - 顯示了兩個預測變數的協同關係的最佳樹
  - 樹模型通常被視為純交互作用，其預測方程可寫為一組乘法陳述
  - 例如，通往終端節點4的路徑經過三次分割，可寫為單個規則：
    ```
    node_4 <- I(x1 < 0.655) * I(x2 < 0.5) * I(x2 < 0.316) * 0.939
    ```
    其中0.939是落入此節點的訓練集數據平均值（I是基於函數內邏輯為零或一的函數）

* **樹模型的複雜性原因**（圖7.10）：
  - 協同關係導致響應的曲線等值線（見圖7.2）
  - 由於遞歸分割模型將空間分解為矩形區域，模型需要更多區域才能充分解釋響應
  - 從本質上講，樹的分割方面阻礙了它們表示平滑、全局交互作用的能力
  - 圖7.10顯示每個終端節點的預測由矩形區域表示，顏色表示區域的預測響應值
  - 樹模型通過「像素化」方式近似協同響應，需要多個區域解釋關係

* **樹模型的局限性**：
  - 樹將預測變數與響應之間的關係像素化
  - 樹的粗糙預測性質通常使此模型不如其他模型準確
  - 樹有一些潛在的已知缺陷，如單個樹可能高度可變（數據小變化可能導致模型大幅變化）

* **樹集成方法**：
  - **裝袋**（Bagging）(Breiman 2001)和**提升**（Boosting）(Friedman 2001)已被證明可緩解問題並改進預測性
  - **裝袋**：
    * 獨立生成原始數據的有放回樣本
    * 為每個樣本建立樹
    * 預測新樣本時，模型計算所有樹的預測響應平均值
  - **提升**：
    * 也使用樹序列，但創建方式根本不同
    * 限制樹深度而非建立最大深度的樹
    * 使用先前樹在樣本上的預測性能重新加權預測不佳的樣本
    * 每棵樹的貢獻使用與單個模型擬合相關的統計數據加權
    * 新樣本的預測是整個樹集的加權組合

* **研究證明**：
  - 這些集成方法能識別預測變數之間的重要交互作用
  - García-Magariños等人(2009)和Qi (2012)：**隨機森林**（Random Forest，裝袋的進一步修改）可用於查找生物數據中的交互作用
  - Basu等人(2018)：使用隨機森林算法變體識別高階交互作用
  - Lampa等人(2014)和Elith、Leathwick和Hastie (2008)：提升方法也可用於發現重要交互作用
  - 理論角度：Friedman和Popescu (2008)提出評估樹模型中交互作用重要性的新統計量

* **樹集成有效性原因**（圖7.11）：
  - 匯總來自原始數據略微不同版本的多棵樹
  - 聚合允許更多可能的響應值，更好地近似預測變數-響應關係
  - 圖7.11顯示樹集成更能近似協同關係
  - 交叉驗證均方根誤差(RMSE)：
    * 提升：0.27
    * 裝袋樹：0.6
    * 最佳單樹模型：0.9
    * 線性迴歸交互作用模型：0.05（更平滑的模型效果最佳）

* **特徵工程的主要前提**：
  - 如果能識別有意義的預測變數關係並放置在更有效的形式中，則可使用更簡單且更合適的模型估計預測變數與響應之間的關係
  - 樹方法在特定情況下可能有更好的預測能力：
    * 當重要交互作用發生在一個或多個樣本子集（局部）內時
    * 樹模型具有正確架構來建模交互作用
    * 在更簡單的模型中創建局部交互作用的特徵更繁瑣
    * 需要創建局部空間標識符（虛擬預測變數）與該空間內交互的預測變數的乘積項
    * 本質上是三項之間的交互作用

* **從樹集成中提取交互作用信息的方法**：
  1. **基於權重的隨機森林**（Basu等人，2018）：
     - 使用隨機森林的形式，根據特徵重要性隨機選擇特徵
     - 創建集成後，計算每個共現特徵集的指標
     - 用於識別頂級交互特徵

  2. **H統計量方法**（Friedman和Popescu，2008）：
     - 基於部分依賴理論概念
     - 比較兩個（或更多）預測變數的聯合效應與模型中每個預測變數的個別效應
     - 如果個別預測變數與其他預測變數不交互，聯合效應與個別效應之間的差異接近零
     - 如果存在交互作用，差異大於零
     - 這種比較在數值上總結為H統計量
     - 圖7.12顯示Ames數據的H統計量估計
     - 使用閾值H≥0.001選擇變數，表明6個預測變數至少參與一個交互作用

  3. **基於預測變數重要性的方法**：
     - 使用預測變數重要性信息及層級、稀疏性和遺傳原則
     - 如果預測變數間的交互作用重要，則樹方法將在多棵樹中多次使用這些預測變數
     - 這些預測變數將具有非零或高重要性值
     - 可創建最重要預測變數之間的成對交互作用並評估其預測響應的相關性
     - 類似於兩階段建模方法，但基學習器為樹模型
     - 圖7.13顯示應用於Ames數據的隨機森林模型的前10個變數重要性分數
     - 最重要的是居住面積和建造年份，這兩者組合成科學上有意義的交互作用

* **隨機森林重要性計算**：
  - 使用引導樣本創建集成中的多個模型
  - 每棵樹有相關的評估集（稱為袋外(OOB)樣本）
  - 可用於了解模型質量
  - 對於特定預測變數，將其值打亂，重新計算OOB估計
  - 如果預測變數不重要，原始OOB統計與預測變數變得不相關的統計差異不大
  - 如果預測變數至關重要，置換OOB統計應顯著更差
  - 隨機森林為模型中的每個預測變數進行此分析，創建每個預測變數的單一分數

### 7.4.3 可行解算法 (The Feasible Solution Algorithm)

* **識別重要交互作用問題**：
  - 可視為識別包含主效應和交互作用的最佳模型
  - 即使對於中等數量的預測變數，完全列舉和評估所有可能的模型在實際上是不可能的
  - 模型數量為$p!$

* **傳統模型選擇方法**：
  - 使用線性或邏輯迴歸等線性模型時，通常使用前向、後向和逐步選擇定位最佳預測變數集
  - **前向選擇**：
    * 從無預測變數開始
    * 識別與響應最優相關的預測變數
    * 下一步選擇與第一個預測變數組合最優相關的下一個預測變數
    * 繼續，直到剩餘預測變數中沒有能改進模型的
  - **後向選擇**：
    * 從所有預測變數開始
    * 順序移除對優化標準貢獻最小的預測變數
    * 繼續，直到移除任何預測變數會顯著降低模型性能
  - **逐步程序**：
    * 每步添加或移除一個預測變數
    * 基於優化標準
    * 直到添加或移除任何預測變數不能顯著改進模型

* **Miller替代搜索技術**（Miller 1984）：
  - 不是移除預測變數，而是建議替換方法
  - 每個選定的預測變數系統地被選擇進行替換
  - 例如：10個預測變數中找出最佳的3個預測變數模型
    * 隨機選擇三個，計算模型性能
    * 固定前兩個預測變數，用其他七個預測變數計算性能
    * 如果沒有其他預測變數比第三個更好，則保留它；否則替換
    * 然後固定第一和第三個預測變數，為第二個預測變數重複過程
    * 依此類推，直到收斂到最優解
  - 由於算法不保證找到全局最優解，應選擇不同的隨機起始預測變數集
  - 為每個不同的起始集確定收斂解和每個解的頻率
  - Hawkins (1994)將此算法擴展到穩健建模方法，稱為**可行解算法**（Feasible Solution Algorithm, FSA）

* **FSA算法優勢**：
  - 搜索空間大小為$q \times m \times p$
    * $q$：隨機起始數
    * $m$：子集中的項數
    * $p$：預測變數數量
  - 遠小於整個搜索空間($p^m$)
  - FSA算法已被證明收斂到最優解，雖然不保證是全局最優

* **FSA算法擴展**（Lambert等人，2018）：
  - 原始FSA專注於找到主效應的最優子集，不考慮搜索預測變數間的交互作用
  - Lambert提供了算法的推廣，搜索交互作用
  - 方法：
    * 從基本模型開始（通常包括被認為對預測響應重要的預測變數）
    * 選擇所需的交互作用順序（如二因素、三因素等）
    * 識別潛在相關交互作用的過程遵循與原始FSA算法相同的邏輯
    * 隨機選擇兩個預測變數，計算包含基本項和此交互作用項的模型性能
    * 固定第一個預測變數，系統地用其餘預測變數替換第二個預測變數，計算相應的模型性能
    * 如果沒有其他預測變數提供更好的預測性能，則保留第二個；否則替換
    * 然後固定第二個預測變數，用其他預測變數交換第一個預測變數
    * 繼續迭代直至收斂
    * 選擇不同的隨機預測變數對並重複該過程
    * 跨隨機起始計算收斂交互作用，以識別潛在的可行解
    * 算法可輕易推廣到任何階次的交互作用

* **Ames數據應用FSA算法**：
  - 不同於先前分析，使用單獨的虛擬變數來對比結果類型
  - 選擇約束：來自同一預測變數的虛擬變數不會配對
  - 使用50個隨機起始，每次隨機起始中有60次潛在預測變數交換
  - 使用重抽樣確定每個候選交互作用項集是否與較小的RMSE值相關
  - 表7.2顯示前15個交互作用（按p值索引）的結果
  - 許多相同變數出現在這些列表中（如居住面積、鄰里等）以及一些非線性項（如樣條曲線的地理編碼）
  - 下一步將在更多探索工作後將這些項的選擇添加到基本模型中

* **交互作用可視化示例**（圖7.14）：
  - 建築類型和居住面積之間的潛在交互作用
  - 散點圖按每個建築類型分開，疊加了線性迴歸線
  - 顯示几個建築類型有非常不同的斜率
  - 最大對比是鎮屋端單元（TwnhsE）和兩戶家庭改建（TwoFmCon，最初建為單戶住宅）之間
  - 可能有意義編碼所有五個交互作用項，或將一些具有相似斜率的項分組以簡化編碼


## 7.5 其他潛在有用工具 (Other Potentially Useful Tools)

* 還有一些值得提及的能夠發現預測性交互作用的模型

## 多變量自適應迴歸樣條 (Multivariate Adaptive Regression Splines)

* **MARS模型**（Multivariate Adaptive Regression Splines）是一種用於連續響應的非線性建模技術（Friedman 1991）
* 工作原理：
  - 搜索個別預測變數及每個預測變數的個別值
  - 尋找用於創建鉸鏈函數的預測變數和樣本值
  - 鉸鏈函數描述預測變數與響應之間的關係
  - 鉸鏈函數在第6.2.1節中描述過，使模型能夠搜索非線性關係

* **MARS的關鍵功能**：
  - 不僅能搜索個別預測變數
  - 還能搜索預測變數的乘積，創建隔離預測變數空間部分的非線性交互作用
  - 具有內置特徵選擇功能
  - **主要缺點**：計算成本高

* **MARS的擴展**：
  - 已擴展到分類結果
  - 擴展版本稱為**彈性判別分析**（Flexible Discriminant Analysis, FDA）

## Cubist規則基模型 (Cubist Rule-Based Model)

* **Cubist**是一種規則基迴歸模型（Kuhn和Johnson 2013）：
  - 構建初始樹，然後將其分解為一組規則
  - 這些規則經過修剪，甚至可能被消除
  - 對每條規則定義單獨的線性模型

* **規則示例**（Ames住房數據）：
  ```
  if
      Year_Built <= 1952
      Central_Air = No
      Longitude <= -93.6255
      
  then
      log10 Sale Price = 261.38176 + 0.355 Gr_Liv_Area + 
                                     2.88  Longitude + 
                                     0.26  Latitude
  ```

* **Cubist模型的特點**：
  - 創建一組不相交的交互作用
  - 規則集可能不覆蓋用於規則的預測變數的每種值組合
  - 被預測的新數據點可能屬於多條規則
  - 在這種情況下，相關線性模型預測取平均值
  - 這種模型結構非常靈活，很有可能找到整個預測變數空間內的局部交互作用
  
* **實際案例**：
  - 當房產總居住面積小但臥室數量大時，迴歸模型中與臥室數量相關的斜率通常為負
  - 這在一般情況下不一定正確，但當僅應用於幾乎「全是臥室」的房產時可能成立

## 7.6 總結 (Summary)

* 構建預測模型時，預測變數之間的交互作用通常被忽視
* 這可能有兩個原因：
  - 現代建模技術能夠隱蔽地識別交互作用
  - 交互作用可能由於潛在的大量額外項將添加到模型中而被忽視
  - 特別是當分析師不知道哪些交互作用項可能有益於解釋結果時

### 尋找交互作用的方法 (Approaches for Finding Interactions)

* **專家知識**：
  - 開始搜索交互作用時，系統的專家知識始終最有益
  - 可以幫助縮小搜索範圍

* **算法方法**：
  - 對於具有相對較少預測變數的數據，可以完全列舉每個成對交互作用
  - 然後使用**重抽樣方法**（Resampling Approaches）或**懲罰模型**（Penalized Models）定位可能有用改進模型的交互作用

* **大型數據集策略**：
  - 隨著預測變數數量增加，完全列舉在實際上變得不可行
  - 應使用能夠在不搜索整個空間的情況下迅速識別潛在重要交互作用的方法：
    * **兩階段建模**（Two-stage Modeling）
    * **基於樹的方法**（Tree-based Methods）
    * **可行解算法**（Feasible Solution Algorithm）

### 實施方法 (Implementation Approach)

* 搜索完成後，最有可能改進模型性能的交互作用項可添加到更簡單的模型中：
  - 線性迴歸（Linear Regression）
  - 邏輯迴歸（Logistic Regression）等

* 通過**交叉驗證**（Cross-validation）估計預測性能
* 與不含這些項的模型比較，確定整體預測改進程度

# 8 處理缺失數據 (Handling Missing Data)

* 真實數據集中缺失數據並不罕見
* 隨著數據集大小增加，至少有一個數據點缺失的可能性也增加

## 缺失數據的常見原因 (Common Causes of Missing Data)

* **源數據集合併**：
  - 當通過樣本標識符(ID)合併兩個數據集時
  - 如果一個ID只出現在第一個數據集中，合併後的數據將對第二個數據集的所有預測變數包含缺失值

* **隨機事件**：
  - 任何測量過程都容易受到隨機事件影響，從而阻止數據收集
  - 例如：醫學診斷實驗室中的生物樣本（如血液或血清）被意外錯放或損壞
  - 活動記錄設備也可能受隨機事件影響（如電池耗盡或設備損壞）

* **測量失敗**：
  - 基於圖像的測量需要圖像對焦清晰
  - 未對焦或損壞的圖像可能導致缺失值
  - 臨床研究中患者錯過預定醫師訪問也會導致測量數據缺失

## 特徵工程的目標與缺失數據的影響 (Feature Engineering Goals and Impact of Missing Data)

* 特徵工程的目標：將預測變數轉換為模型能更好利用的形式
* 缺失數據的問題：
  - 許多特徵工程技術要求數據沒有缺失值：
    * 投影方法（第6.3節）
    * 自編碼器轉換（第6.3.2節）
    * 分類預測變數的似然編碼（第5.4節）
  - 許多預測模型無法容忍原始預測變數中的缺失值
  - 因此，必須首先解決數據中的缺失問題
  - 缺失本身可能是響應的重要預測因子

* **響應變數中的缺失值**：
  - 大多數建模技術無法使用訓練數據中響應值缺失的樣本
  - **半監督學習**（Semi-supervised Learning）方法可利用訓練集中響應值未知的樣本
  - 本章將專注於解決預測變數中缺失值的方法

## 缺失數據的類型 (Types of Missing Data)

* 面對缺失數據時的首要問題：「為什麼這些值缺失？」
* 有時答案可能已知或通過研究數據易於推斷
* 如果數據來自科學實驗或臨床研究，實驗室筆記或臨床研究日誌可能提供直接線索
* 但對許多其他數據集，可能無法確定缺失數據的原因

### 缺失數據的機制框架 (Framework for Missing Data Mechanisms)

1. **數據結構缺陷**（Structural Deficiencies）：
   - 定義：預測變數中省略的組件
   - 這種缺失通常在識別必要組件後最容易解決
   - Ames住房數據中的「Alley」預測變數例子：
     * 取值為「gravel」、「paved」或缺失
     * 93.2%的房屋此預測變數值缺失
     * 簡單移除此預測變數會丟失有價值的預測信息
     * 缺失在這裡意味著房產沒有小巷
     * 更好的處理方式：將缺失值替換為「No Alley Access」

2. **隨機事件**（Random Occurrences）：
   - Little和Rubin (2014)將這種隨機性細分為兩類：
     
     a. **完全隨機缺失**（Missing Completely at Random, MCAR）：
     - 所有數據點（已觀察或未觀察）缺失結果的可能性相等
     - 換言之，缺失值獨立於數據
     - 這是最理想的情況
     
     b. **隨機缺失**（Missing at Random, MAR）：
     - 所有數據點（已觀察或未觀察）缺失結果的可能性不相等
     - 缺失結果的概率取決於已觀察數據，但不取決於未觀察數據
   
   - 實務中很難或不可能區分這兩種情況
   - 本文描述的方法適用於兩種情況

3. **特定原因缺失**（Missingness due to Specific Cause）：
   - 也稱為**非隨機缺失**（Not Missing at Random, NMAR）(Little和Rubin 2014)
   - 臨床研究中常見，患者隨時間定期測量
   - 例如：患者可能因治療的不良副作用或死亡而退出研究
   - 這種患者在退出時間後不會記錄測量數據
   - 非隨機缺失的數據最具挑戰性
   - 本章介紹的技術可能適用或不適用於這種缺失
   - 因此，在實施任何技術前必須努力了解缺失數據的性質

## 本章內容概述 (Chapter Overview)

* 介紹評估數據中缺失值性質和嚴重程度的方法
* 突出當存在缺失值時可使用的模型
* 回顧移除或插補缺失數據的技術

## 案例數據集 (Example Datasets)

1. **芝加哥列車乘客數據**（第4章）

2. **動物糞便數據**（Reid 2015，之前在第6.3.3節中見過）：
   - 包含在野外收集的動物糞便信息
   - 對每個樣本進行了多種測量：
     * 形態學觀察（形狀）
     * 位置/時間信息
     * 實驗室測試
   - 通過DNA基因型確定樣本的物種（灰狐、郊狼或山貓）
   - 目標：建立糞便測量與物種之間的預測關係
   - 收集新的糞便樣本後，使用模型預測產生樣本的物種
   - 110個收集的糞便樣本中，19個有一個或多個預測變數值缺失

## 8.1 了解缺失信息的性質和嚴重程度 (Understanding the Nature and Severity of Missing Information)

* 本書通篇所示，視覺化數據是指導我們實施適當特徵工程技術的重要工具
* 同樣的原則適用於理解數據中缺失信息的性質和嚴重程度
* 視覺化和數值摘要是掌握數據集中缺失信息挑戰的第一步

### 缺失信息的視覺化 (Visualizing Missing Information)

#### 適用於中小型數據集的視覺化技術 (Visualization for Small to Moderate Datasets)

* 當訓練集有中等數量的樣本和預測變數時，視覺化缺失信息的簡單方法是**熱圖**（Heatmap）
  - 使用兩種顏色表示存在和缺失的值
  - 圖8.1上部展示了動物糞便數據的缺失信息熱圖
  - 可以使用**層次聚類分析**（Hierarchical Cluster Analysis）重組以突出預測變數和樣本間的共同性
  - 顯示大多數預測變數和樣本信息完整或幾乎完整
  - 三個形態學預測變數（直徑、錐度和錐度指數）更頻繁缺失
  - 兩個樣本缺失所有三個實驗室測量值（d13N、d15N和CN）

* **共現圖**（Co-occurrence Plot）可進一步加深對缺失信息的理解
  - 顯示缺失預測變數組合的頻率（圖8.1下部）
  - 易於看出TI、Tapper和Diameter有最多缺失值
  - Tapper和TI最常一起缺失
  - 六個樣本缺失所有三個形態學預測變數

#### 探索與缺失相關的關係 (Exploring Relationships Related to Missingness)

* 探索與缺失可能相關的數據關係很重要
* 當探索預測變數之間的成對關係時，可在另一軸的邊緣標記一個預測變數的缺失值
* 例如：圖8.2顯示糞便直徑和質量之間的關係
  - 任一軸上的刻度標記表示該預測變數有觀測值但相應預測變數有缺失值的樣本
  - 點按樣本是否為「缺乏其他形態特徵的扁平水坑」著色
  - 由於缺失直徑測量的樣本具有這種不適宜的質量，無法確定某些形狀屬性是合理的
  - 可視為結構性缺失，但仍可能有隨機或信息性缺失成分
  - 然而，在結果方面，六個扁平樣本分佈在灰狐(n=5)和郊狼(n=1)數據中
  - 鑑於小樣本量，不清楚這種缺失是否與結果相關（這將有問題）
  - 需諮詢主題專家確定是否如此

#### 大型數據集的視覺化方法 (Visualization Methods for Large Datasets)

* 當數據有大量樣本或預測變數時，熱圖或共現圖對理解缺失模式和特徵效果較差
* 在此情況下，必須先壓縮缺失信息再進行視覺化
* **主成分分析**（Principal Component Analysis, PCA）
  - 第6.3節首次描述為降維技術
  - 也可用於視覺化和理解具有問題性缺失水平的樣本和預測變數
  - 使用方法：將預測變數矩陣轉換為二元矩陣，0表示非缺失值，1表示缺失值
  - 由於PCA搜索總結最大變異性的方向，初始維度捕捉由缺失值存在引起的變異
  - 沒有任何缺失值的樣本將被投影到接近原點的相同位置
  - 有缺失值的樣本將投影到遠離原點的位置
  - 因此，通過簡單繪製前兩個維度的得分，可以開始識別大型數據集中缺失的性質和程度

* **應用於預測變數缺失度理解**：
  - 將代表缺失值的二元矩陣轉置，使預測變數現在位於行中，樣本位於列中
  - 對此矩陣應用PCA，結果維度現在捕捉由預測變數間缺失值存在引起的變異

#### 芝加哥列車乘客數據案例 (Chicago Train Ridership Data Example)

* 芝加哥列車乘客數據包含5,733天和137個車站的乘客數據
* 缺失數據矩陣可由日期和車站表示
* 圖8.3顯示對此矩陣應用PCA生成的前兩個成分
  - 圖中點按照缺失車站數量大小調整
  - 每天至少有一個車站有缺失值
  - 共有8種不同的缺失數據模式
  - 進一步探索發現許多缺失值發生在2013年9月
  - 這些缺失值的原因需進一步調查

* 圖8.4呈現車站的得分
  - 點按車站間缺失信息量標記
  - 在此情況下，2個車站有大量缺失數據
  - 這些車站缺失值的根本原因應進行調查，以更好地確定處理方式

* 圖8.5顯示具有過多缺失和隨時間缺失數據模式的車站
  - 車站順序使用聚類算法設置，x軸按日期排序
  - 九個車站的數據幾乎完整，只有一個月的間隔
  - 這些車站都在紅線上，發生在紅線重建項目期間，該項目影響了Cermak-Chinatown北部至95th Street車站的車站
  - Conservatory車站於2001年開通，Morgan車站於2012年中期開通，這解釋了這些日期之前的缺失數據

### 缺失信息的數值摘要 (Summarizing Missing Information)

* 簡單數值摘要可有效識別數據太大無法視覺檢查時的問題性預測變數和樣本
* 首先，可以容易計算預測變數和樣本的缺失值總數或百分比
* 回到動物糞便數據：
  - 表8.1按缺失量列出預測變數
  - 表8.2按缺失量列出樣本
  - 這些摘要可用於調查值缺失的潛在原因
  - 或作為移除具有過多缺失值的預測變數或樣本的篩選機制

* **動物糞便數據缺失摘要**：
  - Mass：0.909%缺失
  - d13C、d15N和CN：1.818%缺失
  - Diameter：5.455%缺失
  - Taper和TI：15.455%缺失
  - 多個樣本有10.5%或15.8%的缺失值

## 8.2 抗缺失值的模型 (Models that are Resistant to Missing Values)

* 許多流行的預測模型無法容忍任何缺失值：
  - **支持向量機**（Support Vector Machines）
  - **glmnet**
  - **神經網絡**（Neural Networks）

* 然而，有一些預測模型可以內部處理不完整數據[^1]

### 樹基模型的缺失值處理 (Tree-Based Models for Missing Values)

#### CART方法 (CART Methodology)

* **CART方法**（Breiman等，1984）使用**替代分割**（Surrogate Splits）的概念：
  - 在創建樹時，會編目一組單獨的分割（使用當前被分割的預測變數以外的替代預測變數）
  - 如果該預測變數值缺失，這些替代分割可以近似原始分割邏輯
  
* **動物糞便數據案例**（圖8.6）：
  - 樹選擇的三個預測變數都包含缺失值（如圖8.1所示）
  - 初始分割基於碳/氮比率（CN < 8.7）
  - 當樣本的CN值缺失時，CART模型使用基於糞便是否扁平的替代分割
  - 這兩種分割對80.6%的數據產生相同分區，可在碳/氮比率缺失時使用
  - 在樹的更深處，d13C和質量（Mass）的替代預測變數分別是質量和d13C
  - 這是可能的，因為這些預測變數不會同時缺失

#### C5.0方法 (C5.0 Method)

* **C5.0**（Quinlan 1993; Kuhn和Johnson 2013）採用不同方法：
  - 基於具有缺失數據的預測變數分布，在後續分割中使用**分數計數**（Fractional Counts）
  - 例如：該模型對糞便數據的第一次分割是d13C > 24.55
  - 當此陳述為真時，所有13個訓練集樣本都是郊狼
  - 然而，該預測變數有一個缺失值
  - 由於調整分割變數的缺失值數量，後續節點中每個物種的計數是分數
  - 這使模型能夠持續記錄缺失值可能在分區中落在何處

### 朴素貝葉斯方法 (Naive Bayes Method)

* **朴素貝葉斯**是另一種可以容忍缺失數據的方法：
  - 分別建模每個預測變數的特定於類別的分布
  - 如果缺失數據機制非病理性，這些分布摘要可以使用每個單獨預測變數的完整觀測
  - 避免逐案刪除

## 8.3 數據刪除 (Deletion of Data)

* 當希望使用不能容忍缺失數據的模型時，必須從數據中清除缺失值
* 處理缺失值的最簡單方法是移除包含缺失值的整個預測變數和/或樣本
* 然而，在採取這種方法之前，必須仔細考慮數據的多個方面

### 刪除方法的考量 (Considerations for Deletion)

* 消除缺失值的兩種基本方法：
  - 移除至少包含一個缺失值的所有預測變數
  - 移除任何有缺失值的所有樣本
  
* 根據**沒有免費午餐定理**（No Free Lunch Theorem），這些方法不適用於所有數據：
  - 某些數據集中，特定預測變數可能比其他更有問題；移除這些預測變數可解決缺失數據問題
  - 其他數據集中，特定樣本可能在預測變數間有一致的缺失值；移除這些樣本同樣可解決問題
  - 實務中，特定預測變數和特定樣本通常包含大部分缺失信息

### 樣本與預測變數的價值比較 (Value of Samples vs. Predictors)

* 與預測變數相比，樣本的內在價值是重要考量
* 當樣本難以獲取或數據包含少量樣本時，不希望從數據中移除樣本
* 一般而言，**樣本比預測變數更關鍵**，應優先保留盡可能多的樣本
* 鑑於樣本通常更優先，初始策略應為：
  1. 首先識別並移除具有足夠高缺失數據量的預測變數
     - 例外：已知有價值和/或能預測結果的預測變數不應移除
  2. 移除有問題的預測變數後，關注超過缺失閾值的樣本

### 芝加哥列車乘客數據示例 (Chicago Train Ridership Data Example)

* 本章前面對這些數據的調查揭示：
  - 幾個車站有大量連續缺失數據（圖8.5）
  - 每個日期（樣本）至少有一個缺失車站（預測變數）
  - 預測變數間的缺失程度最小
  - 少數車站缺失過多，連續缺失數據太多而無法保留在分析集中
  - 這種情況更有意義的做法是移除這少數車站
  - 對紅線車站是否保留在分析集中的決定不太明確
  - 鑑於車站間的高度相關性，決定從本文所有分析中排除所有有缺失數據的車站不會對建立有效的Clark-Lake車站乘客量預測模型產生不利影響
  - 這個例子說明在確定處理缺失值的方法時需要考慮幾個重要方面

### 刪除數據的主要問題 (Main Concerns with Data Deletion)

* 除了丟棄數據外，移除訓練集樣本（行）的主要問題是可能使關聯預測變數與結果的模型產生偏差
* 經典例子來自醫學研究：
  - 一部分患者隨機分配到當前標準護理治療，另一部分分配到新治療
  - 新治療可能對某些患者產生不良影響，導致他們退出研究，從而導致未來臨床訪問的數據缺失
  - 這種缺失數據明顯不是隨機缺失
  - 從分析中消除這些數據會錯誤地測量結果，使其看起來比包含不良結果時更好
* Allison (2001)指出：如果數據是完全隨機缺失，這可能是一種可行的方法

## 8.4 編碼缺失性 (Encoding Missingness)

* 當預測變數本質上是離散的，可以將缺失直接編碼到預測變數中，視為一個自然出現的類別
* 這對結構性缺失值特別有意義

### 結構性缺失值的編碼 (Encoding Structural Missing Values)

* **Ames住房數據中的小巷例子**：
  - 合理的做法是將缺失值更改為「無小巷」（"no alley"）類別
  
* **其他情況**：
  - 缺失值可以簡單地編碼為「缺失」（"missing"）或「未知」（"unknown"）
  - 這使包含缺失值的樣本仍可用於建模過程

### 補助金申請數據案例研究 (Grant Proposal Dataset Case Study)

* Kuhn和Johnson (2013)使用的數據集，目標是預測補助金提案的接受或拒絕
* **補助金贊助商預測變數**：
  - 取值包括「澳大利亞競爭性補助金」、「合作研究中心」、「工業」等
  - 共有超過245個可能值
  - 約10%的補助金申請沒有贊助商值
  - 為了在建模中使用這些申請，將空贊助商值編碼為「未知」

* **分析結果**：
  - 在研究的許多模型中，未知贊助商的指標是補助金成功的最重要預測因子之一
  - 對比已知與未知贊助商的風險比大於6
  - 這意味著如果贊助商預測變數未知，補助金獲得成功資助的可能性要高得多
  - 在訓練集中，與未知贊助商相關的補助金成功率為82.2%，而已知贊助商為42.1%

### 編碼缺失性的評估 (Evaluating the Encoding of Missingness)

* **關鍵問題**：編碼缺失值是否成功？
  - 導致缺失贊助商標籤被識別為與補助金接受高度相關的機制確實重要
  - 不幸的是，不可能知道為什麼這種關聯如此重要
  - 重要的是這裡發生了某些事情，而這種編碼幫助識別了其發生

* **局限性**：
  - 將這種分析作為最終結果並暗示某種因果關係可能會有問題[^1]
  - 如果該信息對模型變得重要，需要考慮結果將如何解釋

### 指導原則 (Guiding Principle)

* 確定編碼缺失性是否合適的指導原則：
  - 思考如果該信息對模型變得重要，結果將如何被解釋
  - 需要考慮對模型消費者解釋「我們知道這很重要，但不知道為什麼！」的難度

## 8.5 插補方法 (Imputation methods)

### 插補概述
* **插補** (Imputation) 是另一種處理缺失值的方法，使用非缺失數據中的關係來估計缺失值
* 插補在統計學中有悠久歷史，參考文獻包括：
  - Little 和 Rubin (2014)
  - Van Buuren (2012)
  - Allison (2001)
* 插補本質上使用非缺失預測變數間的關係來提供缺失值的估計

### 推論模型與預測模型的差異
* **推論模型** (Inferential models) 關注缺失值對統計檢驗的影響
  - 主要使用**多重插補** (Multiple imputations)：創建多個數據集變體，每個有不同的缺失值估計
  - 從這些變體中計算多組檢驗統計量，構建假設檢驗

* **預測模型** (Predictive models) 與推論模型在處理缺失值上的主要差異：
  1. 許多預測模型沒有分佈假設或難以處理分佈假設
  2. 預測模型通常計算成本高，多次插補會大幅增加計算時間
  3. 預測模型關注的是準確預測未見樣本的能力，而非統計適當性
  4. 推論模型專注理解現有數據中的關係，預測模型則關注可泛化到新樣本的關係

## 預測性插補方法的關鍵特性
* 主要目標：產生對缺失數據點**最準確的預測**
* 理想的預測性插補方法應具備：
  - 能夠容忍樣本中的其他缺失數據
  - 計算速度快，預測方程簡潔
  - 能同時處理數值和分類預測變數
  - 相對穩定且不受極端值過度影響

### 插補量的考量
* 一般而言，當一個列中有超過20%的數據缺失時，應考慮是否適合插補
* 這個"尊嚴線"取決於具體情況和訓練集中缺失值的模式

## 插補在預處理序列中的位置
* 插補通常是預處理序列中的第一步
* 建議在創建指標變數前先進行分類預測變數的插補，以保留二元性
* 插補應在涉及參數估計的其他步驟之前進行
  - 例如：如果在插補前進行中心化和縮放，則所得平均值和標準差會繼承數據刪除帶來的潛在偏差

### K-最近鄰插補 (K-Nearest Neighbors)
* 當訓練集規模較小或中等時，**K-最近鄰** (K-Nearest Neighbors) 是快速有效的插補方法
  - 參考：Eskelson 等 (2009); Tutz 和 Ramzan (2015)

* 工作原理：
  1. 識別具有缺失值的樣本
  2. 找出訓練數據中最相似的 K 個完整樣本（無缺失值）
  3. 使用這 K 個樣本的平均值替換缺失值

* 相似度計算：
  - 全數值預測變數：通常使用**歐氏距離** (Euclidean distance)
  - 混合數值與分類預測變數：使用**高維距離** (Gower's distance)
    + 分類預測變數：相同值距離為1，否則為0
    + 數值預測變數距離計算公式：
      $$d(x_i, x_j) = 1 - \frac{|x_i - x_j|}{R_x}$$
      其中 $R_x$ 是預測變數的範圍

* 插補方式：
  - 分類預測變數：使用眾數
  - 數值預測變數：使用平均值或中位數
  - K 可作為可調參數，但 5-10 通常是合理的默認值

### 樹基插補方法 (Tree-based Methods)
* **樹基模型** (Tree-based models) 是合理的插補技術選擇，因為：
  - 可在存在其他缺失數據的情況下構建
  - 通常具有良好的準確性
  - 不會推斷超出訓練數據範圍的值

* **隨機森林** (Random forests) 可用於插補 (Stekhoven 和 Buhlmann, 2011)
  - 缺點：需要大量樹 (500-2000) 才能達到穩定可靠的模型
  - 每棵樹都是未修剪的，計算量大

* **裝袋樹** (Bagged tree) 是計算負擔較小的替代方案
  - 與隨機森林的主要區別：在裝袋模型中，每次分割都評估所有預測變數
  - 使用 25-50 棵樹的裝袋樹性能通常與隨機森林相近
  - 較少的樹數量是尋找合理插補值的明顯優勢

### 線性方法插補 (Linear Methods)
* 當完整預測變數與需要插補的預測變數之間存在強線性關係時，**線性模型** (Linear models) 可能是最佳方法
  - 優勢：計算速度快，保留的開銷很小
  - 對於數值預測變數：使用**線性迴歸** (Linear regression)
  - 對於分類預測變數：使用**邏輯迴歸** (Logistic regression)

* 線性模型雖然需要完整預測變數進行插補，但模型係數（斜率）使用所有數據進行估計，這不是致命缺陷

* 可以將線性插補概念擴展到高維數據
  - 例如，Audigier, Husson 和 Josse (2016) 基於相似度測量和**主成分分析** (Principal Components Analysis) 開發了插補缺失數據的方法

## 8.6 特殊情況 (Special Cases)

### 審查數據 (Censored Data)
* 有些情況下，數據點不是完全缺失，但也不是完整的
* **審查數據** (Censored data)：知道部分但不是完整的數值資訊
  - 例如：測量事件發生的時間時，可能知道持續時間至少為某個時間 T（因為事件尚未發生）
  - 各種統計方法已被開發用於分析這類數據

### 右審查與左審查 (Right and Left Censoring)
* **右審查** (Right censoring)：終止值未知
  - 最常見的審查類型
  - 例如：研究中的患者在研究結束時仍然存活，確切的存活時間未知

* **左審查** (Left censoring)：
  - 實驗室測量可能有**檢測下限** (Lower limit of detection)
  - 檢測下限通常在測量系統開發過程中實驗確定
  - 當預測變數值低於檢測下限時，通常報告為"< X"

### 處理左審查數據的方法
* **傳統做法**：使用下限值 X 作為結果
  - 缺點：對某些模型可能有不利影響，因為假設這些是真實值
  - 會低估變異性，類似於第6.2.2節中的分箱效應
  - 模型可能過度擬合於具有相同值的數據點群集

* **改進方法**：使用隨機均勻值進行插補
  - 在零和 X 之間生成隨機均勻值
  - 如果有關於 X 以下分布的良好信息，可以使用更能代表該分布的其他隨機值分配方案
  - 例如：有可能存在科學或生理原因，使最小可能值大於零（但小於 X）
  - 雖然這種方式會為數據添加隨機噪聲，但可能優於將值設為 X 可能導致的過度擬合問題

### 具有強時間成分的數據
* 當數據具有強時間成分時，需要保留數據的時間特性
* 可使用**簡單移動平均平滑器** (Simple moving average smoothers) 插補數據
  - 確保不破壞任何時間效應
* 需特別注意數據邊緣處理
  - 如第6.1節所述，必須確保不使用測試（或其他）數據來插補訓練集值

### 截斷數據 (Truncated Data)
* 當數據在下限或上限之外沒有定義時，數據被視為**截斷** (Truncated)
* 與審查數據不同，截斷數據完全缺失，而非部分觀測

## 8.7 總結 (Summary)

### 缺失值的普遍性
* 缺失值在數據中是常見現象
* 大多數預測建模技術無法處理任何缺失值，因此必須在建模前解決
* 缺失數據可能由於：
  - 隨機偶然性
  - 系統性原因
* 理解缺失值的性質有助於指導如何最佳地移除或插補數據的決策過程

### 理解缺失值的視覺化方法
* 透過適當的視覺化是理解缺失值數量和性質的最佳方法之一
* 針對較小的數據集：
  - **熱圖** (Heatmap)
  - **共現圖** (Co-occurrence plot)
* 針對較大的數據集：
  - 繪製缺失數據指標矩陣的**主成分分析** (Principal Component Analysis, PCA)模型的前兩個得分

### 缺失值處理策略選擇
* 一旦了解缺失值的嚴重程度，需要決定如何處理這些值
* 當預測變數或樣本中有嚴重缺失數據時，可能需要：
  - 移除有問題的預測變數或樣本
  - 若預測變數為定性變數，可將缺失值編碼為「缺失」類別

### 中小程度缺失的處理方法
* 對於小到中等程度的缺失，可以進行**插補** (Imputation)
* 多種插補技術中，特別適用於預測模型的方法包括：
  - **K-最近鄰** (K-nearest neighbors)
  - **裝袋樹** (Bagged trees)
  - **線性模型** (Linear models)
* 這些方法的優點：
  - 計算負擔相對較小
  - 計算速度快
  - 可以納入預測建模的重抽樣過程中

# 9 處理剖面數據 (Working with Profile Data)

## 剖面數據的基本概念

* **剖面數據** (Profile data) 是指具有多層次結構或時間序列特性的數據
* 與之前討論的數據集相比，剖面數據的預測單位不那麼容易確定：
  - 愛荷華州房屋價格
  - Clark和Lake車站的每日列車乘客量
  - 在步道上採集的糞便樣本物種
  - 患者中風的概率

* 在簡單數據結構中，**預測單位** (Unit of prediction) 很容易確定：
  - 房屋數據：每行是一個房屋，每列是描述該房屋的欄位
  - 房屋在統計上通常彼此獨立，是**獨立實驗單位** (Independent experimental unit)
  - 因為我們對房屋進行預測，所以房屋是預測單位

## 階層式數據的預測單位

* **芝加哥列車數據**的例子：
  - 數據集的行對應特定日期，列是這些日期的特徵
  - 預測是按日進行的（預測單位是日）
  - 然而，天氣測量數據是每天多次收集的（通常是每小時）
  - 同一天內的每小時測量值彼此相關性更高，因此具有**統計依賴性** (Statistically dependent)

* 當預測單位（日）與收集數據的單位（小時）不同時：
  - 需要將內部測量**概括** (Summarize)為預測單位級別
  - 可以使用均值、中位數等數值統計量
  - 對於定性條件，可以計算標明為「晴朗」、「陰天」等的一天中的百分比

## 複雜數據結構中的層次關係

* **中風數據**的例子：
  - 如果數據在患者住院時收集，並包含縱向測量值
  - 預測單位可能是「特定日期的患者」
  - 但獨立實驗單位僅僅是患者本身

* **在線教育**的例子：
  - 多層次結構：作業-課程部分-課程-學生
  - 可以在不同層次創建特徵：
    + 學生層次：人口統計學（如年齡）
    + 課程層次：先前完成的課程數量
    + 更具體的特徵：如「在STAT203中完成t檢定作業的時間是否能預測通過數據科學課程的能力？」

* 階層結構使得可以創建多種有趣的特徵，但提出了一個問題：「概括剖面數據是否有好壞之分？」
  - 答案取決於問題性質和剖面數據本身
  - 需要考慮數據中的變異性和相關性

## 高內容篩選數據案例

* **高內容篩選數據** (High content screening, HCS) 是生物科學中的另一個例子
  - 參考：Giuliano等（1997）；Bickle（2010）；Zanella、Lorens和Link（2010）

* HCS實驗通常在**微型滴定板** (Microtiter plates) 上進行：
  - 最小的板有96個孔（通常是8×12排列）
  - 每個孔添加細胞樣本和處理（如藥物候選物）
  - 處理後，使用顯微鏡對細胞進行成像，每個孔內進行多次成像
  - 數據結構是：細胞-孔-板

* 預測挑戰：
  - 要對藥物進行預測，需要將數據概括到**孔級別** (Wellular level)
  - 簡單方法：計算細胞屬性的平均值和標準差
  - 問題：這可能破壞細胞數據的相關結構

## 保持相關結構的重要性

* 細胞層次的特徵通常高度相關
  - 例如：細胞中蛋白質的豐度與細胞的總大小相關

* 計算差異的兩種方法比較：
  1. 先計算均值$\bar{X}$和$\bar{Y}$，然後計算均值之間的差異
  2. 先計算每個細胞的差異($D_i = X_i - Y_i$)，然後計算差異的平均值($\bar{D}$)

* 當$X$和$Y$高度相關時，這兩種方法的結果差異很大
  - 從概率理論，我們知道差異的方差是：
    $$Var[X-Y] = Var[X] + Var[Y] - 2Cov[X,Y]$$
  - 如果兩個細胞特徵相關，協方差可能很大
  - 如果先計算差異再平均（方法2），可以大幅減少方差
  - 先計算平均值再差異（方法1）會忽略協方差項，導致特徵更嘈雜

* Wickham和Grolemund（2016）指出：
  > "如果你認為變異是產生不確定性的現象，那麼協變異是減少不確定性的現象。"

## 醫學研究中的複雜數據結構

* 現代測量設備能在很短時間內生成大量測量值：
  - **活動監測儀** (Actigraphy monitors)
  - **磁共振成像掃描儀** (Magnetic resonance imaging, MRI)
  - **腦電圖** (Electroencephalogram, EEG)
  - **心電圖** (Electrocardiogram, ECG)

* 這些設備越來越多地被納入醫學研究中：
  - Sathyanarayana等（2016）使用活動數據預測睡眠質量
  - Chato和Latifi（2017）研究MRI圖像與腦癌患者存活之間的關係

* 除醫學領域外，其他領域也收集類似數據：
  - 設備維護領域：卡特彼勒公司收集已部署機器的連續實時數據以優化性能（Marr 2017）

## 總結

* 複雜數據集可能具有多層次的數據層次結構
* 需要對這些數據進行摺疊，使建模數據集與預測單位一致
* 概括數據有好壞之分，但策略通常是特定於主題的
* 本章其餘部分將是一個擴展案例研究，展示如何處理具有多層數據的預測單位
* 良好的預處理和特徵工程對結果的影響可能比所使用的模型類型更大

## 9.1 說明性數據：製藥製造監測 (Illustrative Data: Pharmaceutical Manufacturing Monitoring)

### 光譜測量在製藥製造中的應用
* 製藥公司使用**光譜測量** (Spectroscopy measurements) 評估生物藥物製造過程中的關鍵過程參數 (Berry 等, 2015)
* 利用這些過程建立的模型可與實時數據結合，推薦能增加產品產量的變更
* 本例中使用的是**拉曼光譜** (Raman spectroscopy) 生成數據 (Hammes 2005)
* 生物藥物製造過程：
  - 需要特定類型的蛋白質，由特定類型的細胞產生
  - 將一批細胞接種到**生物反應器** (Bioreactor) 中
  - 生物反應器是設計用於促進細胞生長和維持的裝置

### 生物反應器與藥物生產過程
* 在生產環境中：
  - 大型生物反應器容量約2000升
  - 用於在約兩週內製造大量蛋白質
  - 許多因素可影響產品產量：
    + 細胞需要適當溫度和足夠食物（葡萄糖）
    + 細胞在工作過程中會產生廢物（氨）
    + 過多廢物可能殺死細胞並降低整體產品產量
  
* 監測過程：
  - 關鍵屬性（如葡萄糖和氨）通常每日監測
  - 收集樣本並進行離線測量
  - 如測量顯示潛在問題，監督過程的製造科學家可調整生物反應器內容，優化細胞條件

### 光譜技術的優勢與挑戰
* 傳統測量葡萄糖和氨的方法耗時，結果可能來不及解決問題
* 光譜技術是潛在更快的獲取結果方法，前提是能夠建立有效模型
* 挑戰：使用多個大規模生物反應器進行實驗不可行

### 實驗設計
* 使用兩個平行實驗系統：
  1. 15個小型（5升）生物反應器：
     - 接種細胞，每日監測14天
  2. 3個大型生物反應器：
     - 同樣接種來自同批次的細胞
     - 同樣每日監測14天
  
* 數據收集：
  - 每天從所有生物反應器收集樣本
  - 使用光譜法和傳統方法測量葡萄糖

* 研究目標：
  - 使用較多小型生物反應器的數據創建模型
  - 評估這些結果是否能準確預測大型生物反應器中發生的情況

### 光譜數據的特性
* 圖9.3(a)展示了一個小型和一個大型生物反應器在幾天內的光譜：
  - 光譜在剖面中展現相似模式
  - 顯著峰值在不同日期的相似波長區域出現
  - 強度高度在小型生物反應器內部以及小型和大型生物反應器之間有所不同

* 圖9.3(b)顯示了光譜如何在兩週期間變化：
  - 大多數剖面在波數中具有相似的整體模式
  - 隨著時間推移，強度增加
  - 這種模式在其他生物反應器中也存在
  - 每個生物反應器都有獨特的額外強度變化

> 注：本例中使用的數據是基於真實數據生成的，但為保密和達到說明目的，已進行明顯修改

## 9.2 什麼是實驗單位與預測單位？ (What are the Experimental Unit and the Unit of Prediction?)

### 數據的層次結構 (Hierarchical Structure)
* 研究中的數據收集情況：
  - 每個小型生物反應器在兩週內每天測量近2600個波長
  - 共有15個小型生物反應器
  - 形成**層次結構** (Hierarchical structure)：波長數據嵌套於日期中，日期又嵌套於生物反應器中

* 嵌套數據結構的特點：
  - 每個嵌套內的測量值彼此之間比嵌套之間的測量值更相關
  - 一天內的波長彼此更相關，比不同天之間的波長相關性更強
  - 同一生物反應器不同天的波長之間的相關性比不同生物反應器之間的波長相關性更強

### 波長內相關性分析 (Within-wavelength Correlation Analysis)
* 通過**自相關** (Autocorrelation) 觀察日期和生物反應器內波長之間的複雜關係
  - 自相關是原始序列與每個順序滯後版本序列之間的相關性
  - 通常，自相關隨著滯後值增加而減少
  - 當存在季節性趨勢時，可能在後期滯後增加

* 圖9.4(a)顯示第一個生物反應器在幾天內的波長自相關：
  - 不同日期的波長之間的相關性不同
  - 後期天數往往具有更高的波長間相關性
  - 在所有情況下，需要數百個滯後才能使相關性降至零以下

### 日期間相關性分析 (Between-day Correlation Analysis)
* 層次結構的下一級是生物反應器內跨日期的相關性
* 用於理解這一層次結構的相關結構的步驟：
  1. 在生物反應器和日期內平均波長強度
  2. 跨日期滯後平均強度
  3. 創建滯後之間的相關性

* 圖9.4(b)顯示前13個滯後日的自相關：
  - 第一個滯後的相關性大於0.95
  - 相關性相當快地減弱

### 生物反應器層級的獨立性 (Independence at Bioreactor Level)
* 最高層次結構是生物反應器
* 由於一個生物反應器內的反應不影響另一個反應器內發生的事情，因此這些層次的數據彼此獨立
* 但如前所述，生物反應器內的數據可能彼此相關

### 預測單位的確定 (Determination of Unit of Prediction)
* 由於光譜是同時測量的，可以將這一層次視為低於預測單位
* 不會對特定波長進行預測
* 模型的使用情景是對特定細胞生長天數的生物反應器進行預測
* 因此，**預測單位** (Unit of prediction) 是生物反應器內的日期

### 重抽樣方法的選擇指導 (Guidance for Resampling Method Selection)
* 理解單位對選擇**交叉驗證** (Cross validation) 方法（第3.4節）至關重要
* 對於新日期的模型預測能力進行誠實評估非常重要

* 錯誤的重抽樣方法示例：
  - 如果將每一天（在每個生物反應器內）視為獨立實驗單位
  - 使用V折交叉驗證作為重抽樣技術
  - 這種情況下，同一生物反應器內的日期可能同時出現在分析集和評估集中
  - 考慮到日期內相關數據的數量，這是一個壞主意，會導致模型特徵的人為樂觀化

* 更適當的重抽樣技術：
  - 整體排除一個或多個生物反應器的所有數據
  - 模型中將使用日期效應，因此不同日期的數據集合應隨每個生物反應器移動
  - 分配到分析集或評估集時保持完整

* 本章最後部分將比較這兩種重抽樣方法

## 後續內容預告
* 接下來幾節將描述適用於這類數據的預處理方法序列
* 雖然這些方法主要用於光譜學，但它們說明了如何將預處理和特徵工程應用於不同層次的數據

## 9.3 減少背景干擾 (Reducing Background)

### 光譜數據中的基線漂移
* 圖9.3中可以觀察到，小型和大型生物反應器的每日數據在強度軸上發生偏移
  - 在較高波長處，強度趨於下降
* 在光譜數據中，強度從零點偏離稱為**基線漂移** (Baseline drift)
  - 參考：Rinnan, Van Den Berg 和 Engelsen (2009)
  - 主要由測量系統噪聲、干擾或熒光造成
  - 而非由樣本的實際化學物質組成造成

### 背景干擾的影響
* 基線漂移是測量變異的主要貢獻因素
* 對於圖9.3中的小型生物反應器：
  - 垂直變異遠大於由光譜峰值造成的變異
  - 這些峰值可能與葡萄糖分子的含量相關
* 來自虛假來源的過度變異會對某些模型產生不利影響：
  - **主成分迴歸** (Principal Component Regression)
  - **偏最小平方法** (Partial Least Squares)
  - 這些模型受預測變數變異驅動

### 背景干擾的理想消除
* 如果背景完全被消除，那麼對樣本中不存在的分子沒有反應的波數強度應為零
* 雖然在測量過程中可以採取特定步驟減少噪聲、干擾和熒光
* 但實驗上幾乎不可能移除所有背景
* 因此，必須近似背景模式，並從觀測強度中移除這種近似

### 多項式擬合背景近似法
* 近似背景的簡單而巧妙的方法是**多項式擬合** (Polynomial fit)
  - 使用多項式擬合光譜中的最低強度值
* 這個過程的算法未在原文中完整顯示

* 對於大多數數據，3至5階的多項式近似基線通常就足夠了

### 背景校正的實際效果
* 圖9.5說明了：
  - 原始光譜
  - 最終5階多項式擬合
  - 校正後的波長強度
* 校正效果：
  - 雖然光譜的某些區域仍高於零
  - 但光譜已被扭曲，使低強度區域接近零
  - 這有助於突出與分析物相關的峰值，減少測量系統引入的背景干擾

## 9.4 減少其他噪聲 (Reducing Other Noise)

### 處理剖面數據中的噪聲類型
* 剖面數據中的噪聲主要以兩種形式出現：
  - **振幅變化噪聲** (Amplitude variation noise)
  - **波長內強度測量噪聲** (Within-wavelength intensity measurement noise)

### 處理振幅變化噪聲
* 第一種噪聲表現為光譜間振幅的巨大差異：
  - 很可能是由測量系統變異造成
  - 而非樣本中分子類型和數量的變異
* 樣本內容的指示更多來自於光譜間的**相對峰值振幅** (Relative peak amplitudes)

* 解決方法：**標準化** (Standardization)
  - 將基線調整後的強度值在每個光譜內標準化
  - 使每個光譜的整體平均值為0，標準差為1
  - 在光譜文獻中被稱為**標準正態變量** (Standard Normal Variate, SNV)
  - 這種方法確保樣本內容可以直接在樣本間比較

* 標準化過程中的注意事項：
  - 平均值和標準差可能受到極端值的影響
  - 可使用**修剪** (Trimming) 方法：
    + 計算統計量時排除最極端的值
    + 提供更穩健的中心和離散程度估計
    + 代表數據絕大多數的典型情況

* 圖9.6比較了：
  - (a)部分：所有日期和小型生物反應器中變異最低和最高的光譜剖面（基線校正後）
  - 展示了剖面振幅可能有很大變化
  - (b)部分：標準化後的剖面變得更具可比性
  - 使與反應相關的光譜中更微妙的變化可被識別為信號

### 處理波長內強度測量噪聲
* 第二種噪聲表現為光譜內各波長測量的強度噪聲
  - 可以從圖9.5的「原始」或「校正」面板中看到剖面的鋸齒狀性質

* 減少這種噪聲的方法：
  - **光滑樣條** (Smoothing splines)
  - **移動平均** (Moving averages)

* **移動平均**的計算方法：
  - 計算大小為$k$的點$p$的移動平均：將$k-1$個前面的值和當前值平均，用平均值替換當前值
  - 平均計算中考慮的點越多，曲線越平滑

* 圖9.7展示了移動平均的影響：
  - 聚焦於第一個小型生物反應器第一天的波長950至1200區域
  - 顯示了標準化光譜以及窗口大小為5、10和50的移動平均結果
  - 標準化光譜在此區域相當鋸齒狀
  - 隨著移動平均中波長數的增加，剖面變得更加平滑

* 選擇最佳窗口大小的考量：
  - 關鍵是找到能代表剖面峰值和谷值一般結構而不移除它們的值
  - 窗口大小為5：移除了標準化剖面的大部分鋸齒，但仍然緊密追蹤原始剖面
  - 窗口大小為50：不再代表原始剖面的主要峰值和谷值
  - 對於這些數據，15是保留整體結構的合理數字

* 重要提示：
  - 移動平均計算中考慮的適當點數對每種數據類型和應用都會不同
  - 通過視覺檢查不同窗口大小計算的影響（如圖9.7所示）是識別適當窗口大小的好方法

## 9.5 利用相關性 (Exploiting Correlation)

### 降低波長間相關性的必要性
* 先前步驟（估計和減少基線、減少噪聲）有助於：
  - 精煉剖面
  - 增強與反應相關的真實信號
* 但這些步驟**並未減少**每個樣本內的**波長間相關性** (Between-wavelength correlation)
* 這種相關性對許多預測模型仍是一個問題特性

### 降低波長間相關性的方法
* 可使用第6.3節描述的幾種方法：
  - **主成分分析** (Principal Component Analysis, PCA)
  - **核主成分分析** (Kernel PCA)
  - **獨立成分分析** (Independent Component Analysis)

* 這些技術對所有樣本的預測變數進行**維度降低** (Dimension reduction)
* 與之前步驟的不同之處：
  - 基線校正和噪聲減少步驟在**每個樣本內**執行
  - 這些降維技術是**跨所有樣本**執行

* 傳統PCA的特點：
  - 以最大化所有預測變數測量值的跨樣本變異的方式壓縮預測變數
  - PCA不考慮反應變數，可能不會產生具預測性的特徵
  - 雖可解決預測變數間相關性問題，但不保證產生有效模型

### PCA在小型生物反應器數據上的應用
* 圖9.8(a)顯示了**陡坡圖** (Scree plot)：
  - 11個成分解釋了約80%的預測變數變異
  - 33個成分解釋了近90%的變異

* 圖9.8(b)顯示反應與前三個新成分之間的關係：
  - 新成分彼此不相關，是任何預測模型的理想輸入
  - 但前三個成分與反應都沒有強關係

* 對於高度相關的預測變數，當目標是找到預測變數與反應之間的最佳關係時：
  - 更有效的替代技術是**偏最小平方法** (Partial Least Squares)（第6.3.1.5節描述）

### 一階微分法降低相關性
* 第二種降低相關性的方法是通過每個剖面內的**一階微分** (First-order differentiation)
* 計算方法：
  - 從剖面中第$p$個值的響應中減去第$(p-1)$個值的響應
  - 差異代表剖面中連續測量之間響應的變化率
  - 較大的變化對應於較大的移動，可能與反應中的信號相關

* 一階微分的益處：
  - 使新值相對於前一個值
  - 移除與當前值相距2步或更多步的值的關係
  - 大幅降低剖面間的自相關
  - 與本章引言中顯示的方程直接相關：波長間的大正協方差可極大降低差異中的噪聲和變異

### 微分處理的效果
* 圖9.9顯示在計算微分前後，第一天和生物反應器內跨前200個滯後的自相關值
  - 自相關大幅下降
  - 只有前3個滯後的相關性大於0.95

* 圖9.10比較了第一個小型生物反應器第一天的原始剖面和經過基線校正、標準化和一階微分處理後的剖面
  - 展示了如何去除光譜內漂移
  - 最小化與峰值無關的大多數趨勢

### 處理剩餘相關性的方法
* 雖然高度相關的滯後差異數量很小，但仍會對預測模型造成問題
* 解決方案：
  1. 選擇每第$m$個剖面：
     - $m$的選擇使得該滯後的自相關降至閾值以下（如0.9或0.95）
  2. 使用相關性過濾器（第2節）跨所有剖面過濾高度相關的差異

## 9.6 數據處理對建模的影響 (Impacts of Data Processing on Modeling)

### 預處理作為調優參數
* 數據預處理的程度可被視為**調優參數** (Tuning parameter)
* 目標：選擇最佳模型和適當的信號處理量
* 策略：
  - 使用小型反應器數據及其相應的重抽樣性能估計，選擇分析方法的最佳組合
  - 一旦確定一到兩個預處理和建模的候選組合，使用小型反應器光譜建立的模型來預測大型反應器的類似數據
  - 小型生物反應器作為訓練數據，大型生物反應器作為測試數據
  - 希望在一個數據集上建立的模型適用於生產規模反應器的數據

### 交叉驗證策略選擇
* 訓練數據包含15個小型生物反應器，每個有14個每日測量值
* 雖然實驗單位數量小，但仍有幾種合理的交叉驗證選項：

#### 留一生物反應器出交叉驗證 (Leave-one-bioreactor-out)
* 將14個小型生物反應器的數據放入分析集
* 使用1個小型生物反應器的數據作為評估集

#### 分組V折交叉驗證 (Grouped V-fold)
* 使用生物反應器作為實驗單位
* 在這種情況下，V=5是一個自然選擇
* 每個折將12個生物反應器放入分析集，3個放入評估集
* 例如：
  | 重抽樣 | 保留的生物反應器 |
  | ------ | ---------------- |
  | 1      | 5, 9, 13         |
  | 2      | 4, 6, 11         |
  | 3      | 3, 7, 15         |
  | 4      | 1, 8, 10         |
  | 5      | 2, 12, 14        |

#### 重複交叉驗證的必要性 (Repeated Cross-validation)
* 當實驗單位數量少時，一個單位對模型調優和交叉驗證性能的影響增加
* 僅一個不尋常的單位就可能改變最佳調優參數選擇和模型預測性能估計
* 跨更多交叉驗證複製品平均性能（重複交叉驗證）是減弱不尋常單位影響的有效方法
* 通常，分組V折交叉驗證的5次重複就足夠
* 重複次數依賴於：
  - 問題的計算負擔
  - 樣本大小
  - 對於較小的數據集，增加重複次數將產生更準確的性能指標
  - 對於較大的數據集，重複次數可能需要少於5次才能在計算上可行
* 本例中，執行了5次重複的分組5折交叉驗證

### 不同類型模型的適用性

#### 剖面數據原始形式的限制
* 當剖面數據處於原始形式時，建模技術的選擇有限
* 常選擇同時進行維度降低和預測的建模技術：
  - **主成分迴歸** (Principal Component Regression, PCR)
  - **偏最小平方法** (Partial Least Squares, PLS)

#### PLS的受歡迎原因
* 將預測變數信息壓縮到與反應最佳相關的預測變數空間的較小區域
* 但PLS和PCR僅在預測變數和反應之間的關係遵循直線或平面時有效
* 當預測變數和反應之間的基礎關係是非線性時，這些方法不是最佳的

#### 其他模型類型的考量
* **神經網絡**和**支持向量機** (Support Vector Machines)：
  - 無法直接處理剖面數據
  - 但發現預測變數和反應之間非線性關係的能力使其成為非常理想的建模技術
  - 本章前面介紹的預處理技術將使這些技術可應用於剖面數據

* **樹基方法** (Tree-based methods)：
  - 也能容忍剖面數據的高度相關性
  - 主要缺點：由於預測變數之間的高相關性，變數重要性計算可能被誤導
  - 如果數據中的趨勢確實是線性的，這些模型將必須更努力地近似線性模式

### 模型比較與預處理影響

#### 模型訓練策略
* 訓練線性、非線性和樹基模型
* 具體探索以下模型的性能：
  - **偏最小平方法** (PLS)
  - **Cubist** (基於規則的模型)
  - **徑向基函數支持向量機** (Radial Basis Function SVMs)
  - **前饋神經網絡** (Feed-forward Neural Networks)

* 每個模型使用預處理序列訓練：
  - 基線校正
  - 標準化
  - 平滑處理
  - 一階導數
  - 模型特定的預處理步驟也在每個剖面預處理序列中應用
  - 例如，對PLS有益的是對每個預測變數進行中心化和縮放
  - 雖然剖面預處理步驟顯著降低了預測變數之間的相關性，但仍存在一些高相關性
  - 因此，對SVM和神經網絡模型，移除高度相關的預測變數是額外步驟

#### 預處理對模型性能的影響
* 圖9.11顯示了跨模型和剖面預處理步驟的重複交叉驗證結果
* 重要發現：
  1. 對於SVM和神經網絡模型，評估集的整體平均模型性能在剖面預處理後大幅提升
     - 神經網絡的原始數據交叉驗證RMSE為5.34
     - 剖面預處理後，交叉驗證RMSE降至3.41
  2. 對於PLS和Cubist模型，剖面預處理提供了一些整體預測能力的改善
     - 更明顯的是RMSE變異的減少
     - PLS在無剖面預處理時評估集性能的標準差為3.08
     - 剖面預處理後降至2.6

* 基於這些結果：
  - 使用**導數特徵** (Derivative features) 的PLS似乎是最佳組合
  - 使用導數的支持向量機也顯得有希望
  - Cubist模型在計算導數之前也顯示了良好的結果

#### PLS模型詳細結果
* 圖9.12更詳細地顯示了PLS結果：
  - 展示了每種預處理方法保留的成分數量與重抽樣RMSE估計的關係
  - 在許多預處理方法中，性能是等效的
  - 但使用導數明顯對結果產生了積極影響
  - 不僅RMSE值更小，而且只需少量成分就能優化性能
  - 這很可能是由於差分使用後波長特徵之間相關性的減少

### 最終模型評估與選擇
* 將兩個候選模型應用於大型生物反應器測試集
* 圖9.13顯示了每種預處理方法的觀測值和預測值（按天著色）
* 結果顯示：
  - 直到標準化之前，模型明顯低估了葡萄糖值（特別是在初始日）
  - 即使在模型擬合最佳時，反應的最初幾天在預測中似乎具有最多的噪聲
  - 在數值上，Cubist模型的RMSE值略小（Cubist為2.07，PLS為2.14）
  - 但考慮到PLS模型的簡單性，這種方法可能更受青睞

### 實驗單位選擇對交叉驗證的影響
* 建模例證基於實驗單位是生物反應器而非生物反應器內的單個日期的知識
* 理解用於適當交叉驗證方案的單位非常重要
* 對這些數據，生物反應器內的每日測量比生物反應器之間的測量更高度相關
* 基於每日測量的交叉驗證可能導致更好的保留性能
* 但這種性能會產生誤導，因為在這種情況下，新數據將基於全新的生物反應器

* 額外實驗結果（圖9.14）：
  - 在小型數據上執行了重複交叉驗證，不當地將日期視為實驗單位
  - 比較表明：當使用日期作為實驗單位時，所有模型和所有剖面預處理步驟的保留RMSE值都人為地低於使用生物反應器作為單位時
  - 忽視或不知道生物反應器是實驗單位，可能導致對模型預測性能過於樂觀

## 9.7 總結 (Summary)

### 剖面數據的特性與來源
* **剖面數據** (Profile data) 是一種特殊類型的數據，可源自多種不同結構：
  - 樣本隨時間重複測量
  - 樣本具有許多高度相關/相關的預測變數
  - 樣本測量通過層次結構進行

### 實驗單位識別的重要性
* 分析師需要敏銳地意識到**實驗單位** (Experimental unit) 是什麼
* 了解單位影響多項關鍵決策：
  - 如何對剖面進行預處理
  - 如何將樣本分配到訓練集和測試集
  - 重抽樣期間如何分配樣本

### 剖面數據的基本預處理步驟
* 剖面數據的預處理可包括三個主要方面：
  1. **減少基線效應** (Reducing baseline effect)
  2. **減少剖面間的噪聲** (Reducing noise across the profile)
  3. **利用預測變數間相關性中包含的信息** (Harnessing information in correlation)

### 預處理的根本目標
* 移除阻止這類數據用於大多數預測模型的特性
* 同時保留剖面與結果之間的預測信號
* 沒有一種特定的步驟組合適用於所有數據
* 但是，組合適當的步驟可以產生非常有效的模型

# 10 特徵選擇概述 (Feature Selection Overview)

## 特徵工程與特徵選擇的關係
* 第5至9章提供了工程化特徵（或預測變數）的工具，使模型能更好地找到與結果相關的預測信號
* 不同特徵工程技術產生不同的結果：
  - **1:1轉換** (1-1 Transformations) 或**維度降低方法** (Dimension Reduction Methods)：
    + 產生與原始數據相同或更少的預測變數
  - **基底展開** (Basis Expansions)：
    + 生成比原始數據更多的特徵

## 特徵選擇的必要性
* 新工程化的預測變數可能捕捉到與結果的預測關係，但有些可能與結果無關
* 許多原始預測變數也可能不包含預測信息
* 對於多種模型而言，隨著無信息預測變數數量增加，預測性能會下降
* 因此，為建模適當選擇預測變數是真正的需求

## 本章與後續章節的重點
* 後續章節將專注於**監督式特徵選擇** (Supervised Feature Selection)：
  - 保留哪些預測變數的選擇由它們對結果的影響引導
* 本章將提供：
  - 特徵選擇的介紹
  - 這些方法的一般術語
  - 一些顯著的陷阱

## 10.1 特徵選擇的目標 (Goals of Feature Selection)

### 預測能力與可解釋性的權衡
* 實踐中，合作者通常希望擁有：
  - 具有最佳預測能力的模型
  - 且模型具可解釋性
* 這種情況很少發生，因為**預測性能**與**模型可解釋性**之間存在權衡
* 對此權衡的誤解導致錯誤認知：
  - 認為只需過濾掉無信息的預測變數，就能闡明哪些因素影響結果
  - 再根據剩餘預測變數與結果的關係構建解釋

### 簡單過濾方法的問題
* 這種簡單過濾方法存在多個問題：

#### 大量預測變數的場景
* 當預測變數數量遠大於樣本數時：
  - 可能存在許多互斥的預測變數子集，產生預測性能近乎相等的模型（局部最優）
  - 找到最佳全局解決方案（表現最佳的預測變數子集）需要評估所有可能的子集組合
  - 這在計算上可能不可行

#### 全局最優解的局限性
* 即使找到全局最優解，由於可用數據中預測變數和結果的固有噪聲，識別的子集也可能不是真正的全局最優解

### 複雜模型中的解釋問題
* 許多模型將預測變數與結果關聯的方式非常複雜
* 對此類模型，幾乎不可能破譯任何單個預測變數與結果之間的關係
* 一種嘗試獲得洞察的方法：
  - 將所有其他選定的預測變數固定為單一值
  - 通過改變感興趣的預測變數觀察對結果的影響
* 這種方法過於簡單化，只提供對預測變數真實影響的有限洞察

### 移除預測變數的主要動機
* 應重新關注移除預測變數的動機，主要動機應為：
  1. 減輕預測變數與模型之間相互作用的特定問題
  2. 降低模型複雜性

#### 具體應用場景
* **支持向量機** (Support Vector Machines) 和**神經網絡** (Neural Networks)：
  - 對無關預測變數敏感
  - 多餘的預測變數在某些情況下會降低預測性能

* **線性迴歸** (Linear Regression) 或**邏輯迴歸** (Logistic Regression)：
  - 易受相關預測變數影響（見第6章）
  - 移除相關預測變數可減少**多重共線性** (Multicollinearity)
  - 使這些類型的模型能夠擬合

* 更通用的情況：
  - 即使預測模型對額外預測變數不敏感，包含能提供可接受結果的最小可能集合仍是科學上合理的
  - 在某些情況下，移除預測變數可降低獲取數據的成本或提高用於預測的軟件吞吐量

### 特徵選擇的實際目標
* 工作前提：通常模型中的預測變數越少越好
* 特徵選擇的目標重新定義為：
  > 在不影響預測性能的前提下，盡可能減少預測變數的數量

* 有多種方法可以減少預測變數集
* 下一節將概述特徵選擇技術的一般類別

## 10.2 特徵選擇方法的分類 (Classes of Feature Selection Methodologies)

### 特徵選擇的三大類別
特徵選擇方法可分為三大類：
* **內在方法** (Intrinsic/Implicit Methods)：特徵選擇自然地融入建模過程
* **過濾方法** (Filter Methods)：在建模前預先篩選特徵
* **包裝方法** (Wrapper Methods)：通過迭代搜索程序選擇最佳特徵子集

### 內在方法 (Intrinsic Methods)
* 特徵選擇最無縫且重要的類別
* 主要示例：
  1. **樹型和規則型模型** (Tree- and Rule-based Models)：
     - 搜索最佳預測變數和分割點，使每個新分區內的結果更均質（參見第5.7節）
     - 如果預測變數未用於任何分割，則功能上獨立於預測方程，從模型中排除
     - 樹集成也具有相同特性，但某些算法（如隨機森林）會在構建樹時在不相關預測變數上強制分割

  2. **多變量自適應迴歸樣條模型** (MARS)：
     - 創建涉及一次或兩次預測變數的新特徵（第6.2.1節）
     - 然後將預測變數按順序加入線性模型
     - 如果預測變數未涉及至少一個MARS特徵，則從模型中排除

  3. **正則化模型** (Regularization Models)：
     - 通過懲罰或收縮預測變數係數來改善模型擬合
     - **LASSO** 使用將係數收縮至絕對零的懲罰類型，強制從最終模型中排除預測變數

* **優勢**：
  - 相對較快，因為選擇過程嵌入在模型擬合過程中
  - 不需要外部特徵選擇工具
  - 在選擇特徵和目標函數之間提供直接連接
  - 選擇與建模目標之間的直接連接使得在特徵稀疏性和預測性能之間做出明智選擇更容易

* **劣勢**：
  - 模型依賴性：如果數據更適合非內在特徵選擇類型的模型，當使用所有特徵時，預測性能可能次優
  - 某些內在模型（如單樹模型）使用貪婪方法進行特徵選擇，通常會識別出具有次優預測性能的狹窄特徵集

### 過濾方法 (Filter Methods)
* **定義**：對預測變數進行初步監督分析以確定重要性，然後只將這些提供給模型
* **特點**：搜索僅執行一次，通常單獨考慮每個預測變數（非必需）

* **案例**：第5.6節中的OkCupid文本關鍵詞
  - 使用**勝算比** (Odds-ratio) 評估關鍵詞出現與結果之間的關係
  - 基於統計顯著性和勝算比大小制定保留或排除的規則
  - 通過過濾的單詞添加到邏輯迴歸模型中

* **優勢**：
  - 簡單且通常較快
  - 有效捕捉數據中的大型趨勢（即個別預測變數-結果關係）

* **劣勢**：
  - 傾向於過度選擇預測變數
  - 由於單獨考慮每個預測變數，可能無法捕捉獨立趨勢
  - 過濾方法的目標函數（如顯著性）與模型所需（預測性能）之間可能存在脫節
  - 滿足統計顯著性等過濾標準的預測變數集可能不是改善預測性能的集合

### 包裝方法 (Wrapper Methods)
* **定義**：使用迭代搜索程序，重複向模型提供預測變數子集，然後使用結果模型性能估計來指導下一個評估子集的選擇
* **目標**：迭代到具有比原始預測變數集更好預測性能的較小預測變數集

* **方法類型**：
  1. **貪婪方法** (Greedy Search)：
     - 基於當時看起來最好的方向選擇搜索路徑，以實現最佳即時收益
     - 可能在局部最佳設置停滯
     - 例如：**後向選擇**/**遞歸特徵消除** (RFE)
       + 預測變數初始按重要性排序
       + 使用完整預測變數集創建初始模型
       + 下一個模型基於移除最不重要的較小預測變數集
       + 過程按照預定路徑繼續，直到模型中只有很少數量的預測變數
       + 性能估計用於確定移除了過多特徵的時間點

  2. **非貪婪方法** (Non-greedy Search)：
     - 重新評估先前的特徵組合
     - 有能力在初始不利的方向移動，如果在當前步驟后似乎有潛在收益
     - 允許非貪婪方法擺脫局部最優的陷阱
     - 例如：**遺傳算法** (Genetic Algorithms, GA) 和**模擬退火** (Simulated Annealing, SA)
       + SA方法非貪婪，因為在特徵選擇過程中納入了隨機性
       + 隨機成分幫助SA找到新的搜索空間，通常導致更優結果

* **優勢**：
  - 潛在可搜索比簡單過濾器或具有內置特徵選擇的模型更廣泛的預測變數子集
  - 最有可能找到全局最佳預測變數子集（如果存在）

* **劣勢**：
  - 計算時間可能過長，甚至適得其反
  - 當與計算密集型模型（如SVM和神經網絡）結合時，計算時間問題可能進一步加劇
  - 最有可能將預測變數過度擬合到訓練數據，需要外部驗證

### 貪婪與非貪婪搜索方法比較
* 圖10.1直觀對比了貪婪和非貪婪搜索方法，使用**Goldstein-Price方程**：
  - 在這個函數中，最小值位於$x=0$和$y=-1$
  - 面板(a)顯示兩種使用梯度下降的貪婪方法結果：
    + 第一個起始值（橙色，右上）沿著梯度定義的路徑移動，在局部最小值停滯
    + 第二次嘗試從不同起點開始，經過多次路徑修正，最終在第131次迭代接近最佳值
  - 面板(b)顯示一種名為模擬退火的全局搜索方法：
    + 通過從當前點隨機偏移生成新候選點
    + 如果新點更好，則繼續；如果更差，也可能以一定概率接受
    + 全局搜索不總是沿當前最佳方向前進，傾向於探索更廣泛的候選值
    + 在這個特定搜索中，更快（第64次迭代）確定了接近最優的值

### 實用策略
* 特徵選擇的良好策略：
  1. 從一個或多個內在方法開始，看看它們能產生什麼
  2. 不要期望使用內在特徵選擇的模型會選擇相同的預測變數子集
  3. 如果非線性內在方法具有良好預測性能，可考慮結合非線性模型的包裝方法
  4. 如果線性內在方法具有良好預測性能，可考慮結合線性模型的包裝方法
  5. 如果多種方法選擇大型預測變數集，可能意味著減少特徵數量不可行

## 10.3 無關特徵的影響 (Effect of Irrelevant Features)

### 無關預測變數對模型的影響
* 多餘預測變數對模型的影響程度取決於：
  - 模型類型
  - 預測變數的性質
  - 訓練集大小與預測變數數量比率（即 p:n 比率）

### 模擬研究設計
* 使用 Sapp, Laan 和 Canny (2014) 的模擬系統
* 包含20個相關預測變數的非線性函數：
  $$y = x_1 + \sin(x_2) + \log(|x_3|) + x_4^2 + x_5x_6 + I(x_7x_8x_9 < 0) + I(x_{10} > 0) + x_{11}I(x_{11} > 0) + \sqrt{(|x_{12}|)} + \cos(x_{13}) + 2x_{14} + |x_{15}| + I(x_{16} < -1) + x_{17}I(x_{17} < -1) - 2x_{18} - x_{19}x_{20} + \epsilon$$

* 模擬設置：
  - 每個預測變數使用獨立標準正態隨機變數生成
  - 誤差項為均值為零、標準差為3的隨機正態分布
  - 添加了10至200個隨機標準正態預測變數（與結果無關）
  - 訓練集大小為 n = 500 或 n = 1,000
  - 使用**均方根誤差** (Root Mean Squared Error, RMSE) 在大型模擬測試集上評估模型質量

* 評估多種模型類型：
  - 線性模型
  - 非線性模型
  - 樹型/規則型模型

### 不同模型的表現結果
圖10.2顯示了不同模型在無關預測變數增加時的RMSE趨勢：

### 線性模型
* **普通線性迴歸** (Ordinary Linear Regression)：
  - 整體表現平平
  - 隨著額外預測變數增加，性能逐漸下降
  - 當訓練集大小從500增加到1,000時，影響較小
  - 證實 p:n 比率可能是潛在問題

* **glmnet**（帶LASSO懲罰）：
  - 相較線性迴歸表現更穩定
  - 隨無關預測變數增加，性能保持穩定
  - 歸因於LASSO懲罰的特性

### 非線性模型
* **K-最近鄰** (K-Nearest Neighbors)：
  - 整體表現不佳
  - 受p增加的影響中等

* **多變量自適應迴歸樣條** (MARS)：
  - 表現良好
  - 對噪聲特徵具有良好抵抗力
  - 歸因於該模型內在的特徵選擇

* **單層神經網絡** (Single Layer Neural Networks) 和**支持向量機** (Support Vector Machines)：
  - 當沒有額外預測變數時表現最佳（最接近最佳可能RMSE 3.0）
  - 然而，噪聲預測變數的加入極大地影響了這些模型
  - 最終成為模擬中表現最差的模型之一

#### 樹型模型
* **隨機森林** (Random Forest) 和**裝袋** (Bagging)：
  - 表現相似但平平
  - 幾乎不受p增加的影響

* **Cubist** (基於規則的集成) 和**提升樹** (Boosted Trees)：
  - 表現較好
  - 隨著預測變數增加，表現出中等程度的下降
  - 提升更容易受到這個問題的影響

### 內在特徵選擇的有效性
* 由於這是一個模擬研究，可以評估內建特徵選擇的模型在找到正確預測變數子集方面的表現
* 計算兩個指標：
  - **靈敏度** (Sensitivity)：保留在模型中的真實預測變數比率
  - **特異度** (Specificity)：未被選中的無關預測變數比率（與假陽性率相關）

* 圖10.3顯示結果：

#### 樹集成模型的表現
* **三種樹集成**（裝袋、隨機森林和提升）：
  - 極佳的真陽性率：真正相關的預測變數幾乎總是被保留
  - 然而，對無關預測變數的結果很差，選擇了許多噪聲變數
  - 可能與模擬性質有關，大多數預測變數作為平滑連續函數進入系統
  - 可能導致基於樹的模型過度嘗試通過訓練深樹或更大集成來提高性能

* **隨機森林的特殊考量**：
  - 主要調優參數 $m_{try}$ 是每次分割時應評估的隨機選擇變數數量
  - 雖然這鼓勵多樣化樹（以提高性能），但可能需要對無關預測變數進行分割
  - 隨機森林通常會對大多數預測變數進行分割（無論其重要性），並會大大過度選擇預測變數
  - 可能需要額外分析來確定哪些預測變數真正重要

* **樹型模型的一般問題**：
  - 對高度相關的預測變數的重要性估計較差
  - 例如，添加與另一個預測變數相同的重複列會在數值上稀釋這些模型生成的重要性得分
  - 因此，使用這些模型時，重要的預測變數可能排名較低

#### Cubist模型
* **更平衡的結果**：
  - 真陽性率大於70%
  - 假陽性率約為50%（較小訓練集）
  - 當訓練集更大且額外預測變數不超過100個時，假陽性率增加
  - 與樹集成的差異解釋：Cubist使用在訓練集小子集上擬合的線性迴歸模型集成
  - 模擬系統的性質可能更適合這種類型的模型

### MARS和glmnet
* 在模擬中，這些模型在靈敏度和特異度之間進行了權衡，傾向於偏好特異度
* 假陽性率低於25%
* 真陽性率通常在40%-60%之間

### 結論
* 該數據集需要非線性模型，但模擬研究主要關注無關預測變數的相對影響
* 對於需要減少預測變數數量來避免性能下降的模型，有多種選擇
* 對於像隨機森林或glmnet這樣的模型，特徵選擇可能有助於找到較小的預測變數子集而不影響模型效能

## 10.4 對預測變數的過度擬合與外部驗證 (Overfitting to Predictors and External Validation)

### 過度擬合問題的回顧
* 第3.5節介紹了在選擇模型調優參數時過度擬合可用數據的問題
* 過度擬合發生時，模型在訓練集上過度解釋模式，導致在新數據上的預測性能下降
* 解決方案：使用未用於估計模型參數的數據集（通過驗證集或評估集）評估調優參數

### 特徵選擇中的類似問題
* 特徵選擇過程中也存在類似的過度擬合問題
* 對許多數據集而言，可能找到在訓練集上具有良好預測性能但在測試集上表現不佳的預測變數子集
* 解決方案：**特徵選擇需要成為重抽樣過程的一部分**

### 常見的特徵選擇錯誤
* 最常見的錯誤：僅在特徵選擇程序內進行重抽樣
* 以一個具有五個預測變數（標記為A至E）的模型為例：
  - 假設每個預測變數都有從訓練集得出的重要性測量（A最重要，E最不重要）
  - 使用後向選擇，第一個模型包含所有五個預測變數
  - 第二個模型移除最不重要的（本例中為E），依此類推

* 這種外部特徵選擇與內部重抽樣的方法有兩個關鍵問題：
  1. 由於特徵選擇在重抽樣之外，重抽樣無法有效測量選擇過程的影響（好或壞）
     - 重抽樣沒有暴露於選擇過程的變異中，因此無法測量其影響
  
  2. 相同的數據被用於測量性能和指導選擇例程的方向
     - 類似於擬合模型到訓練集，然後重新預測同一集合來測量性能
     - 如果模型能夠緊密擬合訓練數據，可能出現明顯偏差
     - 需要某種樣本外數據來準確確定模型的表現
     - 如果選擇過程導致過度擬合，沒有剩餘數據可能告知我們問題

### 真實案例研究
* Ambroise 和 McLachlan (2002) 重新分析了 Guyon 等人 (2002) 的結果：
  - 對來自RNA表達微陣列的高維分類數據進行建模
  - 樣本數量少（少於100），而預測變數數量高（2K至7K）
  - 使用線性支持向量機擬合數據，後向消除（即RFE）用於特徵選擇
  - 原始分析使用留一交叉驗證 (LOO)，錯誤率接近零
  - 然而，當保留一組樣本僅用於測量性能時，錯誤率高出15%-20%
  - 在p:n比率最差的情況下，即使類標籤被打亂，仍能達到零LOO錯誤率

### 正確結合特徵選擇和重抽樣的方法
* 較好的方式：將特徵選擇作為建模過程的組件
* 特徵選擇應與預處理和其他工程任務以相同方式納入
* 適當的特徵選擇方法是**在重抽樣過程內**進行

* 正確的過程（使用前面五個預測變數的例子）：
  1. 進行外部重抽樣：生成多個分析/評估集對
  2. 對於每個分析集，確定預測變數排名
  3. 使用排名執行順序特徵移除
  4. 對每個子集擬合模型並在相應的評估集上測量性能
  5. 選擇具有最佳性能的子集大小
  6. 使用整個訓練集確定最終排名
  7. 選擇最佳大小的預測變數
  8. 使用這些預測變數擬合最終模型

* 在此過程中，不使用相同的數據集來確定最佳子集大小和子集內的預測變數
* 外部重抽樣循環用於決定選擇過程應該向下移除的程度
* 子集大小被視為**調優參數**

### 在重抽樣過程中執行特徵選擇的影響
* 兩個主要影響：

  1. **提供更現實的預測性能估計**：
     - 重抽樣迫使建模過程使用不同的數據集，以便準確測量性能變異
     - 結果反映了整個建模過程的不同實現
     - 如果特徵選擇過程不穩定，了解結果可能有多嘈雜很有幫助
     - 最終，重抽樣用於測量建模過程的整體性能，並嘗試估計使用最佳預測變數將最終模型擬合到訓練集時的性能

  2. **計算負擔增加**：
     - 對於對調優參數選擇敏感的模型，評估每個子集時可能需要重新調優模型
     - 這種情況下，需要單獨的嵌套重抽樣過程來調優模型
     - 在許多情況下，計算成本可能使搜索工具在實際中不可行

### 特殊情況的考量
* 在選擇過程不穩定的情況下：
  - 使用多次重抽樣是昂貴但值得的方法
  - 可能出現在數據集較小、特徵數量較大或存在極端類不平衡時
  - 這些因素可能在數據稍微改變時產生不同的特徵集

* 大型數據集：
  - 大型數據集往往大大降低特徵選擇期間過度擬合預測變數的風險
  - 在這種情況下，使用單獨的數據分割進行特徵排名/過濾、建模和評估可以既高效又有效

## 10.5 案例研究 (A Case Study)

### 不當結合特徵選擇與重抽樣的實例
* 一個近期合作案例突顯了不正確結合特徵選擇和重抽樣的危險
* 案例背景：
  - 研究人員收集了兩個類別，每類75個樣本
  - 每個數據點約有10,000個預測變數
  - 最終目標：識別能將樣本分類至正確響應的預測變數子集，準確率至少達到80%

### 研究人員的方法
* 初始方法：
  - 將70%的數據用於訓練集
  - 使用10折交叉驗證進行模型訓練
  - 採用glmnet和隨機森林的內在特徵選擇方法
  - 結果：經過廣泛調優參數空間搜索後，最佳交叉驗證準確率僅接近60%，遠低於期望

* 嘗試降維：
  - 主成分分析後的線性判別分析
  - 偏最小平方判別分析
  - 結果：這些方法的交叉驗證準確率更差

* 研究人員的推論：
  - 找到預測性子集的挑戰部分來自於數據中大量不相關的預測變數
  - 決定首先識別和選擇與響應有單變量信號的預測變數
  - 為每個預測變數執行t檢定，按分離類別的顯著性對預測變數排序
  - 選擇排名前300的預測變數進行建模

### 研究人員的誤導性"證明"
* 為證明這種選擇方法能夠識別最佳預測變數：
  - 對300個預測變數進行主成分分析
  - 繪製樣本投影到前兩個成分的散點圖，按響應類別著色
  - 圖表顯示組間接近完美的分類
  - 研究人員得出結論：有良好證據表明有預測變數可以分類樣本

### 錯誤結論的揭示
* 遺憾的是，由於特徵選擇在重抽樣外執行，發現的表面信號僅是隨機機會
* 研究人員使用的單變量特徵選擇過程可以在完全隨機的數據中產生兩組之間的完美分離

### 隨機數據實驗證明
* 為說明這一點，生成一個150×10,000的隨機標準正態數字矩陣（μ=0和σ=1）
* 響應也是隨機選擇，使每個類別有75個樣本
* 為每個預測變數執行t檢定，選擇排名前300的預測變數
* 圖10.4顯示了樣本投影到前兩個主成分上按響應狀態著色的散點圖
* 即使在完全隨機的數據上也能得到類似的"良好"分離結果

### 關鍵教訓
* 使用不當方法得到的良好分離並不意味著所選特徵有能力預測響應
* 表面上的分離更可能是由於隨機機會和不當的特徵選擇例程
* 這強調了在特徵選擇過程中正確使用重抽樣的重要性
* 否則，即使在完全沒有真實信號的數據上，也可能誤認為發現了有意義的模式

# 11 貪婪搜索方法 (Greedy Search Methods)

本章討論貪婪搜索方法，如簡單單變量過濾器和遞歸特徵消除。在進一步討論之前，將介紹另一個數據集，用於展示這些方法的優缺點。

## 11.1 說明性數據：預測帕金森病 (Illustrative Data: Predicting Parkinson's Disease)

* **數據來源與結構**：
  - Sakar 等 (2019) 描述了一項實驗，其中252名患者（其中188名曾被診斷為帕金森病）被記錄發出特定聲音三次
  - 每次重複樣本應用多種信號處理技術，創建750個數值特徵
  - 目標：使用這些特徵對患者的帕金森病狀態進行分類

* **數據特性**：
  - 這些數據中的特徵組可視為遵循**剖面** (Profile)（第9章）
  - 許多特徵由每種信號處理器產生的相關字段集組成（例如，跨不同聲音波長或子頻段）
  - 數據存在極端的**多重共線性** (Multicollinearity)
  - 約10,750對預測變數具有大於0.75的絕對秩相關性

* **實驗設置**：
  - 基於患者疾病狀態使用**分層隨機抽樣** (Stratified random sample) 將25%的數據分配到測試集
  - 結果訓練集包含189名患者，其中138名患有該疾病
  - 評估模型的性能指標為**ROC曲線下面積** (Area under the ROC curve)

## 11.2 簡單過濾器 (Simple Filters)

### 特徵篩選的基本方法
* 最基本的特徵選擇方法是在將預測變數納入模型前篩選它們與結果的關係
* 需要一種數值評分技術來量化關係強度
* 使用評分對預測變數進行排序和過濾：
  - 使用閾值過濾
  - 選取排名前p個預測變數
* 預測變數評分方式：
  - 單獨評分每個預測變數
  - 同時評分所有預測變數（取決於使用的技術）

### 不同類型變數的評分技術
評分技術根據預測變數和結果的類型而有所不同：

#### 分類預測變數的篩選選項
* **當結果為分類型**：
  - 預測變數與結果形成**列聯表** (Contingency table)
  - 當預測變數有三個或更多水平時，可使用：
    + **卡方檢驗** (Chi-squared tests)
    + **精確方法** (Exact methods)（Agresti 2012）
  - 當預測變數恰好有兩個類別時，**勝算比** (Odds-ratio) 是有效選擇（見第5.6節）

* **當結果為數值型**：
  - 對於二級分類預測變數：使用基本的**t檢驗** (T-test)
  - 也可創建**ROC曲線** (ROC curves)和**精確率-召回率曲線** (Precision-recall curves)，計算曲線下面積
  - 當預測變數有兩個以上級別時：計算傳統**ANOVA的F統計量** (F-statistic)

#### 數值預測變數的篩選選項
* **當結果為分類型**：
  - 可使用上述方法，但角色顛倒
  - 當有大量檢驗或預測變數有實質性多重共線性時：
    + **相關調整t分數** (Correlation-adjusted t-scores)（Opgen-Rhein和Strimmer，2007；Zuber和Strimmer，2009）
    + 是簡單ANOVA統計的良好替代方案

* **當結果為數值型**：
  - 可計算簡單的**成對相關性** (Pairwise correlation)（或秩相關性）統計量
  - 若關係為非線性，可使用：
    + **MIC值** (MIC values)（Reshef等，2011）
    + **A統計量** (A statistics)（Murrell，Murrell和Murrell，2016）

#### 替代方法
* **廣義加性模型** (Generalized Additive Model, GAM)（Wood，2006）：
  - 可同時為一組預測變數擬合非線性平滑項
  - 使用p值測量重要性，檢驗每個預測變數無趨勢的虛無假設
  - 圖4.15(b)展示了此類分類結果模型的示例

### 混合預測變數類型的處理
* 當所有預測變數類型相同時，生成個別預測變數有效性的摘要很簡單
* 大多數數據集包含混合預測變數類型，導致排名挑戰：
  - 篩選統計量在不同尺度上，如勝算比和t統計量不兼容
  - 可將每個統計量轉換為**p值** (P-value)，使篩選統計量有共同性

#### p值的使用與計算
* p值源自統計假設檢驗框架：
  - 假設預測變數與結果間無關聯
  - 使用數據反駁無關聯假設
  - p值是在無關聯情況下觀察到統計量值的概率

* 將統計量轉換為p值：
  - 某些統計量轉換容易（如t統計量，前提是基本假設成立）
  - 某些統計量轉換困難（如AUC）
  - 解決方案：**置換方法** (Permutation method)（Good，2013；Berry，Mielke Jr和Johnston，2016）

* **隨機化方法**工作原理：
  1. 對選定預測變數和相應結果，隨機置換預測變數（結果不變）
  2. 計算置換數據上的感興趣統計量
  3. 多次重複過程生成統計量分布（無關聯分布，即虛無分布）
  4. 將原始數據統計量與無關聯分布比較，得到p值

### 假陽性問題與解決方案
* 簡單過濾器易於找到在可用數據中有強關聯但在新數據中無關聯的預測變數（假陽性）
* 統計學專門研究開發最小化假陽性機會的方法，特別是在假設檢驗和p值背景下
* 減少假陽性的方法：
  - 調整p值使其變大（如前幾章所示）
  - 使用獨立數據集評估選定特徵

### 雙層交叉驗證進行特徵選擇
* 建模過程需實現兩個目標：
  1. 識別有效特徵子集
  2. 識別適當調優參數，使選定特徵和調優參數不過度擬合可用數據

* 使用簡單篩選過濾器時，不能在同一層交叉驗證中選擇特徵子集和模型調優參數
* 解決方案：納入另一層交叉驗證
  - 第一層（外部層）用於過濾特徵
  - 第二層（內部層）用於選擇調優參數
  - 圖11.2說明這個過程

* 計算成本：
  - 構建和評估的模型數量為$I \times E \times T$
  - $I$：內部重抽樣數
  - $E$：外部重抽樣數
  - $T$：調優參數組合總數

### 帕金森病數據的簡單過濾器應用

#### 數據特性與模型選擇
* 帕金森病數據的特點：
  - 預測變數高度多重共線性
  - 樣本量小
  - 結果不平衡（74.6%患者有疾病）

* 選擇**偏最小平方判別分析** (Partial Least Squares Discriminant Analysis, PLSDA)（Barker和Rayens，2003）：
  - 產生線性類別邊界，可能限制過度擬合多數類
  - 調優參數是保留的成分數量

#### 過濾標準與過程
* 使用**ROC曲線下面積** (Area under the ROC curve) 決定是否包含特徵：
  - 訓練集初步分析顯示5個預測變數ROC曲線下面積至少為0.80
  - 21個介於0.75至0.80之間

* 內部重抽樣過程：
  - 若音頻特徵ROC曲線下面積至少為0.75，則選中
  - 選中的特徵傳遞到相應外部抽樣節點進行模型調優

* 重抽樣設置：
  - 內部重抽樣：20次自助重抽樣
  - 外部重抽樣：10折交叉驗證的2次重複
  - 最終性能估計基於外部重抽樣保留集
  - 數據預處理：對每個預測變數應用Yeo-Johnson轉換，然後中心化和縮放

#### 結果與分析
* 重抽樣期間選定的預測變數數量：
  - 範圍：2至12
  - 平均：5.7
  - 2個預測變數在每次重抽樣中通過ROC閾值

* 最終模型：
  - 選中5個預測變數
  - PLSDA模型使用4個成分優化
  - 估計ROC曲線下面積為0.827

* 選定預測變數的冗餘性：
  - 圖11.3顯示秩相關矩陣熱圖
  - 很少絕對相關小於0.5的對，許多極端值
  - PLS模型創建原始數據的線性組合提取特徵
  - 最終模型使用4個成分，每個成分是5個原始預測變數的組合
  - 相關矩陣顯示明顯的區塊/集群分離，表明過濾預測變數中只有少數基本信息
  - 主成分分析顯示只需1個特徵即可捕獲90%總變異

#### 過濾過程的效用評估
* 與完整預測變數集的比較：
  - 使用同樣重抽樣擬合PLS模型（使用全部預測變數集）
  - 也偏好少量投影（4個成分）
  - ROC曲線下面積估計略差（0.812對0.827）
  - 成對t檢驗p值（p=0.446）顯示改進無統計顯著性
  - 但僅使用原始變數的0.7%，新模型的簡單性可能非常有吸引力

* 事後分析：確定所選子集是否優於相同大小的隨機子集
  - 創建100個5個預測變數的隨機子集
  - 使用相同外部交叉驗證例程擬合PLS模型
  - 結果：上述方法ROC曲線下面積（0.827）優於所有使用相同大小隨機子集生成的值
  - 隨機子集最大AUC為0.832
  - 這表明過濾和建模方法找到了具有可信信號的子集

#### 進一步優化的可能性
* PLS能很好地適應高度共線性的預測變數集
* 可通過增加過濾器嚴格性（AUC≥0.75）減少子集大小
* 改變閾值會影響解決方案的稀疏性和模型性能
* 此過程與下一節將討論的**後向選擇** (Backwards selection) 相同

#### 簡單過濾方法總結
* 簡單篩選在建模前可以有效且相對高效
* 過濾器應包含在重抽樣過程中，避免性能的過於樂觀評估
* 缺點：
  - 選定特徵可能存在冗餘
  - 過濾閾值的主觀性可能使建模者想了解在性能受影響前可移除多少特徵

## 11.3 遞迴特徵消除（Recursive Feature Elimination）

- **遞迴特徵消除**（Recursive Feature Elimination, RFE）是一種基本的預測變數向後選擇方法，最初由 Guyon 等人（2002）提出
  - 運作原理：
    * 首先基於全部預測變數建立模型並計算每個預測變數的重要性分數
    * 移除最不重要的預測變數
    * 重新建立模型並再次計算重要性分數
    * 重複此過程

- **實際應用中**：
  * 分析師需指定要評估的預測變數子集數量以及每個子集的大小
  * 子集大小作為 RFE 的調整參數（tuning parameter）
  * 選擇能優化性能標準的子集大小來選擇預測變數
  * 最佳子集用於訓練最終模型

- **子集大小估計**：
  * 如第 10.4 節所述，選擇過程的重採樣（resampling）方式與模型的基本調整參數相同
  * 特徵選擇例程包含在重採樣過程中
  * 外部重採樣用於估計適當的子集大小

- **模型限制**：
  * 並非所有模型都適用於 RFE 方法
  * 當預測變數數量超過樣本數量時，某些模型無法使用
    - 這些模型包括：多元線性迴歸（multiple linear regression）、邏輯迴歸（logistic regression）和線性判別分析（linear discriminant analysis）
  * 若要將這些技術與 RFE 結合使用，必須先減少預測變數

- **隨機森林**（Random Forest）與 RFE：
  * 隨機森林特別適合與 RFE 結合使用的原因：
    1. 隨機森林傾向於不排除變數 → 源於集成模型本質
       - 集成模型性能提升與組成模型的多樣性相關
       - 隨機森林強制樹包含預測變數的次優分割，使用預測變數的隨機樣本($m_{try}$)
       - 限制可能用於分割的預測變數數量增加了不相關預測變數被使用的可能性
    2. 隨機森林有著名的內部特徵重要性測量方法（如第 7.4 節所述）

- **多共線性問題**（Multicollinearity）：
  * 高度相關的預測變數會稀釋重要性分數
  * 圖 11.4 展示了這種現象：當添加冗餘特徵時，$x_4$ 的重要性明顯下降
  * 建議：在開始 RFE 程序前過濾高度相關的特徵

- **帕金森病數據案例**：
  * 使用隨機森林進行向後選擇，每個集成包含 10,000 棵樹
  * 使用模型的重要性分數對預測變數進行排名
  * 由於預測變數數量眾多且相關性高，子集大小以 $log_{10}$ 尺度指定
  * 分析了有無相關性過濾器的情況（限制所有絕對成對相關性小於 0.50）
  * 結果（如圖 11.6 所示）：
    - 使用所有預測變數的模型性能略優
    - ROC 曲線 AUC 值高出 0.064，差異的置信區間為 (0.037, 0.091)
    - 基於未過濾模型，數值上最佳子集大小為 377 個預測變數
    - 經過過濾的模型使用約 30 個預測變數可能達到類似性能

- **ROC 曲線統計量與隨機森林重要性分數比較**：
  * 測試使用 ROC 曲線統計量而非隨機森林重要性分數進行排名
  * 考慮因素：
    - 共享高相關性的重要預測變數可能排名更高
    - 隨機森林排名考慮所有特徵的同時存在
    - 隨機森林可能考慮到重要的預測變數交互作用
  * 結果（圖 11.6 右側面板）：
    - 性能值相近，過濾模型使用 ROC 曲線排名略佳
    - 消除過程導致更快的性能損失
    - 基於過濾模型，128 個預測變數的子集可產生約 0.786 的 ROC 曲線下面積
    - 使用 128 個大小的子集的優化模型，ROC 分數高於 32% 的隨機子集

- **預測變數排名一致性**：
  * 圖 11.6 顯示每個預測變數的 ROC 重要性分數
  * 點表示重採樣平均分數，帶表示這些值的兩個標準差
  * 排名合理一致（趨勢不是完全平坦的水平線）
  * 從置信帶可以看出一些預測變數只被選擇一次（即無帶）、只被選幾次或在重採樣中有變化的值

- **結論**：
  * RFE 是一種有效且相對高效的技術，通過移除不相關預測變數來降低模型複雜度
  * 雖然它是一種貪婪方法，但很可能是最廣泛使用的特徵選擇方法

### 注意點：

- 隨機樣本的大小，通常表示為 $m_{try}$，是隨機森林的主要調整參數
- 雖然 $m_{try}$ 是隨機森林模型的調整參數，但默認值 $m_{try} \approx \sqrt{p}$ 通常提供良好的整體性能
- 調整這個參數可能會導致性能提升，但通常改進幅度較小
- 對於帕金森病數據集，每個模型使用不同的隨機數種子重複五次
- 一般不建議每個隨機森林模型都使用大型集成（如10,000棵樹），但對於相對較小（但寬）的訓練集，增加樹的數量至10K-15K有助於獲得準確可靠的預測變數排名